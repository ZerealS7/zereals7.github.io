<!doctype html>
<html lang="en-us">
  <head>
    <title>2022.1.15 // Zereal-宋致远技术博客</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.72.0" />
    <meta name="viewport" content="width=device-width, initial-scale=0.75，minimum-scale=0.75, maximum-scale=0.75, user-scalable=no" />
    <meta name="author" content="Zereal" />
    <meta name="description" content="" />
    <meta name="referrer" content="never"/>
    <link rel="stylesheet" href="https://zereals7.github.io/css/main.min.f90f5edd436ec7b74ad05479a05705770306911f721193e7845948fb07fe1335.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="2022.1.15"/>
<meta name="twitter:description" content="【现代操作系统】 调度程序激活工作的目标是模拟内核线程的功能，但是为线程包提供通常在用户空间中才能实现的更好的性能和更大的灵活性。
特别的，如果用户线程从事某种系统调用时是完全的，那就不应该进行专门的非阻塞调用或者进行提前检查。无论如何，如果线程阻塞在某个系统调用或页面故障上，只要在同一个进程中有任何就绪的线程，就应该有可能运行其他的线程。由于避免了在用户空间和内核空间的不必要的转换，从而提高了效率。例如，如果某个线程由于等待另一个线程的工作而阻塞，此时没有理由请求内核，这样就减少了内核-用户转换的开销。用户件的运行时系统可以阻塞同步的线程而另外调度一个新线程。
当使用调度程序激活机制时，内核给每个进程安排一定数量的虚拟处理器，并且让运行时系统将线程分配到处。这一机制也可以用在多理器中，此时虚拟处理器可能成为恶真实的CPU。分配给一个进程的虚拟处理器的初始数量是一个，但是该进程可以申请更多的处理器并且在不用时退回。内核也可以取回已经分配出去的虚拟处理器，以便把它们分给需要更多处理器的进程。
使该机制工作的基本思路是，当内核了解到一个线程被阻塞之后，内核通知该进程的运行时系统。并且在堆栈中以参数形式传递有问题的线程编号和所发生事件的一个描述。内核通过在一个已知的起始地址启动运行时系统，从而发出了通知。这是对UNIX中信号的一种粗略模拟。这个机制称为上行调用。
一旦如此激活,运行时系统就重新调度其线程，这个过程通常是这样：把当前的线程标记为阻塞并从就绪表中取出另一个线程，设置寄存器，然后启动之。稍后，当内核知道原来的线程又可运行时，内核就又一次上行调用运行时系统，通知它这一事件。此时该运行时系统按照自己的判断，或者立即重启动被阻塞的线程，或者把它放入就绪表中稍后运行。
在某个用户线程运行的同时发生一个硬件中断时，被中断的CPU切换进内核态。如果被中断的进程对引起该中断的事件不感兴趣，比如，是另一个进程的IO完成了，那么在中断处理程序结束之后，就把被中断的线程恢复到中断之前的状态。不过，如果该进程对中断感兴趣，比如是该进程中的某个线程所需要的页面到达了，那么被中断的线程就不再启动，代之为挂起被中断的进程。而运行时系统则启动对应的虚拟CPU，此时被中断的线程状态还保存在堆栈中。随后，运行时系统决定在该CPU上调度哪个线程：被中断的线程、新就绪的线程还是某个第三种选择。
弹出式线程：一个消息的到达导致系统创建一个处理该消息的线程，这种线程称为弹出式线程。
弹出式线程的关键好处是，由于这种线程相当新，没有历史——没有必须存储的寄存器、堆栈诸如此类的内容，每个线程从全新开始，每个线程彼此之间完全一样。这样，就有可能快速创建这类线程。对该新线程指定所要处理的消息。使用弹出式线程的结果是，消息到达与处理开始的时间非常短。
在内核空间中运行弹出式线程通常比在用户空间中容易且快捷，而且内核空间中弹出式线程可以很容易访问所有的表格和IO设备，这些也许在中断处理时有用。而另一方面，出错的内核线程会比出错的用户线程造成更大的损害。例如，如果某个线程运行时间太长，又没有办法抢占它，就可能造成进来的信息丢失。
【机器学习】 经验误差与过拟合
样本数占样本总数的比例称为错误率
精度=1-错误率
学习器实际预测输出与样本的真实输出之间的差异称为误差
学习器在训练集上的误差称为训练误差或经验误差，在新样本上的误差为泛化误差。
我们实际希望的是在新样本上能表现得很好的学习器。为了达到这个目的，应该从训练样本尽可能学出适用于所有潜在样本的普遍规律，这样才能在遇到新样本的时候做出正确的判别。
然而，当学习器把样本学得太好了的 时候，很可能已经把训练样本自身的一些特点当做了所有潜在样本会具有的一般性质，这样就会导致泛化的性能下降。这种现象在机器学习中称为过拟合。与过拟合相对的是欠拟合，这是指的对训练样本的一般性质尚未学好。
有多种因素可能导致过拟合，其中最常见的是学习能力过于强大，以至于把训练样本所包含的不太一般的特性都学到了，而欠拟合则是由于学习能力低下造成的，欠拟合比较容易客服，例如在决策树学习中，扩展分支，在神经网络学习中增加训练轮数等，而过拟合则很麻烦。过拟合是机器学习面临的关键障碍，各类学习算法都必然带有一些针对过拟合的措施，然而必须认识到，过拟合是无法彻底避免的，我们所做的只是缓解，或者说减小其风险。
机器学习面临的问题通常是NP甚至更难，而有效的学习算法必然是在多项式时间内运行完成，若可彻底避免过拟合，则通过经验误差最小化就能获最优解，这就意味着我们构造性证明了P=NP；因此只要相信P不等于NP，过拟合就不可避免。
评估方法
通常，可通过实验测试来对学习器的泛化误差进行评估并进而做出选择。
为此需要使用一个测试集来测试学习器对新样本的判别能力，然后以测试集上的测试误差作为泛化误差的近似。
通常我们假设测试样本也是从样本真实分布中独立同分布采样而得。但需要注意的是，测试集应该尽可能与训练集互斥，即测试样本进来不在训练集中出现，未在训练中使用过。
显然若测试样本被用作训练了，则得到的将是过于乐观的估计结果。
如果有一个包含M个样本的数据集D，既要训练又要测试，则要对D进行适当的处理，从中产生训练集S和测试集T
几种常见的划分方法
1.留出法
直接将D划分为两个互斥的集合。在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。
需注意的是训练测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外偏差而对最终结果产生影响，例如在分类任务中，至少要保持样本的类别比例相似。
例如通过对D进行分层采用而获得含70%样本的训练集S和含30%样本的测试集T，若D包含500个正例、500个反例，则分层采样得到的S应该包含350个正例、350个反例，而T则包含150个正例和150个反例；若S、T样本类别比例差别很大，则误差估计将由于训练、测试数据分布的差异而产生偏差。
另一个需要注意的问题是，即便在给定训练测试样本的比例后，仍存在多种划分方式对初始数据集D进行分割，例如在上面的例子中，可以把D中的样本排序，然后把前350个正例放在训练集中，也可以把最后350个正例放在训练集中，这些不同的划分会导致不同的训练测试集。相应的评估模型也会有差别，因此单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般采用若干次随机划分，重复进行实验评估后取平均值作为留出法的评估结果。
留出法可能导致一些窘境：若令训练集S包含绝大多数样本，则训练出的模型可能更接近D训练出的模型，但由于T比较小，评估结果可能不够稳定准确；若令测试集T多包含一些样本。则训练集S与D差别更大了，被评估的模型与D训练出的模型相比可能有较大差别，从而降低了评估结果的保真性，这个问题没有完美的解决方案，常见做法是将大约2/3-4/5的样本用于训练，剩余样本用于测试。
2.交叉验证法
交叉验证法先将数据集D划分为K个大小相似的互斥子集，每个自己Di都尽可能保持数据分布的一致性，即从D中通过分层采样得到。然后，每次用K-1个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得k组训练测试集，从而可进行K次训练和测试，最终返回的是这K个测试结果的均值，显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值。
为强调这一点，通常把交叉验证法称为K折交叉验证，K最常用的取值是10，此时称为10折交叉验证；其他常用的k值有5、20等。
与留出法相似，将数据集D划分为k个子集同样存在多种划分方式。为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复P次，最终的评估结果是这P次K折交叉验证的结果的均值，例如10次10折交叉验证。
比如10折有很多随机划分的10折。
假定数据集D中包含M个样本，若令k=m，则得到了交叉验证法的一个特例：留一法。显然留一法不受随机样本划分方式的影响，因为只有一种划分方式。每个子集包含一个样本。留一法使用的训练集和初始数据集只少了一个样本，这就使得在绝大多数情况下，留一法中被实际评估的模型与期望评估的用D训练出的模型很相似。因此留一法的评估结果往往认为比较准确。然而留一法也有其缺陷：在数据集比较大时，训练m个模型的计算开销可能是难以忍受的（一百万个样本则需要训练100万个模型），而这还是在未考虑算法调参的情况下，另外，留一法的估计结果也未必永远比其他评估方法准确。
3.自助法
我们希望评估的是用D训练出的模型，但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本的规模变化影响较小，但计算复杂度又太高了，有没有什么办法可以减少训练样本规模不同造成的影响，同时还能比较高效地进行实验估计呢？
自助法是一个比较好的解决方案，它直接以自助采样法为基础。给定包含m个样本的数据集D，我们对它采用产生数据集D‘:每次随机从D中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能采到；这个过程重复执行M次后，我们就得到了包含m个样本的数据集D‘，这就是自助采样的结果。显然D中有一部分样本会在D’中多次出现，而另一部分样本不出现。可以做一个简单的估计，样本在m次采用中始终不被采到的概率是（1-1/m）的m次方，取极限得到为1/e，约等于0.368。
即通过自助采样，初始数据集D中约有36.8%的样本未出现在采样数据集D‘中。于是我们可将D’作为训练集，D/D‘用作测试集；这样实际评估 模型与期望评估的模型都使用m个训练样本，而我们仍有数据总量约1/3的、没在训练集中出现的样本用于测试。这样的测试结果，亦称包外估计。
自助法在数据集较小、难以有效划分训练、测试集时很有用；此外，自助法能从初始初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差。因此在初始数据量足够时，留出法和交叉验证法更常用一些。
调参与最终模型
参数配置不同，学得模型的性能往往有显著差别。因此在进行模型评估与选择时，撤了要对适用学习的算法进行选择，还需对碎发参数进行设定，这就是通常所说的参数调节或者说调参。
调参和算法选择没什么本质区别：对每种参数配置都训练出模型，然后把对应最好模型的参数作为结果，这样考虑基本是正确的。担忧一点需注意：学习算法的很多参数是在实数范围内取值，因此，对每种参数配置都训练出模型来是不可行的。现实中常用的做法是对每个参数选定一个范围和变化步长。显然，这样选定的参数值往往不是最佳值，但是这是在计算开销和性能估计之间折中的结果，通过这个折中，学习过程才变得可行。事实上，即便在进行这样的折中后，调参往往仍然很困难。
可以简单估算一下，假定算法有3个参数，每个参数仅考虑5个候选值，这样对每组训练测试有125个模型需考察，很多强大的学习算法有不少参数需设定，这将导致极大的调参工程量，以至于不少应用任务中，参数调的好不好往往对最终模型性能有关键性影响。
给定包含M个样本的数据集D，在模型评估与选择过程中由于需要留出一部分数据进行评估测试，事实上我们只使用了一部分数据训练模型，因此在模型选择完成后，学习算法和参数配置已选定，此时应该用数据集D重新训练模型。这个模型在训练过程中使用了所有M个样本，这才是最终提交给用户的模型。
另外需要注意的是，我们通常把学得模型在实际使用中遇到的数据称为测试数据，为了加以区分，模型评估与选择用于评估测试的数据集常称为验证集。例如，在研究对比不同算法的泛化性能时，我们用测试集上的判别效果来估计模型在实际使用时的泛化能力，而把训练数据另外划分为训练集和验证集，基于验证集上的性能来进行模型选择和调参。
即训练数据先分出训练集和验证集调参以及选模型，最后再用整个的训练数据再次作为训练集训练，用测试集评估模型的泛化能力。
性能度量
对学习器泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量。
性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果；这意味着模型的好坏是相对的，什么样的模型是好的，不仅取决于算法和数据，还决定于任务需求。
要评估学习器f的性能，就要把学习器预测的结果与真实标记y进行比较。
回归任务最常用的性能度量是均方误差
每一对误差的平方再求均值。
分类任务中常用的性能度量有以下一些：
错误率与精度
这是分类任务中最常用的两种性能度量，既适用于二分类任务，也适用于多分类任务。错误率是分类错误的样本数占样本总数。
精度则是分类正确的样本数占样本总数的比例。
查准率查全率与F1
错误率和精度虽常用，但并不能满足所有任务需求。以西瓜问题为例，假定瓜农拉来一车西瓜，我们用训练好的模型对这些西瓜进行判别，显然，错误率衡量了有多少比例的瓜被判别错误。但是若我们关心的是挑出来的西瓜有多少比例是好瓜或者好瓜中有多少比例被挑了出来。那么错误率显然不够用了。
对于二分类矩阵分为真正例，假正例，真反例，假反例四种情况。
查准率P与查全率R分别定义为
P=TP/TP&#43;FP 真正例除以所有表面的正例
R=TP/TP&#43;FN 真正例除以所有真正的正例"/>

    <meta property="og:title" content="2022.1.15" />
<meta property="og:description" content="【现代操作系统】 调度程序激活工作的目标是模拟内核线程的功能，但是为线程包提供通常在用户空间中才能实现的更好的性能和更大的灵活性。
特别的，如果用户线程从事某种系统调用时是完全的，那就不应该进行专门的非阻塞调用或者进行提前检查。无论如何，如果线程阻塞在某个系统调用或页面故障上，只要在同一个进程中有任何就绪的线程，就应该有可能运行其他的线程。由于避免了在用户空间和内核空间的不必要的转换，从而提高了效率。例如，如果某个线程由于等待另一个线程的工作而阻塞，此时没有理由请求内核，这样就减少了内核-用户转换的开销。用户件的运行时系统可以阻塞同步的线程而另外调度一个新线程。
当使用调度程序激活机制时，内核给每个进程安排一定数量的虚拟处理器，并且让运行时系统将线程分配到处。这一机制也可以用在多理器中，此时虚拟处理器可能成为恶真实的CPU。分配给一个进程的虚拟处理器的初始数量是一个，但是该进程可以申请更多的处理器并且在不用时退回。内核也可以取回已经分配出去的虚拟处理器，以便把它们分给需要更多处理器的进程。
使该机制工作的基本思路是，当内核了解到一个线程被阻塞之后，内核通知该进程的运行时系统。并且在堆栈中以参数形式传递有问题的线程编号和所发生事件的一个描述。内核通过在一个已知的起始地址启动运行时系统，从而发出了通知。这是对UNIX中信号的一种粗略模拟。这个机制称为上行调用。
一旦如此激活,运行时系统就重新调度其线程，这个过程通常是这样：把当前的线程标记为阻塞并从就绪表中取出另一个线程，设置寄存器，然后启动之。稍后，当内核知道原来的线程又可运行时，内核就又一次上行调用运行时系统，通知它这一事件。此时该运行时系统按照自己的判断，或者立即重启动被阻塞的线程，或者把它放入就绪表中稍后运行。
在某个用户线程运行的同时发生一个硬件中断时，被中断的CPU切换进内核态。如果被中断的进程对引起该中断的事件不感兴趣，比如，是另一个进程的IO完成了，那么在中断处理程序结束之后，就把被中断的线程恢复到中断之前的状态。不过，如果该进程对中断感兴趣，比如是该进程中的某个线程所需要的页面到达了，那么被中断的线程就不再启动，代之为挂起被中断的进程。而运行时系统则启动对应的虚拟CPU，此时被中断的线程状态还保存在堆栈中。随后，运行时系统决定在该CPU上调度哪个线程：被中断的线程、新就绪的线程还是某个第三种选择。
弹出式线程：一个消息的到达导致系统创建一个处理该消息的线程，这种线程称为弹出式线程。
弹出式线程的关键好处是，由于这种线程相当新，没有历史——没有必须存储的寄存器、堆栈诸如此类的内容，每个线程从全新开始，每个线程彼此之间完全一样。这样，就有可能快速创建这类线程。对该新线程指定所要处理的消息。使用弹出式线程的结果是，消息到达与处理开始的时间非常短。
在内核空间中运行弹出式线程通常比在用户空间中容易且快捷，而且内核空间中弹出式线程可以很容易访问所有的表格和IO设备，这些也许在中断处理时有用。而另一方面，出错的内核线程会比出错的用户线程造成更大的损害。例如，如果某个线程运行时间太长，又没有办法抢占它，就可能造成进来的信息丢失。
【机器学习】 经验误差与过拟合
样本数占样本总数的比例称为错误率
精度=1-错误率
学习器实际预测输出与样本的真实输出之间的差异称为误差
学习器在训练集上的误差称为训练误差或经验误差，在新样本上的误差为泛化误差。
我们实际希望的是在新样本上能表现得很好的学习器。为了达到这个目的，应该从训练样本尽可能学出适用于所有潜在样本的普遍规律，这样才能在遇到新样本的时候做出正确的判别。
然而，当学习器把样本学得太好了的 时候，很可能已经把训练样本自身的一些特点当做了所有潜在样本会具有的一般性质，这样就会导致泛化的性能下降。这种现象在机器学习中称为过拟合。与过拟合相对的是欠拟合，这是指的对训练样本的一般性质尚未学好。
有多种因素可能导致过拟合，其中最常见的是学习能力过于强大，以至于把训练样本所包含的不太一般的特性都学到了，而欠拟合则是由于学习能力低下造成的，欠拟合比较容易客服，例如在决策树学习中，扩展分支，在神经网络学习中增加训练轮数等，而过拟合则很麻烦。过拟合是机器学习面临的关键障碍，各类学习算法都必然带有一些针对过拟合的措施，然而必须认识到，过拟合是无法彻底避免的，我们所做的只是缓解，或者说减小其风险。
机器学习面临的问题通常是NP甚至更难，而有效的学习算法必然是在多项式时间内运行完成，若可彻底避免过拟合，则通过经验误差最小化就能获最优解，这就意味着我们构造性证明了P=NP；因此只要相信P不等于NP，过拟合就不可避免。
评估方法
通常，可通过实验测试来对学习器的泛化误差进行评估并进而做出选择。
为此需要使用一个测试集来测试学习器对新样本的判别能力，然后以测试集上的测试误差作为泛化误差的近似。
通常我们假设测试样本也是从样本真实分布中独立同分布采样而得。但需要注意的是，测试集应该尽可能与训练集互斥，即测试样本进来不在训练集中出现，未在训练中使用过。
显然若测试样本被用作训练了，则得到的将是过于乐观的估计结果。
如果有一个包含M个样本的数据集D，既要训练又要测试，则要对D进行适当的处理，从中产生训练集S和测试集T
几种常见的划分方法
1.留出法
直接将D划分为两个互斥的集合。在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。
需注意的是训练测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外偏差而对最终结果产生影响，例如在分类任务中，至少要保持样本的类别比例相似。
例如通过对D进行分层采用而获得含70%样本的训练集S和含30%样本的测试集T，若D包含500个正例、500个反例，则分层采样得到的S应该包含350个正例、350个反例，而T则包含150个正例和150个反例；若S、T样本类别比例差别很大，则误差估计将由于训练、测试数据分布的差异而产生偏差。
另一个需要注意的问题是，即便在给定训练测试样本的比例后，仍存在多种划分方式对初始数据集D进行分割，例如在上面的例子中，可以把D中的样本排序，然后把前350个正例放在训练集中，也可以把最后350个正例放在训练集中，这些不同的划分会导致不同的训练测试集。相应的评估模型也会有差别，因此单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般采用若干次随机划分，重复进行实验评估后取平均值作为留出法的评估结果。
留出法可能导致一些窘境：若令训练集S包含绝大多数样本，则训练出的模型可能更接近D训练出的模型，但由于T比较小，评估结果可能不够稳定准确；若令测试集T多包含一些样本。则训练集S与D差别更大了，被评估的模型与D训练出的模型相比可能有较大差别，从而降低了评估结果的保真性，这个问题没有完美的解决方案，常见做法是将大约2/3-4/5的样本用于训练，剩余样本用于测试。
2.交叉验证法
交叉验证法先将数据集D划分为K个大小相似的互斥子集，每个自己Di都尽可能保持数据分布的一致性，即从D中通过分层采样得到。然后，每次用K-1个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得k组训练测试集，从而可进行K次训练和测试，最终返回的是这K个测试结果的均值，显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值。
为强调这一点，通常把交叉验证法称为K折交叉验证，K最常用的取值是10，此时称为10折交叉验证；其他常用的k值有5、20等。
与留出法相似，将数据集D划分为k个子集同样存在多种划分方式。为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复P次，最终的评估结果是这P次K折交叉验证的结果的均值，例如10次10折交叉验证。
比如10折有很多随机划分的10折。
假定数据集D中包含M个样本，若令k=m，则得到了交叉验证法的一个特例：留一法。显然留一法不受随机样本划分方式的影响，因为只有一种划分方式。每个子集包含一个样本。留一法使用的训练集和初始数据集只少了一个样本，这就使得在绝大多数情况下，留一法中被实际评估的模型与期望评估的用D训练出的模型很相似。因此留一法的评估结果往往认为比较准确。然而留一法也有其缺陷：在数据集比较大时，训练m个模型的计算开销可能是难以忍受的（一百万个样本则需要训练100万个模型），而这还是在未考虑算法调参的情况下，另外，留一法的估计结果也未必永远比其他评估方法准确。
3.自助法
我们希望评估的是用D训练出的模型，但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本的规模变化影响较小，但计算复杂度又太高了，有没有什么办法可以减少训练样本规模不同造成的影响，同时还能比较高效地进行实验估计呢？
自助法是一个比较好的解决方案，它直接以自助采样法为基础。给定包含m个样本的数据集D，我们对它采用产生数据集D‘:每次随机从D中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能采到；这个过程重复执行M次后，我们就得到了包含m个样本的数据集D‘，这就是自助采样的结果。显然D中有一部分样本会在D’中多次出现，而另一部分样本不出现。可以做一个简单的估计，样本在m次采用中始终不被采到的概率是（1-1/m）的m次方，取极限得到为1/e，约等于0.368。
即通过自助采样，初始数据集D中约有36.8%的样本未出现在采样数据集D‘中。于是我们可将D’作为训练集，D/D‘用作测试集；这样实际评估 模型与期望评估的模型都使用m个训练样本，而我们仍有数据总量约1/3的、没在训练集中出现的样本用于测试。这样的测试结果，亦称包外估计。
自助法在数据集较小、难以有效划分训练、测试集时很有用；此外，自助法能从初始初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差。因此在初始数据量足够时，留出法和交叉验证法更常用一些。
调参与最终模型
参数配置不同，学得模型的性能往往有显著差别。因此在进行模型评估与选择时，撤了要对适用学习的算法进行选择，还需对碎发参数进行设定，这就是通常所说的参数调节或者说调参。
调参和算法选择没什么本质区别：对每种参数配置都训练出模型，然后把对应最好模型的参数作为结果，这样考虑基本是正确的。担忧一点需注意：学习算法的很多参数是在实数范围内取值，因此，对每种参数配置都训练出模型来是不可行的。现实中常用的做法是对每个参数选定一个范围和变化步长。显然，这样选定的参数值往往不是最佳值，但是这是在计算开销和性能估计之间折中的结果，通过这个折中，学习过程才变得可行。事实上，即便在进行这样的折中后，调参往往仍然很困难。
可以简单估算一下，假定算法有3个参数，每个参数仅考虑5个候选值，这样对每组训练测试有125个模型需考察，很多强大的学习算法有不少参数需设定，这将导致极大的调参工程量，以至于不少应用任务中，参数调的好不好往往对最终模型性能有关键性影响。
给定包含M个样本的数据集D，在模型评估与选择过程中由于需要留出一部分数据进行评估测试，事实上我们只使用了一部分数据训练模型，因此在模型选择完成后，学习算法和参数配置已选定，此时应该用数据集D重新训练模型。这个模型在训练过程中使用了所有M个样本，这才是最终提交给用户的模型。
另外需要注意的是，我们通常把学得模型在实际使用中遇到的数据称为测试数据，为了加以区分，模型评估与选择用于评估测试的数据集常称为验证集。例如，在研究对比不同算法的泛化性能时，我们用测试集上的判别效果来估计模型在实际使用时的泛化能力，而把训练数据另外划分为训练集和验证集，基于验证集上的性能来进行模型选择和调参。
即训练数据先分出训练集和验证集调参以及选模型，最后再用整个的训练数据再次作为训练集训练，用测试集评估模型的泛化能力。
性能度量
对学习器泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量。
性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果；这意味着模型的好坏是相对的，什么样的模型是好的，不仅取决于算法和数据，还决定于任务需求。
要评估学习器f的性能，就要把学习器预测的结果与真实标记y进行比较。
回归任务最常用的性能度量是均方误差
每一对误差的平方再求均值。
分类任务中常用的性能度量有以下一些：
错误率与精度
这是分类任务中最常用的两种性能度量，既适用于二分类任务，也适用于多分类任务。错误率是分类错误的样本数占样本总数。
精度则是分类正确的样本数占样本总数的比例。
查准率查全率与F1
错误率和精度虽常用，但并不能满足所有任务需求。以西瓜问题为例，假定瓜农拉来一车西瓜，我们用训练好的模型对这些西瓜进行判别，显然，错误率衡量了有多少比例的瓜被判别错误。但是若我们关心的是挑出来的西瓜有多少比例是好瓜或者好瓜中有多少比例被挑了出来。那么错误率显然不够用了。
对于二分类矩阵分为真正例，假正例，真反例，假反例四种情况。
查准率P与查全率R分别定义为
P=TP/TP&#43;FP 真正例除以所有表面的正例
R=TP/TP&#43;FN 真正例除以所有真正的正例" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://zereals7.github.io/post/2022%E5%B9%B41%E6%9C%8815%E6%97%A5%E5%AD%A6%E4%B9%A0%E6%94%B6%E8%8E%B7/" />
<meta property="article:published_time" content="2022-01-09T13:12:00+08:00" />
<meta property="article:modified_time" content="2022-01-09T13:12:00+08:00" />


  </head>
  <body>
    <header class="app-header">
      <a href="https://zereals7.github.io/"><img class="app-header-avatar" src="/avatar.jpg" alt="Zereal" /></a>
      <h1>Zereal</h1>
      <p>Java coder</p>
      <div class="app-header-social">
        
          <a target="_blank" href="https://github.com/ZerealS7" rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg></a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">2022.1.15</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Jan 9, 2022
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          1 min read
        </div></div>
    </header>
    <div class="post-content">
      <h1 id="现代操作系统">【现代操作系统】</h1>
<p>调度程序激活工作的目标是模拟内核线程的功能，但是为线程包提供通常在用户空间中才能实现的更好的性能和更大的灵活性。</p>
<p>特别的，如果用户线程从事某种系统调用时是完全的，那就不应该进行专门的非阻塞调用或者进行提前检查。无论如何，如果线程阻塞在某个系统调用或页面故障上，只要在同一个进程中有任何就绪的线程，就应该有可能运行其他的线程。由于避免了在用户空间和内核空间的不必要的转换，从而提高了效率。例如，如果某个线程由于等待另一个线程的工作而阻塞，此时没有理由请求内核，这样就减少了内核-用户转换的开销。用户件的运行时系统可以阻塞同步的线程而另外调度一个新线程。</p>
<p>当使用调度程序激活机制时，内核给每个进程安排一定数量的虚拟处理器，并且让运行时系统将线程分配到处。这一机制也可以用在多理器中，此时虚拟处理器可能成为恶真实的CPU。分配给一个进程的虚拟处理器的初始数量是一个，但是该进程可以申请更多的处理器并且在不用时退回。内核也可以取回已经分配出去的虚拟处理器，以便把它们分给需要更多处理器的进程。</p>
<p>使该机制工作的基本思路是，当内核了解到一个线程被阻塞之后，内核通知该进程的运行时系统。并且在堆栈中以参数形式传递有问题的线程编号和所发生事件的一个描述。内核通过在一个已知的起始地址启动运行时系统，从而发出了通知。这是对UNIX中信号的一种粗略模拟。这个机制称为上行调用。</p>
<p>一旦如此激活,运行时系统就重新调度其线程，这个过程通常是这样：把当前的线程标记为阻塞并从就绪表中取出另一个线程，设置寄存器，然后启动之。稍后，当内核知道原来的线程又可运行时，内核就又一次上行调用运行时系统，通知它这一事件。此时该运行时系统按照自己的判断，或者立即重启动被阻塞的线程，或者把它放入就绪表中稍后运行。</p>
<p>在某个用户线程运行的同时发生一个硬件中断时，被中断的CPU切换进内核态。如果被中断的进程对引起该中断的事件不感兴趣，比如，是另一个进程的IO完成了，那么在中断处理程序结束之后，就把被中断的线程恢复到中断之前的状态。不过，如果该进程对中断感兴趣，比如是该进程中的某个线程所需要的页面到达了，那么被中断的线程就不再启动，代之为挂起被中断的进程。而运行时系统则启动对应的虚拟CPU，此时被中断的线程状态还保存在堆栈中。随后，运行时系统决定在该CPU上调度哪个线程：被中断的线程、新就绪的线程还是某个第三种选择。</p>
<p>弹出式线程：一个消息的到达导致系统创建一个处理该消息的线程，这种线程称为弹出式线程。</p>
<p>弹出式线程的关键好处是，由于这种线程相当新，没有历史——没有必须存储的寄存器、堆栈诸如此类的内容，每个线程从全新开始，每个线程彼此之间完全一样。这样，就有可能快速创建这类线程。对该新线程指定所要处理的消息。使用弹出式线程的结果是，消息到达与处理开始的时间非常短。</p>
<p>在内核空间中运行弹出式线程通常比在用户空间中容易且快捷，而且内核空间中弹出式线程可以很容易访问所有的表格和IO设备，这些也许在中断处理时有用。而另一方面，出错的内核线程会比出错的用户线程造成更大的损害。例如，如果某个线程运行时间太长，又没有办法抢占它，就可能造成进来的信息丢失。</p>
<h1 id="机器学习">【机器学习】</h1>
<p><strong>经验误差与过拟合</strong></p>
<p>样本数占样本总数的比例称为错误率</p>
<p>精度=1-错误率</p>
<p>学习器实际预测输出与样本的真实输出之间的差异称为误差</p>
<p>学习器在训练集上的误差称为<strong>训练误差或经验误差</strong>，在新样本上的误差为<strong>泛化误差</strong>。</p>
<p>我们实际希望的是在新样本上能表现得很好的学习器。为了达到这个目的，应该从训练样本尽可能学出适用于所有潜在样本的普遍规律，这样才能在遇到新样本的时候做出正确的判别。</p>
<p>然而，当学习器把样本学得太好了的 时候，很可能已经把训练样本自身的一些特点当做了所有潜在样本会具有的一般性质，这样就会导致泛化的性能下降。这种现象在机器学习中称为过拟合。与过拟合相对的是欠拟合，这是指的对训练样本的一般性质尚未学好。</p>
<p>有多种因素可能导致过拟合，其中最常见的是学习能力过于强大，以至于把训练样本所包含的不太一般的特性都学到了，而欠拟合则是由于学习能力低下造成的，欠拟合比较容易客服，例如在决策树学习中，扩展分支，在神经网络学习中增加训练轮数等，而过拟合则很麻烦。过拟合是机器学习面临的关键障碍，各类学习算法都必然带有一些针对过拟合的措施，然而必须认识到，过拟合是无法彻底避免的，我们所做的只是缓解，或者说减小其风险。</p>
<p>机器学习面临的问题通常是NP甚至更难，而有效的学习算法必然是在多项式时间内运行完成，若可彻底避免过拟合，则通过经验误差最小化就能获最优解，这就意味着我们构造性证明了P=NP；因此只要相信P不等于NP，过拟合就不可避免。</p>
<p><strong>评估方法</strong></p>
<p>通常，可通过实验测试来对学习器的泛化误差进行评估并进而做出选择。</p>
<p>为此需要使用一个测试集来测试学习器对新样本的判别能力，然后以测试集上的测试误差作为泛化误差的近似。</p>
<p>通常我们假设测试样本也是从样本真实分布中独立同分布采样而得。但需要注意的是，测试集应该尽可能与训练集互斥，即测试样本进来不在训练集中出现，未在训练中使用过。</p>
<p>显然若测试样本被用作训练了，则得到的将是过于乐观的估计结果。</p>
<p>如果有一个包含M个样本的数据集D，既要训练又要测试，则要对D进行适当的处理，从中产生训练集S和测试集T</p>
<p><strong>几种常见的划分方法</strong></p>
<p>1.<strong>留出法</strong></p>
<p>直接将D划分为两个互斥的集合。在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。</p>
<p>需注意的是训练测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外偏差而对最终结果产生影响，例如在分类任务中，至少要保持样本的类别比例相似。</p>
<p>例如通过对D进行分层采用而获得含70%样本的训练集S和含30%样本的测试集T，若D包含500个正例、500个反例，则分层采样得到的S应该包含350个正例、350个反例，而T则包含150个正例和150个反例；若S、T样本类别比例差别很大，则误差估计将由于训练、测试数据分布的差异而产生偏差。</p>
<p>另一个需要注意的问题是，即便在给定训练测试样本的比例后，仍存在多种划分方式对初始数据集D进行分割，例如在上面的例子中，可以把D中的样本排序，然后把前350个正例放在训练集中，也可以把最后350个正例放在训练集中，这些不同的划分会导致不同的训练测试集。相应的评估模型也会有差别，因此单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般采用若干次随机划分，重复进行实验评估后取平均值作为留出法的评估结果。</p>
<p>留出法可能导致一些窘境：若令训练集S包含绝大多数样本，则训练出的模型可能更接近D训练出的模型，但由于T比较小，评估结果可能不够稳定准确；若令测试集T多包含一些样本。则训练集S与D差别更大了，被评估的模型与D训练出的模型相比可能有较大差别，从而降低了评估结果的保真性，这个问题没有完美的解决方案，常见做法是将大约2/3-4/5的样本用于训练，剩余样本用于测试。</p>
<p>2.<strong>交叉验证法</strong></p>
<p>交叉验证法先将数据集D划分为K个大小相似的互斥子集，每个自己Di都尽可能保持数据分布的一致性，即从D中通过分层采样得到。然后，每次用K-1个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得k组训练测试集，从而可进行K次训练和测试，最终返回的是这K个测试结果的均值，显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值。</p>
<p>为强调这一点，通常把交叉验证法称为K折交叉验证，K最常用的取值是10，此时称为10折交叉验证；其他常用的k值有5、20等。</p>
<p>与留出法相似，将数据集D划分为k个子集同样存在多种划分方式。为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复P次，最终的评估结果是这P次K折交叉验证的结果的均值，例如10次10折交叉验证。</p>
<p>比如10折有很多随机划分的10折。</p>
<p>假定数据集D中包含M个样本，若令k=m，则得到了交叉验证法的一个特例：留一法。显然留一法不受随机样本划分方式的影响，因为只有一种划分方式。每个子集包含一个样本。留一法使用的训练集和初始数据集只少了一个样本，这就使得在绝大多数情况下，留一法中被实际评估的模型与期望评估的用D训练出的模型很相似。因此留一法的评估结果往往认为比较准确。然而留一法也有其缺陷：在数据集比较大时，训练m个模型的计算开销可能是难以忍受的（一百万个样本则需要训练100万个模型），而这还是在未考虑算法调参的情况下，另外，留一法的估计结果也未必永远比其他评估方法准确。</p>
<p>3.<strong>自助法</strong></p>
<p>我们希望评估的是用D训练出的模型，但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本的规模变化影响较小，但计算复杂度又太高了，有没有什么办法可以减少训练样本规模不同造成的影响，同时还能比较高效地进行实验估计呢？</p>
<p>自助法是一个比较好的解决方案，它直接以自助采样法为基础。给定包含m个样本的数据集D，我们对它采用产生数据集D‘:每次随机从D中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能采到；这个过程重复执行M次后，我们就得到了包含m个样本的数据集D‘，这就是自助采样的结果。显然D中有一部分样本会在D’中多次出现，而另一部分样本不出现。可以做一个简单的估计，样本在m次采用中始终不被采到的概率是（1-1/m）的m次方，取极限得到为1/e，约等于0.368。</p>
<p>即通过自助采样，初始数据集D中约有36.8%的样本未出现在采样数据集D‘中。于是我们可将D’作为训练集，D/D‘用作测试集；这样实际评估 模型与期望评估的模型都使用m个训练样本，而我们仍有数据总量约1/3的、没在训练集中出现的样本用于测试。这样的测试结果，亦称包外估计。</p>
<p>自助法<strong>在数据集较小、难以有效划分训练、测试集时很有用</strong>；此外，自助法能从初始初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差。因此在初始数据量足够时，留出法和交叉验证法更常用一些。</p>
<p><strong>调参与最终模型</strong></p>
<p>参数配置不同，学得模型的性能往往有显著差别。因此在进行模型评估与选择时，撤了要对适用学习的算法进行选择，还需对碎发参数进行设定，这就是通常所说的参数调节或者说调参。</p>
<p>调参和算法选择没什么本质区别：对每种参数配置都训练出模型，然后把对应最好模型的参数作为结果，这样考虑基本是正确的。担忧一点需注意：学习算法的很多参数是在实数范围内取值，因此，对每种参数配置都训练出模型来是不可行的。现实中常用的做法是对每个参数选定一个范围和变化步长。显然，这样选定的参数值往往不是最佳值，但是这是在计算开销和性能估计之间折中的结果，通过这个折中，学习过程才变得可行。事实上，即便在进行这样的折中后，调参往往仍然很困难。</p>
<p>可以简单估算一下，假定算法有3个参数，每个参数仅考虑5个候选值，这样对每组训练测试有125个模型需考察，很多强大的学习算法有不少参数需设定，这将导致极大的调参工程量，以至于不少应用任务中，参数调的好不好往往对最终模型性能有关键性影响。</p>
<p>给定包含M个样本的数据集D，在模型评估与选择过程中由于需要留出一部分数据进行评估测试，事实上我们只使用了一部分数据训练模型，因此在模型选择完成后，学习算法和参数配置已选定，此时应该用数据集D重新训练模型。这个模型在训练过程中使用了所有M个样本，这才是最终提交给用户的模型。</p>
<p>另外需要注意的是，我们通常把学得模型在实际使用中遇到的数据称为测试数据，为了加以区分，模型评估与选择用于评估测试的数据集常称为验证集。例如，在研究对比不同算法的泛化性能时，我们用测试集上的判别效果来估计模型在实际使用时的泛化能力，而把训练数据另外划分为训练集和验证集，基于验证集上的性能来进行模型选择和调参。</p>
<p>即训练数据先分出训练集和验证集调参以及选模型，最后再用整个的训练数据再次作为训练集训练，用测试集评估模型的泛化能力。</p>
<p><strong>性能度量</strong></p>
<p>对学习器泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量。</p>
<p>性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果；这意味着模型的好坏是相对的，什么样的模型是好的，不仅取决于算法和数据，还决定于任务需求。</p>
<p>要评估学习器f的性能，就要把学习器预测的结果与真实标记y进行比较。</p>
<p><strong>回归任务最常用的性能度量是均方误差</strong></p>
<p>每一对误差的平方再求均值。</p>
<p>分类任务中常用的性能度量有以下一些：</p>
<p><strong>错误率与精度</strong></p>
<p>这是分类任务中最常用的两种性能度量，既适用于二分类任务，也适用于多分类任务。错误率是分类错误的样本数占样本总数。</p>
<p>精度则是分类正确的样本数占样本总数的比例。</p>
<p><strong>查准率查全率与F1</strong></p>
<p>错误率和精度虽常用，但并不能满足所有任务需求。以西瓜问题为例，假定瓜农拉来一车西瓜，我们用训练好的模型对这些西瓜进行判别，显然，错误率衡量了有多少比例的瓜被判别错误。但是若我们关心的是挑出来的西瓜有多少比例是好瓜或者好瓜中有多少比例被挑了出来。那么错误率显然不够用了。</p>
<p>对于二分类矩阵分为真正例，假正例，真反例，假反例四种情况。</p>
<p>查准率P与查全率R分别定义为</p>
<p>P=TP/TP+FP 真正例除以所有表面的正例</p>
<p>R=TP/TP+FN 真正例除以所有真正的正例</p>
<p>查准率和查全率是一对矛盾的度量。一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。</p>
<p>通常只有在一些简单任务中，才可能使查全率和查准率都很高。</p>
<p>以查准率为纵轴、查全率为横轴作图，就得到了查准率-查全率曲线，简称P-R曲线。</p>
<p>若一个学习器的P-R曲线被另一个学习器的曲线完全包住，则可断言后者性能优于前者。如果A与B有交叉则难以比较，比较好的方法是比较PR曲线下面积的大小。</p>
<p>平衡点是一个度量，它是查准率=查全率时的取值</p>
<p>在一些应用中，对查准率和查全率的重视程度有所不同。例如在商品推荐系统中，为了尽可能少打扰用户，更希望推荐的内容是用户感兴趣的，此时查准率更重要，而在逃犯信息检索系统中，更希望尽可能少漏掉逃犯，此时查全率更重要 。</p>
<p>F1度量</p>
<p>F1=2<em>P</em>R/P+R</p>
<p>还有加入β的一般形式。β=1时退化为标准的F1；β&lt;1时查准率有更大影响，β&gt;1时查全率有更大影响。</p>
<p>很多时候我们有多个二分类混淆矩阵，例如进行多次训练测试，每次得到一个混淆矩阵，或是在多个数据集上进行训练测试，希望估计算法的全局性能，或是执行多分类任务，每两两类别的组合都对应一个混淆矩阵，总之我们希望在n个而非呢类混淆矩阵上综合考察查准率和查全率。</p>
<p>一种直接的做法是先在各混淆矩阵上分别计算出查准率和查全率，再计算平均值。再基于这些平均值计算出微查准率、微查全率和微F1。</p>
<p><strong>ROC与AUC</strong></p>
<p>很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为范雷，例如，神经网络再一般情形下是对每个测试样本预测出一个0-1之间的实值，然后将这个值与0.5进行比较，大于0.5则判为正例，否则为反例。这个实值或概率预测结果的好坏，直接决定了学习器的泛化能力。实际上，根据这个实值或概率预测结果，我们可将测试样本进行排序，最可能是正例的排在最前面，最不可能是正例排在最后面。这样，分类过程就相当于在这个排序中以某个截断点将样本分为两部分，前一部分判作正例，后一部分则判为反例。</p>
<p>在不同的应用任务中，我们可根据任务需求来采用不同的截断点，若更加重视查准率，则可选择排序中靠前的位置进行截断，若更重视查全率则可选择靠后的位置进行截断，因此排序本身质量的好坏，体现了综合考虑学习器在不同任务下的期望泛化性能的好坏，或者说一般情况下泛化性能的好坏。ROC曲线则是从这个角度出发来研究学习器泛化性能的有力工具。</p>
<p>ROC全称是受试者工作特征，我们根据学习器的预测结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算出两个重要量的值，分别以它们为横纵坐标作图，就得到了ROC曲线。与PR曲线不同</p>
<p>ROC曲线的纵轴是真正例率，横轴是假正例率</p>
<p>TPR=TP/TP+FN 真正例除以真正的正例</p>
<p>FPR=FP/TN+FP  假正例除以真正的假例</p>
<p>假正例率为横轴 真正例率为纵轴</p>
<p>进行学习器的比较时，与PR图相似，若一个学习器的ROC曲线被另一个学习器完全包住，则可断言后者的性能优于前者；若两个学习器的曲线发生交叉，则比较面积。这个面积就是AUC，AUC考虑的是样本预测的排序质量，它与排序误差有紧密联系。</p>
<p>考虑每一对正反例，若正例预测值小于反例则记一个罚分。若相等，则记0.5个罚分。</p>
<p>AUC=1-LRANK</p>
<p><strong>代价敏感错误率与代价曲线</strong></p>
<p>现实中常会遇到这样的情况:不同类型的错误所造成的后果不同。例如在医疗诊断中，错误把患者诊断为健康人与错误地把健康人诊断为患者，看起来都是犯了一次错误，但后者的影响是增加了进一步检查的麻烦。前者的后果却可能是丧失了拯救生命的最佳时机。</p>
<p>为权衡不同类型错误所造成的不同损失，可为错误赋予非均等代价。</p>
<p>可根据任务的领域知识设置代价矩阵。</p>
<p>costij表示将第i类样本预测为第J类样本的代价。他们大多隐式假设了均等代价，但并没有考虑不同错误会造成不同的后果。在非均等代价下，我们所希望的不再是简单地最小化错误次数，而是希望最小化总体代价。</p>
<p>若令COSTIJ中的Ij取值不限于01，则可定义出多分类任务的代价敏感性能度量。</p>
<p>在非均等代价下，ROC曲线不能直接反映出学习器的期望总体代价,而代价曲线则可达到该目的，代价曲线图的横轴是取值为[0,1]的正例概率代价。</p>
<p>其中FNR=1-TPR是假反例率。代价曲线的绘制很简单：ROC曲线上每一点对应了代价平面上的一条线段，设ROC曲线上点的坐标为（FPR，TPR），则可相应计算出FNR，然后在代价平面上绘制一条（0，FPR）到（1，FNR）的线段，线段下的面积即表示了该条件下的期望的总体代价。</p>
<p><strong>比较检验</strong></p>
<p>有了实验评估方法和性能度量，看起来就能对学习器的性能进行评估比较了：先使用某种实验评估方法测得学习器的某个性能度量结果，然后对这些结果进行比较。但怎么比较呢？是直接取得性能度量的值然后比大小吗？</p>
<p>实际上机器学习中性能比较这件事比大家想象要复杂的多。这里涉及几个重要因素：首先希望比较的是泛化性能，然而通过实验评估方法我们获得的是测试集上的性能，两者的对比结果可能未必相同。第二，测试集上的性能与测试集本身的选择有很大关系，且不论实验不同大小的测试集会得到不同的结果，即使用相同大小的测试集，若包含的测试阳历不同，测试结果也会有不同。第三机器学习算法本身有一点的随机性，，即使有相同的参数设置在同一个测试集上多次运行，其结果也会有不同。</p>
<p>统计假设检验为我们提供了依据。基于假设检验结果我们可推断出，若在测试集上观察学习器A比B好，则A的泛化性能是否在统计意义上优于B，以及这个结论的把握有多大。下面先介绍两种最基本的假设检验。</p>
<p><strong>假设检验</strong></p>
<p>现实任务中，我们不知道学习器的泛化错误率，只能获知其测试错误率，泛化错误率和测试错误率未必相同，但直观上二者接近的可能性很大，相差很远的可能性比较小，</p>
<p><strong>偏差与方差</strong></p>
<p>偏差方差分解试图对学习算法的期望泛化错误率进行拆解。我们知道，算法在不同训练集上学得的结果很可能不同，即便这些训练集是来自同一个分布。</p>
<p>泛化误差可分解为偏差，方差与噪声之和。</p>
<p>偏差度量了学习算法的期望预测与真实结果的偏离程度。即刻画了学习算法本身的拟合能力；方差度量了同样大小的训练集的变动所导致的学习性能的变化。即刻画了数据扰动所造成的影响；噪声则表达了在当前任务上学习算法所能达到的期望泛化误差的下届，即刻画了学习问题本身的难度。</p>
<p>泛化性能是由学习算法能力，数据的充分性以及学习任务本身的难度所共同决定的。</p>
<p>给定学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即是的数据扰动产生的影响小。</p>
<p>一般来说，偏差与方差是有冲突的，这称为偏差方差窘境。假定我们能控制学习算法的训练程度，则在训练不足时，学习器的拟合能力不够强，训练数据的扰动不足以使学习器产生显著变化，此时偏差主导了泛化错误率，随着训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，方差逐渐主导了泛化错误率，在训练程度充足后，学习器的拟合能力已非常强，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学到了，则将发生过拟合。</p>
<p>第二章不足之处在于对假设检验和比较检验的几种方法理解程度不够。以后有时间需要进行回顾。</p>

    </div>
    <div class="post-footer">
      
    </div>
    <div class="post-comment">
      
      


<span id="/post/2022%E5%B9%B41%E6%9C%8815%E6%97%A5%E5%AD%A6%E4%B9%A0%E6%94%B6%E8%8E%B7/" class="leancloud_visitors" data-flag-title="2022.1.15">
    <span class="post-meta-item-text">文章阅读量 </span>
    <span class="leancloud-visitors-count">1000000</span>
    <p></p>
  </span>
<div id="vcomments"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<script type="text/javascript">
  new Valine({
    el: '#vcomments' ,
    appId: 'cScQmclMsD4OOWCclCP1pNsz-gzGzoHsz',
    appKey: 'dtSaHLmdH3J4ICVVInYg9YFM',
    notify:  false ,
  verify:  false ,
  avatar:'mm',
    placeholder: '说点什么吧...',
    visitor:  true 
  });
</script>

    </div>
  </article>

    </main>
  </body>
</html>
