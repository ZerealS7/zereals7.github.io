<!doctype html>
<html lang="en-us">
  <head>
    <title>2022年1月26 // Zereal-宋致远技术博客</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.72.0" />
    <meta name="viewport" content="width=device-width, initial-scale=0.75，minimum-scale=0.75, maximum-scale=0.75, user-scalable=no" />
    <meta name="author" content="Zereal" />
    <meta name="description" content="" />
    <meta name="referrer" content="never"/>
    <link rel="stylesheet" href="https://zereals7.github.io/css/main.min.f90f5edd436ec7b74ad05479a05705770306911f721193e7845948fb07fe1335.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="2022年1月26"/>
<meta name="twitter:description" content="【支持向量机】 间隔与支持向量
给定训练集D，分类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开，但能将训练样本分开的划分超平面可能有很多，应该找哪个。
直观来看，应该去找位于两类训练样本正中间的划分超平面，因为该划分超平面对训练样本局部扰动的容忍性拍最好。例如，由于训练集的局限性和或噪声的因素，训练集外的样本可能比训练样本更接近两个类的分解，这将使许多划分超平面出现错误，而红色的超平面受影响最小。换言之，这个划分超平面是最鲁棒的，对未见示例泛化能力最强。
划分超平面可通过线性方程来描述。W为法向量，决定了 超平面的方向，b为位移项，决定了超平面与原点之间的距离。显然，划分超平面可被法向量w与位移b确定。
训练样本到超平面的距离为支持向量。两个异类支持向量到超平面的距离之和为间隔。
欲找到具有最大间隔的划分超平面，也就是要找到w,b使γ最大。找到最大间隔来区分，这样分类效果很好。
核函数
存在一个划分超平面能将训练样本正确分类，然而现实任务中，原始样本空间中也许并不存在一个能准确划分两类样本的超平面。然而现实任务中，原始样本空间内也许并不存在一个准确划分两类样本的超平面。
模型最优解可通过训练样本的核函数展开。这一展式亦称支持向量展式。、
任何一个核函数都隐式地定义了一个称为再生希尔伯特空间的特征空间。
我们希望样本在特征空间内线性可分，因此特征空间好坏对支持向量机的性能至关重要需注意的是，在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，而核函数也仅是隐式地定义了这个特征空间。于是，核函数选择成为支持向量机的最大变数。若核函数选择不合适，则意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳。
常用核函数有以下几种
线性核，多项式核，高斯核，拉普拉斯核，sigmod核，高斯核也叫RBF核。
软间隔与正则化
前面的讨论中，我们一直假定训练样本在样本空间或特征空间中是线性可分的，即存在一个超平面能将不同类的样本完全划分开。然而，在现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分，退一步说即便恰好找到了某个核函数使训练集在特征空间中线性可分，也很难断定这个貌似线性可分的结果不是由于过拟合所造成的。
缓解该问题的一个办法是允许支持向量机在一些样本上出错。为此，要引入软间隔的概率。
具体来说，若所有样本都必须划分正确，这称为硬间隔，而软间隔则允许某些样本不满足约束。
当然，在最大化间隔的同时，不满足约束的样本应尽可能少。
替代损失函数一般具有较好的数学性质，如它们通常是凸的连续函数，且是L01的上界。
三种常用的替代损失函数：hinge损失、指数函数、对率损失。
这就是常用的软间隔支持向量机。软间隔支持向量机的最终模型仅与支持向量有关，即通过采用hinge损失函数仍保持稀疏性。
如果使用对率损失函数来替代式中的01损失函数，则几乎就得到了对率回归模型。实际上，支持向量机与对率回归的优化目标相近，通常情形下它们的性能也相当。对率回归的优势主要在于其输出具有自然的概率意义。欲得到概率输出需进行特殊处理。此外，对率回归能直接用于多分类任务，支持向量机为此则需进行推广。hinge损失有一块平坦的零区域，这使得支持向量机的解具有稀疏性，而对率损失是光滑的单调递减函数，不能导出类似支持向量的概念因此对率回归的解依赖于更多的训练样本，其预测开销更大。。
可一替换损失函数得到别的模型，这些模型的性质与所用的替代函数直接相关，但它们具有一个共性：优化目标中的第一项用来描述划分超平面的间隔大小，另一项用来表述训练集上的误差。
结构风险用于描述模型的某些性质，经验风险用于描述模型与训练数据的契合程度。C用于对二者进行折中。结构风险表述了我们希望获得具有何种性质的模型，这为引入领域知识和用户意图提供了途径。另一方面，该信息有助于削减假设空间，从而降低了最小化训练误差的过拟合风险。
支持向量回归
对样本，传统回归模型通常直接基于模型输出与真实输出Y之间的差别来计算损失，当且晋档完全相同时，损失才为0.
与此不同，支持向量回归，假设我们能容忍模型输出与真实输出最多有E的偏差，即仅当差别绝对值大于E才计算损失。这相当于以模型输出为中心，构建了一个宽度为2E的间隔带，若训练样本落入此间隔带，则认为是被预测正确的。
核方法
给定训练样本。若不考虑偏移项b，则无论SVM还是SVR，学得的模型总能表示成核函数的线性组合。不仅如此，事实上我们有下面这个称为表示定理的更一般 结论：
表示定理对损失函数没有现在，对正则化项仅要求单调递增，甚至不要求Ω是凸函数，意味着对于一般的损失函数和正则化项，优化问题的最优解都可表示为核函数的线性组合，这显示出核函数的巨大威力。
人们发展出一系列基于核函数的学习方法，统称为核方法。最常见的，是通过核化来将线性学习器拓展为非线性学习器。下面我们以线性判别分析为例来演示如何通过核化来对其进行非线性拓展，从而得到核线性判别分析。
支持向量机的求解通常是借助于凸优化技术。如何提高效率，使SVM能适用于大规模数据一直是研究重点。对线性核SVM已有很多成果，例如基于割平面法的是SVM具有线性复杂度，基于随机梯度下降的Pegasos速度甚至更快，而坐标下降法则是在稀疏数据上有很高的效率。
支持向量机是针对二分类设计的，对多分类任务要进行专门的推广，对带结构输出的任务也已有相应的算法。
核函数直接决定了支持向量机与核方法的最终性能，但遗憾的是，核函数的选择是一个未决问题。多核学习使用多核核函数并通过学习获得其最优凸组合作为最终的核函数。这实际上是在借助集成学习机制。
替代损失函数在机器学习中被广泛使用，但是，通过求解替代损失函数得到的是否仍是原问题的解？这在理论上称为替代损失的一致性问题。
贝叶斯分类器 贝叶斯决策论
贝叶斯决策论是概率框架下实施决策的基本方法。对分类任务来说，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优类别标记。
对每个样本x，若h能最小化总体风险，则总体风险也将被最小化。这就产生了贝叶斯判定准则：为最小化总体风险，只需在每个样本上选择那个能使条件风险最小的类别标记。此时H称为贝叶斯最优分类器，与之对应的总体风险称为贝叶斯风险。1-RH反映了分类器所能达到的最好性能，即通过机器学习所能产生的模型精度的理论上限。
具体来说，若目标是最小化分类错误，则误判损失。
最小化分类错误率的贝叶斯最优分类器是，对每个样本选择能使后验概率最大的类别标记。
不难看出，欲使用贝叶斯判定准则来最小化决策风险，首先要获得后验概率P。然而，在现实任务中，这通常难以直接获得。从这个角度来看，机器学习所要实现的是基于有限训练样本集尽可能准确地估计出后验概率P,大体来说，有两种主要策略，给定x，直接建模P来预测c，这样得到的是判别式模型。也可先对联合概率分布建模，由此得到P，这样得到的是生成式模型。显然，前面介绍的决策树、BP神经网络、支持向量机都可归入判别式模型的范畴。
估计类条件概率的一种常用策略是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。我们的任务是利用训练集D估计参数。
事实上概率模型的训练过程就是参数估计过程。对于参数估计，统计学界的两个学派分别提供了不同的解决方案：频率主义学派认为参数虽然未知，但却是客观存在的固定值，因此可通过优化似然函数等准则来确定参数值；贝叶斯学派则认为参数是未观察到的随机变量，其本身也可有分布，因此，可假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布。这是根据采样来估计概率分布参数的经典方法。
极大似然估计是试图在所有可能取值中，找到一个能使数据出现的可能性最大的值。
连乘操作易造成下溢.
极大似然估计得到的正太分布均值就是样本均值。参数化的方法虽然能使类条件概率估计变得相对简单，但估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在真实数据分布。
在现实生活中。，欲作出能较好地接近潜在真实分布的假设，往往需在一定程度上利用关于应用任务本身的经验知识，否则仅靠猜测来假设分布形式，很可能产生误导性的结果。
朴素贝叶斯分类器 基于贝叶斯公式来估计后验概率的主要困难在于：类条件概率是所有属性上的联合概率，难以从有限的训练样本直接估计得到。为避开这个障碍，朴素贝叶斯分类器采用了属性条件独立性假设。对已知类别，假设每个属性独立地对分类结果产生影响。朴素贝叶斯分类器的训练过程就是基于训练集D来估计类先验概率，并为每个属性估计条件概率。
令D表示训练集D中第C类样本组成的集合，若有充足的独立同分布样本，则可容易地估计出类先验概率。
若P好瓜大于P坏瓜，则可判定为好瓜。若某个属性值在训练集没有与某个类同时出现过，假如对连乘式计算得到概率值为0.哪怕在其他属性明显像好瓜，分类的结果都将是好瓜等于否，这显然不太合理。
为了避免其他属性携带的信息被训练集中未出现的属性值抹去，在估计概率值时通常要进行平滑，常用拉普拉斯修正。
拉普拉斯修正避免了因训练集样本不充分而导致概率估值为零的问题，并且在训练集变大时，修正过程所引入的先验的影响也会变得可忽略，使得估值渐渐趋于实际概率值。
在现实任务中，朴素贝叶斯分类器有多种使用方式。例如，若任务对预测速度要求较高，则对给定训练集，可将朴素贝叶斯法分类器涉及的所有概率估值事先计算好存储起来。，这样在进行预测时只需查表即可进行判别；若任务数据更替频繁，则可采用懒惰学习方式，先不进行任何训练，待收到预测请求时，再根据当前数据集进行概率估值，若数据不断增加，则可在现有估值基础上，仅对新增样本的属性值所涉及的概率估值进行计数修正，即可实现增量学习。
半朴素贝叶斯分类器
为了降低贝叶斯公式估计后验概率的困难，朴素贝叶斯分类器采用了属性条件独立性假设，但在现实任务中这个假设往往很难成立。于是，人们尝试对属性条件独立性假设进行一定程度的放松，由此产生了一类称为半朴素贝叶斯分类器的学习方法。
半朴素贝叶斯分类器的基本想法是适当考虑一部分属性间的相互依赖信息，从而既不需要进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系。独依赖估计是半朴素贝叶斯分类器最常用的一种策略。顾名思义，所谓独依赖就是假设每个属性在类别之外最多仅依赖于一个其他属性。
问题的关键就转化为如何确定每个属性的父属性，不同的做法产生不同的独依赖分类器。
最直接的做法是假设所有属性都依赖于同一个属性，称为超父，然后通过交叉验证等模型选择方法来确定超父属性，由此形成了SPODE方法。
TAN则是在最大带权生成树算法的基础上，通过以下步骤将属性间的依赖关系约简为树形结构。
1.计算任意两个属性之间的条件互信息
2.以属性为结点构建完全图，任意两个结点之间边的权重设为I。
3.构建此完全图的最大带权生成树，挑选根变量，将边置为有向
4.加入类别结点y，增加从y到每个属性的有向边。
容易看出，条件互信息I刻画了属性在已知类别情况下的相关性，因此通过最大生成树算法，TAN实际上仅保留了强相关属性之间的依赖性。
AODE是一种基于集成学习机制、更为强大的独依赖分类器。与SPODE通过模型选择确定超父属性不同，AODE尝试将每个属性作为超父来构建SPODE，然后将那些具有足够训练数据支撑的SPODE集成起来作为最终结果。与朴素贝叶斯分类器类似，AODE的训练过程也是计数，即在训练数据集上对符合条件的样本进行计数的过程。与朴素贝叶斯分类器相似，AODE无需模型选择，既能通过预计算节省预测时间，也能采取懒惰学习方式在预测时再进行计数，并且易于实现增量学习。"/>

    <meta property="og:title" content="2022年1月26" />
<meta property="og:description" content="【支持向量机】 间隔与支持向量
给定训练集D，分类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开，但能将训练样本分开的划分超平面可能有很多，应该找哪个。
直观来看，应该去找位于两类训练样本正中间的划分超平面，因为该划分超平面对训练样本局部扰动的容忍性拍最好。例如，由于训练集的局限性和或噪声的因素，训练集外的样本可能比训练样本更接近两个类的分解，这将使许多划分超平面出现错误，而红色的超平面受影响最小。换言之，这个划分超平面是最鲁棒的，对未见示例泛化能力最强。
划分超平面可通过线性方程来描述。W为法向量，决定了 超平面的方向，b为位移项，决定了超平面与原点之间的距离。显然，划分超平面可被法向量w与位移b确定。
训练样本到超平面的距离为支持向量。两个异类支持向量到超平面的距离之和为间隔。
欲找到具有最大间隔的划分超平面，也就是要找到w,b使γ最大。找到最大间隔来区分，这样分类效果很好。
核函数
存在一个划分超平面能将训练样本正确分类，然而现实任务中，原始样本空间中也许并不存在一个能准确划分两类样本的超平面。然而现实任务中，原始样本空间内也许并不存在一个准确划分两类样本的超平面。
模型最优解可通过训练样本的核函数展开。这一展式亦称支持向量展式。、
任何一个核函数都隐式地定义了一个称为再生希尔伯特空间的特征空间。
我们希望样本在特征空间内线性可分，因此特征空间好坏对支持向量机的性能至关重要需注意的是，在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，而核函数也仅是隐式地定义了这个特征空间。于是，核函数选择成为支持向量机的最大变数。若核函数选择不合适，则意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳。
常用核函数有以下几种
线性核，多项式核，高斯核，拉普拉斯核，sigmod核，高斯核也叫RBF核。
软间隔与正则化
前面的讨论中，我们一直假定训练样本在样本空间或特征空间中是线性可分的，即存在一个超平面能将不同类的样本完全划分开。然而，在现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分，退一步说即便恰好找到了某个核函数使训练集在特征空间中线性可分，也很难断定这个貌似线性可分的结果不是由于过拟合所造成的。
缓解该问题的一个办法是允许支持向量机在一些样本上出错。为此，要引入软间隔的概率。
具体来说，若所有样本都必须划分正确，这称为硬间隔，而软间隔则允许某些样本不满足约束。
当然，在最大化间隔的同时，不满足约束的样本应尽可能少。
替代损失函数一般具有较好的数学性质，如它们通常是凸的连续函数，且是L01的上界。
三种常用的替代损失函数：hinge损失、指数函数、对率损失。
这就是常用的软间隔支持向量机。软间隔支持向量机的最终模型仅与支持向量有关，即通过采用hinge损失函数仍保持稀疏性。
如果使用对率损失函数来替代式中的01损失函数，则几乎就得到了对率回归模型。实际上，支持向量机与对率回归的优化目标相近，通常情形下它们的性能也相当。对率回归的优势主要在于其输出具有自然的概率意义。欲得到概率输出需进行特殊处理。此外，对率回归能直接用于多分类任务，支持向量机为此则需进行推广。hinge损失有一块平坦的零区域，这使得支持向量机的解具有稀疏性，而对率损失是光滑的单调递减函数，不能导出类似支持向量的概念因此对率回归的解依赖于更多的训练样本，其预测开销更大。。
可一替换损失函数得到别的模型，这些模型的性质与所用的替代函数直接相关，但它们具有一个共性：优化目标中的第一项用来描述划分超平面的间隔大小，另一项用来表述训练集上的误差。
结构风险用于描述模型的某些性质，经验风险用于描述模型与训练数据的契合程度。C用于对二者进行折中。结构风险表述了我们希望获得具有何种性质的模型，这为引入领域知识和用户意图提供了途径。另一方面，该信息有助于削减假设空间，从而降低了最小化训练误差的过拟合风险。
支持向量回归
对样本，传统回归模型通常直接基于模型输出与真实输出Y之间的差别来计算损失，当且晋档完全相同时，损失才为0.
与此不同，支持向量回归，假设我们能容忍模型输出与真实输出最多有E的偏差，即仅当差别绝对值大于E才计算损失。这相当于以模型输出为中心，构建了一个宽度为2E的间隔带，若训练样本落入此间隔带，则认为是被预测正确的。
核方法
给定训练样本。若不考虑偏移项b，则无论SVM还是SVR，学得的模型总能表示成核函数的线性组合。不仅如此，事实上我们有下面这个称为表示定理的更一般 结论：
表示定理对损失函数没有现在，对正则化项仅要求单调递增，甚至不要求Ω是凸函数，意味着对于一般的损失函数和正则化项，优化问题的最优解都可表示为核函数的线性组合，这显示出核函数的巨大威力。
人们发展出一系列基于核函数的学习方法，统称为核方法。最常见的，是通过核化来将线性学习器拓展为非线性学习器。下面我们以线性判别分析为例来演示如何通过核化来对其进行非线性拓展，从而得到核线性判别分析。
支持向量机的求解通常是借助于凸优化技术。如何提高效率，使SVM能适用于大规模数据一直是研究重点。对线性核SVM已有很多成果，例如基于割平面法的是SVM具有线性复杂度，基于随机梯度下降的Pegasos速度甚至更快，而坐标下降法则是在稀疏数据上有很高的效率。
支持向量机是针对二分类设计的，对多分类任务要进行专门的推广，对带结构输出的任务也已有相应的算法。
核函数直接决定了支持向量机与核方法的最终性能，但遗憾的是，核函数的选择是一个未决问题。多核学习使用多核核函数并通过学习获得其最优凸组合作为最终的核函数。这实际上是在借助集成学习机制。
替代损失函数在机器学习中被广泛使用，但是，通过求解替代损失函数得到的是否仍是原问题的解？这在理论上称为替代损失的一致性问题。
贝叶斯分类器 贝叶斯决策论
贝叶斯决策论是概率框架下实施决策的基本方法。对分类任务来说，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优类别标记。
对每个样本x，若h能最小化总体风险，则总体风险也将被最小化。这就产生了贝叶斯判定准则：为最小化总体风险，只需在每个样本上选择那个能使条件风险最小的类别标记。此时H称为贝叶斯最优分类器，与之对应的总体风险称为贝叶斯风险。1-RH反映了分类器所能达到的最好性能，即通过机器学习所能产生的模型精度的理论上限。
具体来说，若目标是最小化分类错误，则误判损失。
最小化分类错误率的贝叶斯最优分类器是，对每个样本选择能使后验概率最大的类别标记。
不难看出，欲使用贝叶斯判定准则来最小化决策风险，首先要获得后验概率P。然而，在现实任务中，这通常难以直接获得。从这个角度来看，机器学习所要实现的是基于有限训练样本集尽可能准确地估计出后验概率P,大体来说，有两种主要策略，给定x，直接建模P来预测c，这样得到的是判别式模型。也可先对联合概率分布建模，由此得到P，这样得到的是生成式模型。显然，前面介绍的决策树、BP神经网络、支持向量机都可归入判别式模型的范畴。
估计类条件概率的一种常用策略是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。我们的任务是利用训练集D估计参数。
事实上概率模型的训练过程就是参数估计过程。对于参数估计，统计学界的两个学派分别提供了不同的解决方案：频率主义学派认为参数虽然未知，但却是客观存在的固定值，因此可通过优化似然函数等准则来确定参数值；贝叶斯学派则认为参数是未观察到的随机变量，其本身也可有分布，因此，可假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布。这是根据采样来估计概率分布参数的经典方法。
极大似然估计是试图在所有可能取值中，找到一个能使数据出现的可能性最大的值。
连乘操作易造成下溢.
极大似然估计得到的正太分布均值就是样本均值。参数化的方法虽然能使类条件概率估计变得相对简单，但估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在真实数据分布。
在现实生活中。，欲作出能较好地接近潜在真实分布的假设，往往需在一定程度上利用关于应用任务本身的经验知识，否则仅靠猜测来假设分布形式，很可能产生误导性的结果。
朴素贝叶斯分类器 基于贝叶斯公式来估计后验概率的主要困难在于：类条件概率是所有属性上的联合概率，难以从有限的训练样本直接估计得到。为避开这个障碍，朴素贝叶斯分类器采用了属性条件独立性假设。对已知类别，假设每个属性独立地对分类结果产生影响。朴素贝叶斯分类器的训练过程就是基于训练集D来估计类先验概率，并为每个属性估计条件概率。
令D表示训练集D中第C类样本组成的集合，若有充足的独立同分布样本，则可容易地估计出类先验概率。
若P好瓜大于P坏瓜，则可判定为好瓜。若某个属性值在训练集没有与某个类同时出现过，假如对连乘式计算得到概率值为0.哪怕在其他属性明显像好瓜，分类的结果都将是好瓜等于否，这显然不太合理。
为了避免其他属性携带的信息被训练集中未出现的属性值抹去，在估计概率值时通常要进行平滑，常用拉普拉斯修正。
拉普拉斯修正避免了因训练集样本不充分而导致概率估值为零的问题，并且在训练集变大时，修正过程所引入的先验的影响也会变得可忽略，使得估值渐渐趋于实际概率值。
在现实任务中，朴素贝叶斯分类器有多种使用方式。例如，若任务对预测速度要求较高，则对给定训练集，可将朴素贝叶斯法分类器涉及的所有概率估值事先计算好存储起来。，这样在进行预测时只需查表即可进行判别；若任务数据更替频繁，则可采用懒惰学习方式，先不进行任何训练，待收到预测请求时，再根据当前数据集进行概率估值，若数据不断增加，则可在现有估值基础上，仅对新增样本的属性值所涉及的概率估值进行计数修正，即可实现增量学习。
半朴素贝叶斯分类器
为了降低贝叶斯公式估计后验概率的困难，朴素贝叶斯分类器采用了属性条件独立性假设，但在现实任务中这个假设往往很难成立。于是，人们尝试对属性条件独立性假设进行一定程度的放松，由此产生了一类称为半朴素贝叶斯分类器的学习方法。
半朴素贝叶斯分类器的基本想法是适当考虑一部分属性间的相互依赖信息，从而既不需要进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系。独依赖估计是半朴素贝叶斯分类器最常用的一种策略。顾名思义，所谓独依赖就是假设每个属性在类别之外最多仅依赖于一个其他属性。
问题的关键就转化为如何确定每个属性的父属性，不同的做法产生不同的独依赖分类器。
最直接的做法是假设所有属性都依赖于同一个属性，称为超父，然后通过交叉验证等模型选择方法来确定超父属性，由此形成了SPODE方法。
TAN则是在最大带权生成树算法的基础上，通过以下步骤将属性间的依赖关系约简为树形结构。
1.计算任意两个属性之间的条件互信息
2.以属性为结点构建完全图，任意两个结点之间边的权重设为I。
3.构建此完全图的最大带权生成树，挑选根变量，将边置为有向
4.加入类别结点y，增加从y到每个属性的有向边。
容易看出，条件互信息I刻画了属性在已知类别情况下的相关性，因此通过最大生成树算法，TAN实际上仅保留了强相关属性之间的依赖性。
AODE是一种基于集成学习机制、更为强大的独依赖分类器。与SPODE通过模型选择确定超父属性不同，AODE尝试将每个属性作为超父来构建SPODE，然后将那些具有足够训练数据支撑的SPODE集成起来作为最终结果。与朴素贝叶斯分类器类似，AODE的训练过程也是计数，即在训练数据集上对符合条件的样本进行计数的过程。与朴素贝叶斯分类器相似，AODE无需模型选择，既能通过预计算节省预测时间，也能采取懒惰学习方式在预测时再进行计数，并且易于实现增量学习。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://zereals7.github.io/post/2022%E5%B9%B41%E6%9C%8826/" />
<meta property="article:published_time" content="2022-01-15T13:43:37+08:00" />
<meta property="article:modified_time" content="2022-01-15T13:43:37+08:00" />


  </head>
  <body>
    <header class="app-header">
      <a href="https://zereals7.github.io/"><img class="app-header-avatar" src="/avatar.jpg" alt="Zereal" /></a>
      <h1>Zereal</h1>
      <p>Java coder</p>
      <div class="app-header-social">
        
          <a target="_blank" href="https://github.com/ZerealS7" rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg></a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">2022年1月26</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Jan 15, 2022
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          1 min read
        </div></div>
    </header>
    <div class="post-content">
      <h1 id="支持向量机">【支持向量机】</h1>
<p><strong>间隔与支持向量</strong></p>
<p>给定训练集D，分类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开，但能将训练样本分开的划分超平面可能有很多，应该找哪个。</p>
<p>直观来看，应该去找位于两类训练样本正中间的划分超平面，因为该划分超平面对训练样本局部扰动的容忍性拍最好。例如，由于训练集的局限性和或噪声的因素，训练集外的样本可能比训练样本更接近两个类的分解，这将使许多划分超平面出现错误，而红色的超平面受影响最小。换言之，这个划分超平面是最鲁棒的，对未见示例泛化能力最强。</p>
<p>划分超平面可通过线性方程来描述。W为法向量，决定了 超平面的方向，b为位移项，决定了超平面与原点之间的距离。显然，划分超平面可被法向量w与位移b确定。</p>
<p>训练样本到超平面的距离为支持向量。两个异类支持向量到超平面的距离之和为间隔。</p>
<p>欲找到具有最大间隔的划分超平面，也就是要找到w,b使γ最大。找到最大间隔来区分，这样分类效果很好。</p>
<p><strong>核函数</strong></p>
<p>存在一个划分超平面能将训练样本正确分类，然而现实任务中，原始样本空间中也许并不存在一个能准确划分两类样本的超平面。然而现实任务中，原始样本空间内也许并不存在一个准确划分两类样本的超平面。</p>
<p>模型最优解可通过训练样本的核函数展开。这一展式亦称支持向量展式。、</p>
<p>任何一个核函数都隐式地定义了一个称为再生希尔伯特空间的特征空间。</p>
<p>我们希望样本在特征空间内线性可分，因此特征空间好坏对支持向量机的性能至关重要需注意的是，在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，而核函数也仅是隐式地定义了这个特征空间。于是，核函数选择成为支持向量机的最大变数。若核函数选择不合适，则意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳。</p>
<p>常用核函数有以下几种</p>
<p>线性核，多项式核，高斯核，拉普拉斯核，sigmod核，高斯核也叫RBF核。</p>
<p><strong>软间隔与正则化</strong></p>
<p>前面的讨论中，我们一直假定训练样本在样本空间或特征空间中是线性可分的，即存在一个超平面能将不同类的样本完全划分开。然而，在现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分，退一步说即便恰好找到了某个核函数使训练集在特征空间中线性可分，也很难断定这个貌似线性可分的结果不是由于过拟合所造成的。</p>
<p>缓解该问题的一个办法是允许支持向量机在一些样本上出错。为此，要引入软间隔的概率。</p>
<p>具体来说，若所有样本都必须划分正确，这称为硬间隔，而软间隔则允许某些样本不满足约束。</p>
<p>当然，在最大化间隔的同时，不满足约束的样本应尽可能少。</p>
<p>替代损失函数一般具有较好的数学性质，如它们通常是凸的连续函数，且是L01的上界。</p>
<p>三种常用的替代损失函数：hinge损失、指数函数、对率损失。</p>
<p>这就是常用的软间隔支持向量机。软间隔支持向量机的最终模型仅与支持向量有关，即通过采用hinge损失函数仍保持稀疏性。</p>
<p>如果使用对率损失函数来替代式中的01损失函数，则几乎就得到了对率回归模型。实际上，支持向量机与对率回归的优化目标相近，通常情形下它们的性能也相当。对率回归的优势主要在于其输出具有自然的概率意义。欲得到概率输出需进行特殊处理。此外，对率回归能直接用于多分类任务，支持向量机为此则需进行推广。hinge损失有一块平坦的零区域，这使得支持向量机的解具有稀疏性，而对率损失是光滑的单调递减函数，不能导出类似支持向量的概念因此对率回归的解依赖于更多的训练样本，其预测开销更大。。</p>
<p>可一替换损失函数得到别的模型，这些模型的性质与所用的替代函数直接相关，但它们具有一个共性：优化目标中的第一项用来描述划分超平面的间隔大小，另一项用来表述训练集上的误差。</p>
<p>结构风险用于描述模型的某些性质，经验风险用于描述模型与训练数据的契合程度。C用于对二者进行折中。结构风险表述了我们希望获得具有何种性质的模型，这为引入领域知识和用户意图提供了途径。另一方面，该信息有助于削减假设空间，从而降低了最小化训练误差的过拟合风险。</p>
<p><strong>支持向量回归</strong></p>
<p>对样本，传统回归模型通常直接基于模型输出与真实输出Y之间的差别来计算损失，当且晋档完全相同时，损失才为0.</p>
<p>与此不同，支持向量回归，假设我们能容忍模型输出与真实输出最多有E的偏差，即仅当差别绝对值大于E才计算损失。这相当于以模型输出为中心，构建了一个宽度为2E的间隔带，若训练样本落入此间隔带，则认为是被预测正确的。</p>
<p><strong>核方法</strong></p>
<p>给定训练样本。若不考虑偏移项b，则无论SVM还是SVR，学得的模型总能表示成核函数的线性组合。不仅如此，事实上我们有下面这个称为表示定理的更一般 结论：</p>
<p>表示定理对损失函数没有现在，对正则化项仅要求单调递增，甚至不要求Ω是凸函数，意味着对于一般的损失函数和正则化项，优化问题的最优解都可表示为核函数的线性组合，这显示出核函数的巨大威力。</p>
<p>人们发展出一系列基于核函数的学习方法，统称为核方法。最常见的，是通过核化来将线性学习器拓展为非线性学习器。下面我们以线性判别分析为例来演示如何通过核化来对其进行非线性拓展，从而得到核线性判别分析。</p>
<p>支持向量机的求解通常是借助于凸优化技术。如何提高效率，使SVM能适用于大规模数据一直是研究重点。对线性核SVM已有很多成果，例如基于割平面法的是SVM具有线性复杂度，基于随机梯度下降的Pegasos速度甚至更快，而坐标下降法则是在稀疏数据上有很高的效率。</p>
<p>支持向量机是针对二分类设计的，对多分类任务要进行专门的推广，对带结构输出的任务也已有相应的算法。</p>
<p>核函数直接决定了支持向量机与核方法的最终性能，但遗憾的是，核函数的选择是一个未决问题。多核学习使用多核核函数并通过学习获得其最优凸组合作为最终的核函数。这实际上是在借助集成学习机制。</p>
<p>替代损失函数在机器学习中被广泛使用，但是，通过求解替代损失函数得到的是否仍是原问题的解？这在理论上称为替代损失的一致性问题。</p>
<h2 id="贝叶斯分类器">贝叶斯分类器</h2>
<p><strong>贝叶斯决策论</strong></p>
<p>贝叶斯决策论是概率框架下实施决策的基本方法。对分类任务来说，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优类别标记。</p>
<p>对每个样本x，若h能最小化总体风险，则总体风险也将被最小化。这就产生了贝叶斯判定准则：为最小化总体风险，只需在每个样本上选择那个能使条件风险最小的类别标记。此时H称为贝叶斯最优分类器，与之对应的总体风险称为贝叶斯风险。1-RH反映了分类器所能达到的最好性能，即通过机器学习所能产生的模型精度的理论上限。</p>
<p>具体来说，若目标是最小化分类错误，则误判损失。</p>
<p>最小化分类错误率的贝叶斯最优分类器是，对每个样本选择能使后验概率最大的类别标记。</p>
<p>不难看出，欲使用贝叶斯判定准则来最小化决策风险，首先要获得后验概率P。然而，在现实任务中，这通常难以直接获得。从这个角度来看，机器学习所要实现的是基于有限训练样本集尽可能准确地估计出后验概率P,大体来说，有两种主要策略，给定x，直接建模P来预测c，这样得到的是判别式模型。也可先对联合概率分布建模，由此得到P，这样得到的是生成式模型。显然，前面介绍的决策树、BP神经网络、支持向量机都可归入判别式模型的范畴。</p>
<p>估计类条件概率的一种常用策略是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。我们的任务是利用训练集D估计参数。</p>
<p>事实上概率模型的训练过程就是参数估计过程。对于参数估计，统计学界的两个学派分别提供了不同的解决方案：频率主义学派认为参数虽然未知，但却是客观存在的固定值，因此可通过优化似然函数等准则来确定参数值；贝叶斯学派则认为参数是未观察到的随机变量，其本身也可有分布，因此，可假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布。这是根据采样来估计概率分布参数的经典方法。</p>
<p>极大似然估计是试图在所有可能取值中，找到一个能使数据出现的可能性最大的值。</p>
<p>连乘操作易造成下溢.</p>
<p>极大似然估计得到的正太分布均值就是样本均值。参数化的方法虽然能使类条件概率估计变得相对简单，但估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在真实数据分布。</p>
<p>在现实生活中。，欲作出能较好地接近潜在真实分布的假设，往往需在一定程度上利用关于应用任务本身的经验知识，否则仅靠猜测来假设分布形式，很可能产生误导性的结果。</p>
<h2 id="朴素贝叶斯分类器">朴素贝叶斯分类器</h2>
<p>基于贝叶斯公式来估计后验概率的主要困难在于：类条件概率是所有属性上的联合概率，难以从有限的训练样本直接估计得到。为避开这个障碍，朴素贝叶斯分类器采用了属性条件独立性假设。对已知类别，假设每个属性独立地对分类结果产生影响。朴素贝叶斯分类器的训练过程就是基于训练集D来估计类先验概率，并为每个属性估计条件概率。</p>
<p>令D表示训练集D中第C类样本组成的集合，若有充足的独立同分布样本，则可容易地估计出类先验概率。</p>
<p>若P好瓜大于P坏瓜，则可判定为好瓜。若某个属性值在训练集没有与某个类同时出现过，假如对连乘式计算得到概率值为0.哪怕在其他属性明显像好瓜，分类的结果都将是好瓜等于否，这显然不太合理。</p>
<p>为了避免其他属性携带的信息被训练集中未出现的属性值抹去，在估计概率值时通常要进行平滑，常用拉普拉斯修正。</p>
<p>拉普拉斯修正避免了因训练集样本不充分而导致概率估值为零的问题，并且在训练集变大时，修正过程所引入的先验的影响也会变得可忽略，使得估值渐渐趋于实际概率值。</p>
<p>在现实任务中，朴素贝叶斯分类器有多种使用方式。例如，若任务对预测速度要求较高，则对给定训练集，可将朴素贝叶斯法分类器涉及的所有概率估值事先计算好存储起来。，这样在进行预测时只需查表即可进行判别；若任务数据更替频繁，则可采用懒惰学习方式，先不进行任何训练，待收到预测请求时，再根据当前数据集进行概率估值，若数据不断增加，则可在现有估值基础上，仅对新增样本的属性值所涉及的概率估值进行计数修正，即可实现增量学习。</p>
<p><strong>半朴素贝叶斯分类器</strong></p>
<p>为了降低贝叶斯公式估计后验概率的困难，朴素贝叶斯分类器采用了属性条件独立性假设，但在现实任务中这个假设往往很难成立。于是，人们尝试对属性条件独立性假设进行一定程度的放松，由此产生了一类称为半朴素贝叶斯分类器的学习方法。</p>
<p>半朴素贝叶斯分类器的基本想法是适当考虑一部分属性间的相互依赖信息，从而既不需要进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系。独依赖估计是半朴素贝叶斯分类器最常用的一种策略。顾名思义，所谓独依赖就是假设每个属性在类别之外最多仅依赖于一个其他属性。</p>
<p>问题的关键就转化为如何确定每个属性的父属性，不同的做法产生不同的独依赖分类器。</p>
<p>最直接的做法是假设所有属性都依赖于同一个属性，称为超父，然后通过交叉验证等模型选择方法来确定超父属性，由此形成了SPODE方法。</p>
<p>TAN则是在最大带权生成树算法的基础上，通过以下步骤将属性间的依赖关系约简为树形结构。</p>
<p>1.计算任意两个属性之间的条件互信息</p>
<p>2.以属性为结点构建完全图，任意两个结点之间边的权重设为I。</p>
<p>3.构建此完全图的最大带权生成树，挑选根变量，将边置为有向</p>
<p>4.加入类别结点y，增加从y到每个属性的有向边。</p>
<p>容易看出，条件互信息I刻画了属性在已知类别情况下的相关性，因此通过最大生成树算法，TAN实际上仅保留了强相关属性之间的依赖性。</p>
<p>AODE是一种基于集成学习机制、更为强大的独依赖分类器。与SPODE通过模型选择确定超父属性不同，AODE尝试将每个属性作为超父来构建SPODE，然后将那些具有足够训练数据支撑的SPODE集成起来作为最终结果。与朴素贝叶斯分类器类似，AODE的训练过程也是计数，即在训练数据集上对符合条件的样本进行计数的过程。与朴素贝叶斯分类器相似，AODE无需模型选择，既能通过预计算节省预测时间，也能采取懒惰学习方式在预测时再进行计数，并且易于实现增量学习。</p>
<p>既然将属性条件独立性假设放松为独依赖假设可能获得泛化性能的提升。那么，能否通过考虑属性间的高阶依赖来进一步提升泛化性能呢。若训练数据非常充分，泛化性能也可能提审，但在有限样本条件下，则又陷入估计高阶联合概率泥沼。</p>
<p><strong>贝叶斯网</strong></p>
<p>贝叶斯网也叫信念网，它借助有向无环图来刻画属性之间的依赖关系，并使用条件概率表来描述属性的联合概率分布。</p>
<p>具体来说，一个贝叶斯网B由结构G和参数Θ两部分构成，网络结构G是一个有向无环图，其每个结点对应于一个属性，若两个属性有直接依赖关系，则它们由一条边连接起来，参数Θ定量描述这种依赖关系。</p>
<p>贝叶斯网结构有效地表达了属性间的条件独立性。给定父结点，贝叶斯网假设每个属性与它的非后裔属性独立。于是B将联合概率分布定义。</p>
<p>找出有向图中所有的V型结构，在V型结构的两个父节点之间加上一条无向边</p>
<p>将所有有向边改为无向边。</p>
<p>这时的无向边就是道德图。</p>
<p><strong>学习</strong></p>
<p>若网络结构已知，即属性间的依赖关系已知，则贝叶斯网的学习过程相对简单只需通过对训练样本计数，估计出每个结点的条件概率表即可。但在现实中，往往不清楚网络结构。贝叶斯网学习的首要任务就是根据训练数据集来找出结构最恰当的贝叶斯网。评分搜索是求解这一问题的常用办法。具体来说，我们先定义一个评分函数，以此来评估贝叶斯网与训练数据的契合程度，然后基于这个评分函数来寻找结构最优的贝叶斯网。显然评分函数引入了关于我们希望获得什么样贝叶斯网的归纳偏好。</p>
<p>常用评分函数通常是基于信息论准则，此类准则将学习问题看作一个数据压缩任务，学习的目标是找到一个能以最短编码长度描述训练数据的模型，此时编码长度包括描述模型自身所需的编码位数和使用该模型描述数据所需的编码位数。对贝叶斯网学习而言，模型就是一个贝叶斯网，同时，每个贝叶斯网描述了一个在训练数据上的概率分布，自有一套编码机制能使那些经常出现的样本有更短的编码。于是，我们应该选择那个综合编码长度最短的贝叶斯网，这就是最小描述长度准则。第一项是计算编码贝叶斯网B所需的编码位数。第二项是计算B所对应的概率分布对D描述得多好。于是，学习任务就转化为一个优化任务，即寻找一个贝叶斯网B，使评分函数最小。</p>
<p>为了最小化评分函数，只需对网络结构进行搜索，而候选结构的最优参数可直接在训练集计算得到。</p>
<p>不幸的是，从所有可能的网络结构空间搜索最优贝叶斯结构是一个NP难问题，难以快速求解。有两种常用的策略能在有限时间内求得近似解：第一种是贪心法，例如从某个网络结构出发，每次调整一条边，直到评分函数值不再降低为止，第二种是通过网络结构施加约束来削减搜索空间，例如将网络结构限定为树形结构等。</p>
<p><strong>推断</strong></p>
<p>贝叶斯网训练好之后就能用来回答查询，即通过一些属性变量的观测值来推测其他属性变量的取值。例如在西瓜问题中，若我们通过已知变量观测值来推测待查询变量的过程称为推断，已知变量观测值称为证据。</p>
<p>最理想的是直接根据贝叶斯网定义的联合概率分布来精确计算后验概率，不幸的是，精确推断被证明是NP难的，换言之，当网络结点较多、连接稠密时，难以进行精确推断，此时需借助近似推断，通过降低精度要求，在有限时间内求得近似解。在现实应用中，贝叶斯网的近似推断常采用吉布斯采样来完成，这是一种随机采样方法。</p>
<p>吉布斯采样算法先随机产生一个与证据一致的样本作为初始点，然后每步从当前样本出发产生下一个样本。具体来说，在t次采样中，算法先假设qt=qt-1，然后对非证据变量逐个采样改变其取值，采样概率根据贝叶斯网B和其他变量的当前取值计算获得，假定经过T次采样得到的与q一致的样本共有n个，则可近似估算出后验概率。可近似估算出N/T</p>
<p>实质上，吉布斯采样是在贝叶斯网所有变量的联合状态空间与证据一致的子空间中进行随机漫步，每一步仅依赖于前一步的状态，这是一个马尔可夫链。在一定条件下，无论从什么初始状态开始，马尔可夫链第t步的状态分布在t趋于正无穷时，必收敛于一个平稳分布，对于吉布斯采样来说，这个分布恰好是P，因此在T很大时，吉布斯采样相当于根据P采样，从而保证了式收敛于后验概率。因此在T很大时，吉布斯采样相当于根据P采样，从而保证了式收敛于P。</p>
<p>需注意的是，由于马尔可夫链通常需很长时间才能趋于平稳分布，因此吉布斯采样算法的收敛速度较慢。此外，若贝叶斯网中存在极端概率0或1，则不能保证马尔可夫链存在平稳分布，此时吉布斯采样会给出错误估计结果。</p>
<p>EM<strong>算法</strong></p>
<p>之前一直假设训练样本所有属性变量的值都已被观测到，即训练样本是完整的，但在现实应用中往往会遇到不完整的训练样本。在这种存在未观测变量的情形下，是否仍能对模型参数进行估计呢？</p>
<p>未观测变量的学名是隐变量，令X表示已观测变量集，Z表示隐变量集。</p>
<p>EM算法是常用的估计参数隐变量的利器，它是一种迭代式的方法，其基本思想若参数已知，则可根据训练数据推断出最优隐变量Z的值，反之若Z的值已知，则可方便的对参数做极大似然估计。</p>
<p>以初始值为起点，可迭代执行以下步骤直至收敛：</p>
<p>1.基于Θ推断隐变量Z的期望</p>
<p>2.基于已观测变量X和Z对参数做极大似然估计。</p>
<p>这就是EM算法的原型。</p>
<p>M步：寻找参数最大化期望似然。</p>

    </div>
    <div class="post-footer">
      
    </div>
    <div class="post-comment">
      
      


<span id="/post/2022%E5%B9%B41%E6%9C%8826/" class="leancloud_visitors" data-flag-title="2022年1月26">
    <span class="post-meta-item-text">文章阅读量 </span>
    <span class="leancloud-visitors-count">1000000</span>
    <p></p>
  </span>
<div id="vcomments"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<script type="text/javascript">
  new Valine({
    el: '#vcomments' ,
    appId: 'cScQmclMsD4OOWCclCP1pNsz-gzGzoHsz',
    appKey: 'dtSaHLmdH3J4ICVVInYg9YFM',
    notify:  false ,
  verify:  false ,
  avatar:'mm',
    placeholder: '说点什么吧...',
    visitor:  true 
  });
</script>

    </div>
  </article>

    </main>
  </body>
</html>
