<!doctype html>
<html lang="en-us">
  <head>
    <title> // Zereal-宋致远技术博客</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.72.0" />
    <meta name="viewport" content="width=device-width, initial-scale=0.75，minimum-scale=0.75, maximum-scale=0.75, user-scalable=no" />
    <meta name="author" content="Zereal" />
    <meta name="description" content="" />
    <meta name="referrer" content="never"/>
    <link rel="stylesheet" href="https://zereals7.github.io/css/main.min.f90f5edd436ec7b74ad05479a05705770306911f721193e7845948fb07fe1335.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="对常见的机器学习算法进行汇总，主要用到的参考书籍为《机器学习实战》、周志华的《机器学习》以及李航的《统计学习方法》。
一、K邻近算法 1、主要思想 K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例。特征空间中的每个样本都可以用与它最近的这K个实例来代表。分类的过程是：k个实例进行投票，将待预测样本归入得票最多的类别里面。
不同的K值分类的结果会有偏差，因此如何确定K值呢？k为多少效果最好呢？所谓的最近邻又是如何来判断给定呢？
k太小会导致过拟合，很容易将一些噪声学习到模型中，而忽略了数据真实的分布！
如果我们选取较大的k值，就相当于用较大邻域中的训练数据进行预测，这时与输入实例较远的（不相似）训练实例也会对预测起作用，使预测发生错误，k值的增大意味着整体模型变得简单。
**如果k=N（N为训练样本的个数）,那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型是不是非常简单，这相当于你压根就没有训练模型呀！**直接拿训练数据统计了一下各个数据的类别，找最大的而已！这个时候，模型过于简单，完全忽略训练数据实例中的大量有用信息，是不可取的。
k值既不能过大，也不能过小
一般对于二分类问题来说，把K设置为奇数是容易防止平局的现象。但对于多分类来说，设置为奇数未必一定能够防平局。
我们也一般都用欧式距离来衡量我们高维空间中俩点的距离。
欧式距离与曼哈顿距离区别 欧式距离：平方差求和再开方
曼哈顿距离：坐标差的绝对值求和
一般用欧式距离而非曼哈顿距离的原因：欧式距离可适用于不同空间，表示不同空间点之间的距离； 曼哈顿距离则只计算水平或垂直距离，有维度的限制。
特征的数值必须进行归一化，应该让每个特征都是同等重要的，这也是我们要归一化的原因。
k近邻最简单的实现方法是线性扫描，即计算输入实例与每一个训练实例的距离，当训练集很大时，计算非常耗时，所以这种方法是不可行的。 那么如何快速找到样本点的最近邻？
kd树（K-dimension tree)是一种二叉树，表示对特征空间的一个划分； 构造kd’树的过程是不断的用垂直于坐标轴的超平面特征空间划分，构造一系列的矩形区域，kd树的每一个结点对应于一个k维超矩形区域。 利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。
KNN算法算是监督学习还是无监 督学习呢？首先来看一下监督学习和无监督学习的定义。对于监督学习，数据都有明确的label（分类针对离散分布，回归针对连续分布），根据机器学习产生的模型可以将新数据分到一个明确的类或得到一个预测值。对于非监督学习，数据没有label，机器学习出的模型是从数据中提取出来的pattern（提取 决定性特征或者聚类等）。例如聚类是机器根据学习得到的模型来判断新数据“更像”哪些原数据集合。KNN算法用于分类时，每个训练数据都有明确的 label，也可以明确的判断出新数据的label，KNN用于回归时也会根据邻居的值预测出一个明确的值，因此KNN属于监督学习。
2、实现代码 from numpy import *#科学计算包 from numpy import tile from numpy import zeros import operator #运算符模块 import importlib import sys importlib.reload(sys) def createDataSet(): group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) labels = [&#39;A&#39;,&#39;A&#39;,&#39;B&#39;,&#39;B&#39;] return group,labels def classify0(inX, dataSet, labels, k): dataSetSize = dataSet.shape[0] #距离计算 diffMat = tile(inX, (dataSetSize,1)) - dataSet sqDiffMat = diffMat**2 #平方 sqDistances = sqDiffMat."/>

    <meta property="og:title" content="" />
<meta property="og:description" content="对常见的机器学习算法进行汇总，主要用到的参考书籍为《机器学习实战》、周志华的《机器学习》以及李航的《统计学习方法》。
一、K邻近算法 1、主要思想 K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例。特征空间中的每个样本都可以用与它最近的这K个实例来代表。分类的过程是：k个实例进行投票，将待预测样本归入得票最多的类别里面。
不同的K值分类的结果会有偏差，因此如何确定K值呢？k为多少效果最好呢？所谓的最近邻又是如何来判断给定呢？
k太小会导致过拟合，很容易将一些噪声学习到模型中，而忽略了数据真实的分布！
如果我们选取较大的k值，就相当于用较大邻域中的训练数据进行预测，这时与输入实例较远的（不相似）训练实例也会对预测起作用，使预测发生错误，k值的增大意味着整体模型变得简单。
**如果k=N（N为训练样本的个数）,那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型是不是非常简单，这相当于你压根就没有训练模型呀！**直接拿训练数据统计了一下各个数据的类别，找最大的而已！这个时候，模型过于简单，完全忽略训练数据实例中的大量有用信息，是不可取的。
k值既不能过大，也不能过小
一般对于二分类问题来说，把K设置为奇数是容易防止平局的现象。但对于多分类来说，设置为奇数未必一定能够防平局。
我们也一般都用欧式距离来衡量我们高维空间中俩点的距离。
欧式距离与曼哈顿距离区别 欧式距离：平方差求和再开方
曼哈顿距离：坐标差的绝对值求和
一般用欧式距离而非曼哈顿距离的原因：欧式距离可适用于不同空间，表示不同空间点之间的距离； 曼哈顿距离则只计算水平或垂直距离，有维度的限制。
特征的数值必须进行归一化，应该让每个特征都是同等重要的，这也是我们要归一化的原因。
k近邻最简单的实现方法是线性扫描，即计算输入实例与每一个训练实例的距离，当训练集很大时，计算非常耗时，所以这种方法是不可行的。 那么如何快速找到样本点的最近邻？
kd树（K-dimension tree)是一种二叉树，表示对特征空间的一个划分； 构造kd’树的过程是不断的用垂直于坐标轴的超平面特征空间划分，构造一系列的矩形区域，kd树的每一个结点对应于一个k维超矩形区域。 利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。
KNN算法算是监督学习还是无监 督学习呢？首先来看一下监督学习和无监督学习的定义。对于监督学习，数据都有明确的label（分类针对离散分布，回归针对连续分布），根据机器学习产生的模型可以将新数据分到一个明确的类或得到一个预测值。对于非监督学习，数据没有label，机器学习出的模型是从数据中提取出来的pattern（提取 决定性特征或者聚类等）。例如聚类是机器根据学习得到的模型来判断新数据“更像”哪些原数据集合。KNN算法用于分类时，每个训练数据都有明确的 label，也可以明确的判断出新数据的label，KNN用于回归时也会根据邻居的值预测出一个明确的值，因此KNN属于监督学习。
2、实现代码 from numpy import *#科学计算包 from numpy import tile from numpy import zeros import operator #运算符模块 import importlib import sys importlib.reload(sys) def createDataSet(): group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) labels = [&#39;A&#39;,&#39;A&#39;,&#39;B&#39;,&#39;B&#39;] return group,labels def classify0(inX, dataSet, labels, k): dataSetSize = dataSet.shape[0] #距离计算 diffMat = tile(inX, (dataSetSize,1)) - dataSet sqDiffMat = diffMat**2 #平方 sqDistances = sqDiffMat." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://zereals7.github.io/post/ml%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5/" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://zereals7.github.io/"><img class="app-header-avatar" src="/avatar.jpg" alt="Zereal" /></a>
      <h1>Zereal</h1>
      <p>Java coder</p>
      <div class="app-header-social">
        
          <a target="_blank" href="https://github.com/ZerealS7" rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg></a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title"></h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Jan 1, 0001
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          27 min read
        </div></div>
    </header>
    <div class="post-content">
      <p>对常见的机器学习算法进行汇总，主要用到的参考书籍为《机器学习实战》、周志华的《机器学习》以及李航的《统计学习方法》。</p>
<h2 id="一k邻近算法">一、K邻近算法</h2>
<h3 id="1主要思想">1、主要思想</h3>
<p>K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例<strong>最邻近</strong>的K个实例。特征空间中的每个样本都可以用与它最近的这K个实例来代表。分类的过程是：k个实例进行投票，将待预测样本归入得票最多的类别里面。</p>
<p>不同的K值分类的结果会有偏差，因此<strong>如何确定K值呢？k为多少效果最好呢？所谓的最近邻又是如何来判断给定呢？</strong></p>
<p>k太小会导致<strong>过拟合</strong>，<strong>很容易将一些噪声学习到模型中，而忽略了数据真实的分布！</strong></p>
<p><strong>如果我们选取较大的k值，就相当于用较大邻域中的训练数据进行预测，这时与输入实例较远的（不相似）训练实例也会对预测起作用，使预测发生错误，k值的增大意味着整体模型变得简单。</strong></p>
<p>**如果k=N（N为训练样本的个数）,那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型是不是非常简单，这相当于你压根就没有训练模型呀！**直接拿训练数据统计了一下各个数据的类别，找最大的而已！<strong>这个时候，模型过于简单，完全忽略训练数据实例中的大量有用信息，是不可取的。</strong></p>
<p><strong>k值既不能过大，也不能过小</strong></p>
<p>一般对于二分类问题来说，把K设置为奇数是容易防止平局的现象。但对于多分类来说，设置为奇数未必一定能够防平局。</p>
<p>我们也一般都用欧式距离来衡量我们高维空间中俩点的距离。</p>
<p>欧式距离与曼哈顿距离区别 欧式距离：平方差求和再开方</p>
<p>曼哈顿距离：坐标差的绝对值求和</p>
<p>一般用欧式距离而非曼哈顿距离的原因：欧式距离可适用于不同空间，表示不同空间点之间的距离；  曼哈顿距离则只计算水平或垂直距离，有维度的限制。</p>
<p>特征的数值必须进行归一化，应该让每个特征都是同等重要的，这也是我们要归一化的原因。</p>
<p>k近邻最简单的实现方法是线性扫描，即计算输入实例与每一个训练实例的距离，当训练集很大时，计算非常耗时，所以这种方法是不可行的。
那么如何快速找到样本点的最近邻？</p>
<p>kd树（K-dimension tree)是一种二叉树，表示对特征空间的一个划分；
构造kd’树的过程是不断的用垂直于坐标轴的超平面特征空间划分，构造一系列的矩形区域，kd树的每一个结点对应于一个k维超矩形区域。
利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。</p>
<p>KNN算法算是监督学习还是无监 督学习呢？首先来看一下监督学习和无监督学习的定义。对于监督学习，数据都有明确的label（分类针对离散分布，回归针对连续分布），根据机器学习产生的模型可以将新数据分到一个明确的类或得到一个预测值。对于非监督学习，数据没有label，机器学习出的模型是从数据中提取出来的pattern（提取 决定性特征或者聚类等）。例如聚类是机器根据学习得到的模型来判断新数据“更像”哪些原数据集合。KNN算法用于分类时，每个训练数据都有明确的 label，也可以明确的判断出新数据的label，KNN用于回归时也会根据邻居的值预测出一个明确的值，因此KNN属于监督学习。</p>
<h3 id="2实现代码">2、实现代码</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> numpy <span style="color:#f92672">import</span> <span style="color:#f92672">*</span><span style="color:#75715e">#科学计算包</span>
<span style="color:#f92672">from</span> numpy <span style="color:#f92672">import</span> tile
<span style="color:#f92672">from</span> numpy <span style="color:#f92672">import</span> zeros
<span style="color:#f92672">import</span> operator     <span style="color:#75715e">#运算符模块</span>
<span style="color:#f92672">import</span> importlib
<span style="color:#f92672">import</span> sys
importlib<span style="color:#f92672">.</span>reload(sys)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">createDataSet</span>():
    group <span style="color:#f92672">=</span> array([[<span style="color:#ae81ff">1.0</span>,<span style="color:#ae81ff">1.1</span>],[<span style="color:#ae81ff">1.0</span>,<span style="color:#ae81ff">1.0</span>],[<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">0</span>],[<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">0.1</span>]])
    labels <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;A&#39;</span>,<span style="color:#e6db74">&#39;A&#39;</span>,<span style="color:#e6db74">&#39;B&#39;</span>,<span style="color:#e6db74">&#39;B&#39;</span>]
    <span style="color:#66d9ef">return</span> group,labels

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">classify0</span>(inX, dataSet, labels, k):
    dataSetSize <span style="color:#f92672">=</span> dataSet<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
    <span style="color:#75715e">#距离计算</span>
    diffMat <span style="color:#f92672">=</span> tile(inX, (dataSetSize,<span style="color:#ae81ff">1</span>)) <span style="color:#f92672">-</span> dataSet
    sqDiffMat <span style="color:#f92672">=</span> diffMat<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>      <span style="color:#75715e">#平方</span>
    sqDistances <span style="color:#f92672">=</span> sqDiffMat<span style="color:#f92672">.</span>sum(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)     <span style="color:#75715e">#根号下平方相加</span>
    distances <span style="color:#f92672">=</span> sqDistances<span style="color:#f92672">**</span><span style="color:#ae81ff">0.5</span>    <span style="color:#75715e">#根号</span>
    sortedDistIndicies <span style="color:#f92672">=</span> distances<span style="color:#f92672">.</span>argsort()    <span style="color:#75715e">#排序</span>
    classCount<span style="color:#f92672">=</span>{}
    <span style="color:#75715e">#选择距离最小的k个点</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(k):
        voteIlabel <span style="color:#f92672">=</span> labels[sortedDistIndicies[i]]
        classCount[voteIlabel] <span style="color:#f92672">=</span> classCount<span style="color:#f92672">.</span>get(voteIlabel,<span style="color:#ae81ff">0</span>) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
        <span style="color:#75715e">#排序，将classCount字典分解为元祖列表，导入itemgeeter方法，按照第二个元素的次序对元祖进行排序</span>
        <span style="color:#75715e">#此处排序为逆序，即从大到小排序，最后返回发生频率最高的元素标签。</span>
        sortedClassCount <span style="color:#f92672">=</span> sorted(classCount<span style="color:#f92672">.</span>items(),key<span style="color:#f92672">=</span>operator<span style="color:#f92672">.</span>itemgetter(<span style="color:#ae81ff">1</span>),reverse<span style="color:#f92672">=</span>True)
        <span style="color:#66d9ef">return</span> sortedClassCount[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>]
    <span style="color:#75715e"># 为预测数据所在分类：kNN.classify0([0,0], group, labels, 3)</span>

<span style="color:#75715e"># mat()函数可以将数组(array)转化为矩阵（matrix）</span>
<span style="color:#75715e"># randMat = mat(random.rand(4,4))</span>
<span style="color:#75715e"># 求逆矩阵：randMat.I</span>
<span style="color:#75715e"># 存储逆矩阵：invRandMat = randMat.I</span>
<span style="color:#75715e"># 矩阵乘法：randMat*invRandMat</span>
<span style="color:#75715e"># 求误差值：myEye = randMat*invRandMat</span>
        <span style="color:#75715e">#myEye - eye(4)</span>
        <span style="color:#75715e">#eye(4)创建4*4的单位矩阵</span>
<span style="color:#75715e"># 使用createDataSet()函数，创建数据集和标签</span>
<span style="color:#75715e"># 创建变量group和labels：group,labels = kNN.createDataSet()</span>
<span style="color:#75715e"># labels包含的元素个数 = group矩阵的行数</span>
<span style="color:#75715e"># 输入变量名字检验是否正确：group和labels</span>
<span style="color:#75715e">#</span>
<span style="color:#75715e">#</span>
<span style="color:#75715e"># 准备数据：从文本文件中解析数据</span>
<span style="color:#75715e"># 在kNN.py中创建名为file2matrix的函数，处理输入格式问题</span>
<span style="color:#75715e"># 该函数的输入为文件名字符串，输出为训练样本矩阵和类标签向量</span>
<span style="color:#75715e"># 将文本记录到转换Numpy的解析程序</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">file2matrix</span>(filename):
    fr <span style="color:#f92672">=</span> open(filename)
    arrayOLines <span style="color:#f92672">=</span> fr<span style="color:#f92672">.</span>readlines()
    numberOfLines <span style="color:#f92672">=</span> len(arrayOLines)    <span style="color:#75715e">#得到文件行数</span>
    returnMat <span style="color:#f92672">=</span> zeros((numberOfLines,<span style="color:#ae81ff">3</span>))    <span style="color:#75715e">#创建返回的Numpy矩阵</span>
    classLabelVector <span style="color:#f92672">=</span> []
    index <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> arrayOLines:    <span style="color:#75715e">#解析文件数据列表</span>
        line <span style="color:#f92672">=</span> line<span style="color:#f92672">.</span>strip()     <span style="color:#75715e">#使用line.strip（）截取掉所有的回车字符</span>
        listFromLine <span style="color:#f92672">=</span> line<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#39;</span>)     <span style="color:#75715e">#使用tab字符\t将上一步得到的整行数据分割成一个元素列表</span>
        returnMat[index,:] <span style="color:#f92672">=</span> listFromLine[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">3</span>]      <span style="color:#75715e">#选取前三个元素，存储到特征矩阵中</span>
        classLabelVector<span style="color:#f92672">.</span>append(int(listFromLine[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]))      <span style="color:#75715e">#-1表示列表中的最后一列元素，存储到向量classLabelVector中</span>
        index <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
        <span style="color:#66d9ef">return</span> returnMat,classLabelVector

<span style="color:#75715e">#准备数据：归一化数值</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">autoNorm</span>(dataSet):      <span style="color:#75715e">#autoNorm()函数可以自动将数字特征值转换为0到1的区间</span>
    minVals <span style="color:#f92672">=</span> dataSet<span style="color:#f92672">.</span>min(<span style="color:#ae81ff">0</span>)
    maxVals <span style="color:#f92672">=</span> dataSet<span style="color:#f92672">.</span>max(<span style="color:#ae81ff">0</span>)    <span style="color:#75715e">#ddataSet.max(0)中的参数0使得函数可以从列中选取最小值</span>
    ranges <span style="color:#f92672">=</span> maxVals <span style="color:#f92672">-</span> minVals
    normDataSet <span style="color:#f92672">=</span> zeros(shape(dataSet))
    m <span style="color:#f92672">=</span> dataSet<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
    <span style="color:#75715e">#newValue = (oldValue-min)/(max-min)，该公式可以将任意取值范围的特征值转换为0到1区间内的值</span>
    <span style="color:#75715e">#tile()函数将变量内容复制成输入矩阵同样大小的矩阵（具体特征值相除）</span>
    <span style="color:#75715e">#在numpy库中，矩阵除法需要使用函数linalg.solve(matA,matB)</span>
    normDataSet <span style="color:#f92672">=</span> dataSet <span style="color:#f92672">-</span> tile(minVals, (m,<span style="color:#ae81ff">1</span>))
    normDataSet <span style="color:#f92672">=</span> normDataSet<span style="color:#f92672">/</span>tile(ranges, (m,<span style="color:#ae81ff">1</span>))
    <span style="color:#66d9ef">return</span> normDataSet, ranges, minVals

<span style="color:#75715e">#测试算法：作为完整程序验证分类器</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">datingClassTest</span>():
    hoRatio <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.10</span>  <span style="color:#75715e">#设置测试集比重，前10%作为测试集，后90%作为训练集</span>
    datingDataMat,datingLabels <span style="color:#f92672">=</span> file2matrix(<span style="color:#e6db74">&#39;datingTestSet.txt&#39;</span>)
    normMat, ranges, minVals <span style="color:#f92672">=</span> autoNorm(datingDataMat)
    m <span style="color:#f92672">=</span> normMat<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]    <span style="color:#75715e">#得到样本数量m</span>
    numTestVecs <span style="color:#f92672">=</span> int(m<span style="color:#f92672">*</span>hoRatio)    <span style="color:#75715e">#得到测试集最后一个样本的位置</span>
    errorCount <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>    <span style="color:#75715e">#初始化定义错误个数为0</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(numTestVecs):
        <span style="color:#75715e">#测试集中元素逐一放进分类器测试，k = 3</span>
        classifierResult <span style="color:#f92672">=</span> classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],<span style="color:#ae81ff">3</span>)
        <span style="color:#75715e">#输出分类结果与实际label</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;the classifier came back with: </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">, the real answer is: </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">%</span> (classifierResult, datingLabels[i]))
        <span style="color:#75715e">#若预测结果与实际label不同，则errorCount+1</span>
        <span style="color:#66d9ef">if</span> (classifierResult <span style="color:#f92672">!=</span>datingLabels[i]): errorCount <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1.0</span>
        <span style="color:#75715e">#输出错误率 = 错误的个数 / 总样本个数</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;the total error rate is: </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (errorCount<span style="color:#f92672">/</span>float(numTestVecs)))


<span style="color:#75715e">#约会网站预测数据</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">classifyPersion</span>():
    resultList <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;not at all&#39;</span>,<span style="color:#e6db74">&#39;in small doses&#39;</span>,<span style="color:#e6db74">&#39;in large doses&#39;</span>]
    <span style="color:#75715e">#input()函数允许用户输入文本行命令并返回用户所输入的命令</span>
    percentTats <span style="color:#f92672">=</span> float(input(<span style="color:#e6db74">&#34;percentage of time spent playing video games?&#34;</span>))
    ffMiles <span style="color:#f92672">=</span> float(input(<span style="color:#e6db74">&#34;frequent year?&#34;</span>))
    iceCream <span style="color:#f92672">=</span> float(input(<span style="color:#e6db74">&#34;liters years?&#34;</span>))
    datingDataMat,datingLabels <span style="color:#f92672">=</span> file2matrix(<span style="color:#e6db74">&#39;datingTestSet2.txt&#39;</span>)
    normMat, ranges, minVals <span style="color:#f92672">=</span> autoNorm(datingDataMat)
    inArr <span style="color:#f92672">=</span> array([ffMiles,percentTats, iceCream])
    classifierResult <span style="color:#f92672">=</span> classify0((inArr<span style="color:#f92672">-</span>minVals)<span style="color:#f92672">/</span>ranges,normMat,datingLabels,<span style="color:#ae81ff">3</span>)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;you like person:&#34;</span>,resultList[classifierResult <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>])


<span style="color:#75715e">#准备数据：将图像转换为测试向量</span>
<span style="color:#75715e">#img2vector函数，将图像转换为向量：该函数创建1*2014的numpy数组，</span>
<span style="color:#75715e">#然后打开给定的文件，循环读出文件的前32行，并将每行的头32个字符值存储在numpy数组中，最后返回数组</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">img2vector</span>(filename):
    returnVect <span style="color:#f92672">=</span> zeros((<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1024</span>))
    fr <span style="color:#f92672">=</span> open(filename)
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">32</span>):
        lineStr <span style="color:#f92672">=</span> fr<span style="color:#f92672">.</span>readline()
        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">32</span>):
            returnVect[<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">32</span><span style="color:#f92672">*</span>i<span style="color:#f92672">+</span>j] <span style="color:#f92672">=</span> int(lineStr[j])
            <span style="color:#66d9ef">return</span> returnVect

<span style="color:#75715e">#测试算法：识别手写数字</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">handwritingClassTest</span>():
    hwLabels <span style="color:#f92672">=</span> []
    trainingFileList <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>listdir(<span style="color:#e6db74">&#39;trainingDigits&#39;</span>)
    m <span style="color:#f92672">=</span> len(trainingFileList)
    trainingMat <span style="color:#f92672">=</span> zeros((m,<span style="color:#ae81ff">1024</span>))
    <span style="color:#75715e">#文件名下划线_左边的数字是标签</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(m):
        fileNameStr <span style="color:#f92672">=</span> trainingFileList[i]
        fileStr <span style="color:#f92672">=</span> fileNameStr<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;.&#34;</span>)[<span style="color:#ae81ff">0</span>]
        classNumStr <span style="color:#f92672">=</span> int(fileStr<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;_&#39;</span>)[<span style="color:#ae81ff">0</span>])
        hwLabels<span style="color:#f92672">.</span>append(classNumStr)
        trainingMat[i,:] <span style="color:#f92672">=</span> img2vector(<span style="color:#e6db74">&#39;trainingDigits/</span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> fileNameStr)
    testFileList <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>listdir(<span style="color:#e6db74">&#39;trainingDigits&#39;</span>)
    errorCount <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
    mTest <span style="color:#f92672">=</span> len(testFileList)
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(mTest):
        fileNameStr <span style="color:#f92672">=</span> testFileList[i]
        fileStr <span style="color:#f92672">=</span> fileNameStr<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;.&#39;</span>)[<span style="color:#ae81ff">0</span>]  <span style="color:#75715e"># take off .txt</span>
        classNumStr <span style="color:#f92672">=</span> int(fileStr<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;_&#39;</span>)[<span style="color:#ae81ff">0</span>])
        vectorUnderTest <span style="color:#f92672">=</span> img2vector(<span style="color:#e6db74">&#39;digits/testDigits/</span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> fileNameStr)
        classifierResult <span style="color:#f92672">=</span> classify0(vectorUnderTest, trainingMat, hwLabels, <span style="color:#ae81ff">3</span>)
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;the classifier came back with: </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">, the real answer is: </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (classifierResult, classNumStr))
        <span style="color:#66d9ef">if</span> (classifierResult <span style="color:#f92672">!=</span> classNumStr): errorCount <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1.0</span>
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;the total number of errors is: </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> errorCount)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;the total error rate is: </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (errorCount <span style="color:#f92672">/</span> float(mTest)))
</code></pre></div><h3 id="3优缺点及适用场景">3、优缺点及适用场景</h3>
<p><strong>KNN模型的优缺点</strong></p>
<p>一、优点</p>
<p>简单且容易实现
比较适合多分类问题
对异常值不大敏感
无需估计参数，无需训练。</p>
<p>二、缺点</p>
<p>预测精度一般，大部分问题比不上基于数的方法。</p>
<p>当样本存在范围重叠时，KNN的分类精度很低。</p>
<p>每次分类一个样本都要计算所有数据，在大数据环境下不适应。</p>
<p>懒惰算法，进行分类时计算量大，要扫描全部训练样本计算距离，内存开销大，评分慢；当样本不平衡时，如其中一个类别的样本较大，可能会导致对新样本计算近邻时，大容量样本占大多数，影响分类效果；可解释性较差，无法给出决策树那样的规则。</p>
<p><strong>KNN模型的应用场景和注意点</strong></p>
<p>一、应用场景</p>
<p>比较适合数据量小且精度要求不高的数据
比较适合不能一次性获取训练样本的情况</p>
<p>二、注意点</p>
<p>特征数据归一化
k值的选择
距离度量的方式</p>
<h2 id="二决策树">二、决策树</h2>
<h3 id="1主要思想-1">1、主要思想</h3>
<p>决策树(Decision Tree,又称为判定树)算法是机器学习中常见的一类算法，是一种以树结构(包括二叉树和多叉树)形式表达的预测分析模型。每个决策点实现一个具有离散输出的测试函数，记为分支。决策树由结点和有向边组成。结点有两种类型: 内部结点和叶节点。内部节点表示一个特征或属性，叶节点表示一个类。<strong>决策树通常有三个步骤</strong>：特征选择、决策树的生成、决策树的修剪。</p>
<p>用决策树分类：从根节点开始，对实例的某一特征进行测试，根据测试结果将实例分配到其子节点，此时每个子节点对应着该特征的一个取值，如此递归的对实例进行测试并分配，直到到达叶节点，最后将实例分到叶节点的类中。</p>
<p>决策树学习的目标：根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。</p>
<p>决策树学习的本质：从训练集中归纳出一组分类规则，或者说是由训练数据集估计条件概率模型。</p>
<p>决策树学习的损失函数：正则化的极大似然函数</p>
<p>决策树学习的测试：最小化损失函数</p>
<p>决策树学习的目标：在损失函数的意义下，选择最优决策树的问题。</p>
<p>k-近邻算法可以完成很多分类任务，但是其最大的缺点是无法给出数据的内在含义，决策树的优势在于数据形式非常容易理解。</p>
<p>决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。</p>
<p>1） 开始：构建根节点，将所有训练数据都放在根节点，选择一个最优特征，按着这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。</p>
<p>2） 如果这些子集已经能够被基本正确分类，那么构建叶节点，并将这些子集分到所对应的叶节点去。</p>
<p>3）如果还有子集不能够被正确的分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的节点，如果递归进行，直至所有训练数据子集被基本正确的分类，或者没有合适的特征为止。</p>
<p>4）每个子集都被分到叶节点上，即都有了明确的类，这样就生成了一颗决策树。</p>
<p>划分数据集的大原则是：将无序数据变得更加有序，但是各种方法都有各自的优缺点，信息论是量化处理信息的分支科学，在划分数据集前后信息发生的变化称为信息增益，获得信息增益最高的特征就是最好的选择，所以必须先学习如何计算信息增益，集合信息的度量方式称为香农熵，或者简称熵。当熵中的概率由数据估计(特别是最大似然估计)得到时，所对应的熵称为经验熵。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202161411439.png" alt="image-20220215010950724"></p>
<p><strong>信息增益</strong>：信息增益是相对于特征而言的。所以，特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202171443784.png" alt="image-20220217004156267"></p>
<p>一般地，熵H(D)与条件熵H(D|A)之差成为互信息(mutual information)。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。</p>
<p>信息增益值的大小相对于训练数据集而言的，并没有绝对意义，在分类问题困难时，也就是说在训练数据集经验熵大的时候，信息增益值会偏大，反之信息增益值会偏小，使用信息增益比可以对这个问题进行校正，这是特征选择的另一个标准。</p>
<p><strong>信息增益比</strong>：特征A 对训练数据集D的信息增益比定义为其信息增益与训练数据集D的经验熵之比。</p>
<p><strong>决策树的生成与修剪</strong>：构建决策树的算法有很多，比如C4.5、ID3和CART，这些算法在运行时并不总是在每次划分数据分组时都会消耗特征。由于特征数目并不是每次划分数据分组时都减少，因此这些算法在实际使用时可能引起一定的问题。目前我们并不需要考虑这个问题，只需要在算法开始运行前计算列的数目，查看算法是否使用了所有属性即可。</p>
<p>决策树生成算法递归地产生决策树，直到不能继续下去未为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。</p>
<p><strong>ID3算法</strong>：核心是在决策树各个结点上对应信息增益准则选择特征，递归地构建决策树。</p>
<p>具体方法是：</p>
<p>1）从根结点(root node)开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征。</p>
<p>2）由该特征的不同取值建立子节点，再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止；</p>
<p>3）最后得到一个决策树。</p>
<p>ID3相当于用极大似然法进行概率模型的选择</p>
<p><strong>递归构建决策树：</strong></p>
<p>从数据集构造决策树算法所需的子功能模块工作原理如下：得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分，第一次划分之后，数据将被向下传递到树分支的下一个节点，在此节点在此划分数据，因此可以使用递归的原则处理数据集。</p>
<p><strong>递归结束的条件是：</strong></p>
<p>程序完全遍历所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类，如果所有实例具有相同的分类，则得到一个叶子节点或者终止块，任何到达叶子节点的数据必然属于叶子节点的分类。</p>
<p>决策树生成算法递归的产生决策树，直到不能继续下去为止，这样产生的树往往对训练数据的分类很准确，但对未知测试数据的分类缺没有那么精确，即会出现过拟合现象。过拟合产生的原因在于在学习时过多的考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树，解决方法是考虑决策树的复杂度，对已经生成的树进行简化。</p>
<p>剪枝（pruning）：从已经生成的树上裁掉一些子树或叶节点，并将其根节点或父节点作为新的叶子节点，从而简化分类树模型。</p>
<p>实现方式：极小化决策树整体的损失函数或代价函数来实现</p>
<p>决策树学习的损失函数定义为：<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202161411851.png" alt="image-20220215191158812"></p>
<p>T表示叶子结点 ，Ht表示第t个叶子结点的熵，α为惩罚系数，Nt表示该叶子所含训练样例的数目。</p>
<table>
<thead>
<tr>
<th><em>C</em>(<em>T</em>)</th>
<th>表示模型对训练数据的预测误差，即模型与训练数据的拟合程度；</th>
</tr>
</thead>
<tbody>
<tr>
<td>α</td>
<td>参数α &gt; = 0 控制两者之间的影响，较大的α促使选择较简单的模型（树），较小的α 促使选择较复杂的模型（树），α=0意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。</td>
</tr>
</tbody>
</table>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202161421477.png" alt="image-20220216142141410"></p>
<p>剪枝就是当α确定时，选择损失函数最小的模型，即损失函数最小的子树。</p>
<p>当α 值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度越高；
子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好
损失函数正好表示了对两者的平衡。决策树的剪枝过程（泛化过程）就是从叶子节点开始递归，记其父节点将所有子节点回缩后的子树为Tb（分类值取类别比例最大的特征值），未回缩的子树为Ta，如果C α ( T a ) ≥ C α ( T b ) 说明回缩后使得损失函数减小了，那么应该使这棵子树回缩，递归直到无法回缩为止，这样使用“贪心”的思想进行剪枝可以降低损失函数值，也使决策树得到泛化。
可以看出，决策树的生成只是考虑通过提高信息增益对训练数据进行更好的拟合，而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。</p>
<p>预剪枝是指在决策树的生成过程中，对每个节点在划分前先进行评估，若当前的划分不能带来泛化性能的提升，则停止划分，并将当前节点标记为叶节点。</p>
<p>后剪枝是指先从训练集生成一颗完整的决策树，然后自底向上对非叶节点进行考察，若将该节点对应的子树替换为叶节点，能带来泛化性能的提升，则将该子树替换为叶节点。
那么怎么来判断是否带来泛化性能的提升那？最简单的就是留出法，即预留一部分数据作为验证集来进行性能评估。</p>
<p><strong>ID3、C4.5、CART的区别</strong></p>
<p>这三个是非常著名的决策树算法。简单粗暴来说，ID3 使用信息增益作为选择特征的准则；C4.5 使用信息增益比作为选择特征的准则；CART 使用 Gini 指数作为选择特征的准则。</p>
<p><strong>一、ID3</strong>
熵表示的是数据中包含的信息量大小。熵越小，数据的纯度越高，也就是说数据越趋于一致，这是我们希望的划分之后每个子节点的样子。</p>
<p>信息增益 = 划分前熵 - 划分后熵。信息增益越大，则意味着使用属性 a 来进行划分所获得的 “纯度提升” 越大 **。也就是说，用属性 a 来划分训练集，得到的结果中纯度比较高。</p>
<p>ID3 仅仅适用于二分类问题。ID3 仅仅能够处理离散属性。</p>
<p><strong>二、C4.5</strong></p>
<p>C4.5 克服了 ID3 仅仅能够处理离散属性的问题，以及信息增益偏向选择取值较多特征的问题，使用信息增益比来选择特征。信息增益比 = 信息增益 / 划分前熵 选择信息增益比最大的作为最优特征。</p>
<p>C4.5 处理连续特征是先将特征取值排序，以连续两个值中间值作为划分标准。尝试每一种划分，并计算修正后的信息增益，选择信息增益最大的分裂点作为该属性的分裂点。</p>
<p><strong>三、CART</strong></p>
<p>CART 与 ID3，C4.5 不同之处在于 CART 生成的树必须是二叉树。也就是说，无论是回归还是分类问题，无论特征是离散的还是连续的，无论属性取值有多个还是两个，内部节点只能根据属性值进行二分。</p>
<p>CART 的全称是分类与回归树。从这个名字中就应该知道，CART 既可以用于分类问题，也可以用于回归问题。</p>
<p>回归树中，使用平方误差最小化准则来选择特征并进行划分。每一个叶子节点给出的预测值，是划分到该叶子节点的所有样本目标值的均值，这样只是在给定划分的情况下最小化了平方误差。</p>
<p>要确定最优化分，还需要遍历所有属性，以及其所有的取值来分别尝试划分并计算在此种划分情况下的最小平方误差，选取最小的作为此次划分的依据。由于回归树生成使用平方误差最小化准则，所以又叫做最小二乘回归树。</p>
<p>分类树中，使用 Gini 指数最小化准则来选择特征并进行划分；</p>
<p>Gini 指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合不确定性越高，不纯度也越大。这一点和熵类似。另一种理解基尼指数的思路是，基尼指数是为了最小化误分类的概率。表示在样本集合中一个随机选中的样本被分错的概率。举例来说，现在一个袋子里有3种颜色的球若干个，伸手进去掏出2个球，颜色不一样的概率。<strong>Gini(D)越小，数据集D的纯度越高。</strong></p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202171521875.png" alt="image-20220217144304085"></p>
<p><strong>信息增益 vs 信息增益比</strong></p>
<p>之所以引入了信息增益比，是由于信息增益的一个缺点。那就是：信息增益总是偏向于选择取值较多的属性。信息增益比在此基础上增加了一个罚项，解决了这个问题。</p>
<ul>
<li>缺点：信息增益比偏向取值较少的特征。</li>
</ul>
<p>使用信息增益比：基于以上缺点，并不是直接选择信息增益比最大的特征，而是现在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征。</p>
<p><strong>Gini 指数 vs 熵</strong></p>
<p>既然这两个都可以表示数据的不确定性，不纯度。那么这两个有什么区别那？</p>
<p>Gini 指数的计算不需要对数运算，更加高效；
Gini 指数更偏向于连续属性，熵更偏向于离散属性。</p>
<p><strong>总结三种不同决策树算法</strong></p>
<ul>
<li>
<p><strong>ID3</strong>：取值多的属性，更容易使数据更纯，其信息增益更大。</p>
<p>训练得到的是一棵庞大且深度浅的树：不合理。</p>
</li>
<li>
<p><strong>C4.5</strong>：采用信息增益率替代信息增益。</p>
</li>
<li>
<p><strong>CART</strong>：以基尼系数替代熵，最小化不纯度，而不是最大化信息增益。</p>
</li>
</ul>
<p><strong>使用决策树进行分类</strong></p>
<p>依靠训练数据构造了决策树之后，我们可以将它用于实际数据的分类。在执行数据分类时，需要决策树以及用于构造树的标签向量。然后，程序比较测试数据与决策树上的数值，递归执行该过程直到进入叶子结点；最后将测试数据定义为叶子结点所属的类型。在构建决策树的代码，可以看到，有个featLabels参数。它是用来干什么的？它就是用来记录各个分类结点的，在用决策树做预测的时候，我们按顺序输入需要的分类结点的属性值即可。举个例子，比如我用上述已经训练好的决策树做分类，那么我只需要提供这个人是否有房子，是否有工作这两个信息即可，无需提供冗余的信息。</p>
<p><strong>决策树的存储</strong></p>
<p>构造决策树是很耗时的任务，即使处理很小的数据集，也要花费几秒的时间，如果数据集很大，将会耗费很多计算时间。然而用创建好的决策树解决分类问题，则可以很快完成。因此，为了节省计算时间，最好能够在每次执行分类时调用已经构造好的决策树。为了解决这个问题，需要使用Python模块pickle序列化对象。序列化对象可以在磁盘上保存对象，并在需要的时候读取出来。</p>
<p>假设我们已经得到决策树<code>{'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}}</code>，使用<code>pickle.dump</code>存储决策树。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> pickle
<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">函数说明：存储决策树
</span><span style="color:#e6db74">Parameters：
</span><span style="color:#e6db74">    inputTree：已经生成的决策树
</span><span style="color:#e6db74">    filename：决策树的存储文件名
</span><span style="color:#e6db74">Returns：
</span><span style="color:#e6db74">    无
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">storeTree</span>(inputTree,filename):
    <span style="color:#66d9ef">with</span> open(filename,<span style="color:#e6db74">&#39;wb&#39;</span>) <span style="color:#66d9ef">as</span> fw:
        pickle<span style="color:#f92672">.</span>dump(inputTree,fw)

<span style="color:#66d9ef">if</span> __name__<span style="color:#f92672">==</span><span style="color:#e6db74">&#39;__main__&#39;</span>:
    myTree<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;有自己的房子&#39;</span>:{<span style="color:#ae81ff">0</span>:{<span style="color:#e6db74">&#39;有工作&#39;</span>:{<span style="color:#ae81ff">0</span>:<span style="color:#e6db74">&#39;no&#39;</span>,<span style="color:#ae81ff">1</span>:<span style="color:#e6db74">&#39;yes&#39;</span>}},<span style="color:#ae81ff">1</span>:<span style="color:#e6db74">&#39;yes&#39;</span>}}
    storeTree(myTree,<span style="color:#e6db74">&#39;classifierStorage.txt&#39;</span>)

</code></pre></div><p>运行代码，在该Python文件的相同目录下，会生成一个名为<code>classifierStorage.txt</code>的txt文件，这个文件二进制存储着我们的决策树。</p>
<p><strong>很简单使用<code>pickle.load</code>进行载入即可，编写代码如下：</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> pickle

<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">函数说明:读取决策树
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Parameters:
</span><span style="color:#e6db74">    filename：决策树的存储文件名
</span><span style="color:#e6db74">Returns:
</span><span style="color:#e6db74">    pickle.load(fr)：决策树字典
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grabTree</span>(filename):
    fr <span style="color:#f92672">=</span> open(filename, <span style="color:#e6db74">&#39;rb&#39;</span>)
    <span style="color:#66d9ef">return</span> pickle<span style="color:#f92672">.</span>load(fr)

<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
    myTree <span style="color:#f92672">=</span> grabTree(<span style="color:#e6db74">&#39;classifierStorage.txt&#39;</span>)
    <span style="color:#66d9ef">print</span>(myTree)

</code></pre></div><p><strong>树形结构为什么不需要归一化?</strong></p>
<p>因为数值缩放不影响分裂点位置，对树模型的结构不造成影响。 按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。而且，树模型是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化。</p>
<p>既然树形结构（如决策树、RF）不需要归一化，那为何非树形结构比如Adaboost、SVM、LR、Knn、KMeans之类则需要归一化。</p>
<p>对于线性模型，特征值差别很大时，运用梯度下降的时候，损失等高线是椭圆形，需要进行多次迭代才能到达最优点。 但是如果进行了归一化，那么等高线就是圆形的，促使SGD往原点迭代，从而导致需要的迭代次数较少</p>
<h3 id="2实现代码-1">2、实现代码</h3>
<p>以下是关于借贷问题的经验熵计算代码。</p>
<p>在编写代码之前，我们先对数据集进行属性标注。</p>
<ul>
<li>年龄：0代表青年，1代表中年，2代表老年；</li>
<li>有工作：0代表否，1代表是；</li>
<li>有自己的房子：0代表否，1代表是；</li>
<li>信贷情况：0代表一般，1代表好，2代表非常好；</li>
<li>类别(是否给贷款)：no代表否，yes代表是。</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> math <span style="color:#f92672">import</span> log
<span style="color:#f92672">import</span> operator
<span style="color:#f92672">from</span> matplotlib.font_manager <span style="color:#f92672">import</span> FontProperties
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">函数说明：计算给定数据集的经验熵（香农熵）
</span><span style="color:#e6db74">Parameters：
</span><span style="color:#e6db74">    dataSet：数据集
</span><span style="color:#e6db74">Returns：
</span><span style="color:#e6db74">    shannonEnt：经验熵
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">calcShannonEnt</span>(dataSet):
    <span style="color:#75715e">#返回数据集行数</span>
    numEntries<span style="color:#f92672">=</span>len(dataSet)
    <span style="color:#75715e">#保存每个标签（label）出现次数的字典</span>
    labelCounts<span style="color:#f92672">=</span>{}
    <span style="color:#75715e">#对每组特征向量进行统计</span>
    <span style="color:#66d9ef">for</span> featVec <span style="color:#f92672">in</span> dataSet:
        currentLabel<span style="color:#f92672">=</span>featVec[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]                     <span style="color:#75715e">#提取标签信息</span>
        <span style="color:#66d9ef">if</span> currentLabel <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> labelCounts<span style="color:#f92672">.</span>keys():   <span style="color:#75715e">#如果标签没有放入统计次数的字典，添加进去</span>
            labelCounts[currentLabel]<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
        labelCounts[currentLabel]<span style="color:#f92672">+=</span><span style="color:#ae81ff">1</span>                 <span style="color:#75715e">#label计数</span>

    shannonEnt<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>                                   <span style="color:#75715e">#经验熵</span>
    <span style="color:#75715e">#计算经验熵</span>
    <span style="color:#66d9ef">for</span> key <span style="color:#f92672">in</span> labelCounts:
        prob<span style="color:#f92672">=</span>float(labelCounts[key])<span style="color:#f92672">/</span>numEntries      <span style="color:#75715e">#选择该标签的概率</span>
        shannonEnt<span style="color:#f92672">-=</span>prob<span style="color:#f92672">*</span>log(prob,<span style="color:#ae81ff">2</span>)                 <span style="color:#75715e">#利用公式计算</span>
    <span style="color:#66d9ef">return</span> shannonEnt                                <span style="color:#75715e">#返回经验熵</span>

<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">函数说明：创建测试数据集
</span><span style="color:#e6db74">Parameters：无
</span><span style="color:#e6db74">Returns：
</span><span style="color:#e6db74">    dataSet：数据集
</span><span style="color:#e6db74">    labels：分类属性
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">createDataSet</span>():
    <span style="color:#75715e"># 数据集</span>
    dataSet<span style="color:#f92672">=</span>[[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#e6db74">&#39;no&#39;</span>],
            [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#39;no&#39;</span>],
            [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#39;yes&#39;</span>],
            [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#e6db74">&#39;yes&#39;</span>],
            [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#e6db74">&#39;no&#39;</span>],
            [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#e6db74">&#39;no&#39;</span>],
            [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#39;no&#39;</span>],
            [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#39;yes&#39;</span>],
            [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#e6db74">&#39;yes&#39;</span>],
            [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#e6db74">&#39;yes&#39;</span>],
            [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#e6db74">&#39;yes&#39;</span>],
            [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#39;yes&#39;</span>],
            [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#39;yes&#39;</span>],
            [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, <span style="color:#e6db74">&#39;yes&#39;</span>],
            [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#e6db74">&#39;no&#39;</span>]]
    <span style="color:#75715e">#分类属性</span>
    labels<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;年龄&#39;</span>,<span style="color:#e6db74">&#39;有工作&#39;</span>,<span style="color:#e6db74">&#39;有自己的房子&#39;</span>,<span style="color:#e6db74">&#39;信贷情况&#39;</span>]
    <span style="color:#75715e">#返回数据集和分类属性</span>
    <span style="color:#66d9ef">return</span> dataSet,labels

<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">函数说明：按照给定特征划分数据集
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Parameters：
</span><span style="color:#e6db74">    dataSet:待划分的数据集
</span><span style="color:#e6db74">    axis：划分数据集的特征
</span><span style="color:#e6db74">    value：需要返回的特征值
</span><span style="color:#e6db74">Returns：
</span><span style="color:#e6db74">    无
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">splitDataSet</span>(dataSet,axis,value):
    <span style="color:#75715e">#创建返回的数据集列表</span>
    retDataSet<span style="color:#f92672">=</span>[]
    <span style="color:#75715e">#遍历数据集</span>
    <span style="color:#66d9ef">for</span> featVec <span style="color:#f92672">in</span> dataSet:
        <span style="color:#66d9ef">if</span> featVec[axis]<span style="color:#f92672">==</span>value:
            <span style="color:#75715e">#去掉axis特征</span>
            reduceFeatVec<span style="color:#f92672">=</span>featVec[:axis]
            <span style="color:#75715e">#将符合条件的添加到返回的数据集</span>
            reduceFeatVec<span style="color:#f92672">.</span>extend(featVec[axis<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>:])
            retDataSet<span style="color:#f92672">.</span>append(reduceFeatVec)
    <span style="color:#75715e">#返回划分后的数据集</span>
    <span style="color:#66d9ef">return</span> retDataSet

<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">函数说明：计算给定数据集的经验熵（香农熵）
</span><span style="color:#e6db74">Parameters：
</span><span style="color:#e6db74">    dataSet：数据集
</span><span style="color:#e6db74">Returns：
</span><span style="color:#e6db74">    shannonEnt：信息增益最大特征的索引值
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">chooseBestFeatureToSplit</span>(dataSet):
    <span style="color:#75715e">#特征数量</span>
    numFeatures <span style="color:#f92672">=</span> len(dataSet[<span style="color:#ae81ff">0</span>]) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
    <span style="color:#75715e">#计数数据集的香农熵</span>
    baseEntropy <span style="color:#f92672">=</span> calcShannonEnt(dataSet)
    <span style="color:#75715e">#信息增益</span>
    bestInfoGain <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
    <span style="color:#75715e">#最优特征的索引值</span>
    bestFeature <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
    <span style="color:#75715e">#遍历所有特征</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(numFeatures):
        <span style="color:#75715e"># 获取dataSet的第i个所有特征</span>
        featList <span style="color:#f92672">=</span> [example[i] <span style="color:#66d9ef">for</span> example <span style="color:#f92672">in</span> dataSet]
        <span style="color:#75715e">#创建set集合{}，元素不可重复</span>
        uniqueVals <span style="color:#f92672">=</span> set(featList)
        <span style="color:#75715e">#经验条件熵</span>
        newEntropy <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
        <span style="color:#75715e">#计算信息增益</span>
        <span style="color:#66d9ef">for</span> value <span style="color:#f92672">in</span> uniqueVals:
            <span style="color:#75715e">#subDataSet划分后的子集</span>
            subDataSet <span style="color:#f92672">=</span> splitDataSet(dataSet, i, value)
            <span style="color:#75715e">#计算子集的概率</span>
            prob <span style="color:#f92672">=</span> len(subDataSet) <span style="color:#f92672">/</span> float(len(dataSet))
            <span style="color:#75715e">#根据公式计算经验条件熵</span>
            newEntropy <span style="color:#f92672">+=</span> prob <span style="color:#f92672">*</span> calcShannonEnt((subDataSet))
        <span style="color:#75715e">#信息增益</span>
        infoGain <span style="color:#f92672">=</span> baseEntropy <span style="color:#f92672">-</span> newEntropy
        <span style="color:#75715e">#打印每个特征的信息增益</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;第</span><span style="color:#e6db74">%d</span><span style="color:#e6db74">个特征的增益为</span><span style="color:#e6db74">%.3f</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (i, infoGain))
        <span style="color:#75715e">#计算信息增益</span>
        <span style="color:#66d9ef">if</span> (infoGain <span style="color:#f92672">&gt;</span> bestInfoGain):
            <span style="color:#75715e">#更新信息增益，找到最大的信息增益</span>
            bestInfoGain <span style="color:#f92672">=</span> infoGain
            <span style="color:#75715e">#记录信息增益最大的特征的索引值</span>
            bestFeature <span style="color:#f92672">=</span> i
            <span style="color:#75715e">#返回信息增益最大特征的索引值</span>
    <span style="color:#66d9ef">return</span> bestFeature

<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">函数说明：统计classList中出现次数最多的元素（类标签）
</span><span style="color:#e6db74">Parameters：
</span><span style="color:#e6db74">    classList：类标签列表
</span><span style="color:#e6db74">Returns：
</span><span style="color:#e6db74">    sortedClassCount[0][0]：出现次数最多的元素（类标签）
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">majorityCnt</span>(classList):
    classCount<span style="color:#f92672">=</span>{}
    <span style="color:#75715e">#统计classList中每个元素出现的次数</span>
    <span style="color:#66d9ef">for</span> vote <span style="color:#f92672">in</span> classList:
        <span style="color:#66d9ef">if</span> vote <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> classCount<span style="color:#f92672">.</span>keys():
            classCount[vote]<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
            classCount[vote]<span style="color:#f92672">+=</span><span style="color:#ae81ff">1</span>
        <span style="color:#75715e">#根据字典的值降序排列</span>
        sortedClassCount<span style="color:#f92672">=</span>sorted(classCount<span style="color:#f92672">.</span>items(),key<span style="color:#f92672">=</span>operator<span style="color:#f92672">.</span>itemgetter(<span style="color:#ae81ff">1</span>),reverse<span style="color:#f92672">=</span>True)
        <span style="color:#66d9ef">return</span> sortedClassCount[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>]

<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">函数说明：创建决策树
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Parameters:
</span><span style="color:#e6db74">    dataSet：训练数据集
</span><span style="color:#e6db74">    labels：分类属性标签
</span><span style="color:#e6db74">    featLabels：存储选择的最优特征标签
</span><span style="color:#e6db74">Returns：
</span><span style="color:#e6db74">    myTree：决策树
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">createTree</span>(dataSet,labels,featLabels):
    <span style="color:#75715e">#取分类标签（是否放贷：yes or no）</span>
    classList<span style="color:#f92672">=</span>[example[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#66d9ef">for</span> example <span style="color:#f92672">in</span> dataSet]
    <span style="color:#75715e">#如果类别完全相同，则停止继续划分</span>
    <span style="color:#66d9ef">if</span> classList<span style="color:#f92672">.</span>count(classList[<span style="color:#ae81ff">0</span>])<span style="color:#f92672">==</span>len(classList):
        <span style="color:#66d9ef">return</span> classList[<span style="color:#ae81ff">0</span>]
    <span style="color:#75715e">#遍历完所有特征时返回出现次数最多的类标签</span>
    <span style="color:#66d9ef">if</span> len(dataSet[<span style="color:#ae81ff">0</span>])<span style="color:#f92672">==</span><span style="color:#ae81ff">1</span>:
        <span style="color:#66d9ef">return</span> majorityCnt(classList)
    <span style="color:#75715e">#选择最优特征</span>
    bestFeat<span style="color:#f92672">=</span>chooseBestFeatureToSplit(dataSet)
    <span style="color:#75715e">#最优特征的标签</span>
    bestFeatLabel<span style="color:#f92672">=</span>labels[bestFeat]
    featLabels<span style="color:#f92672">.</span>append(bestFeatLabel)
    <span style="color:#75715e">#根据最优特征的标签生成树</span>
    myTree<span style="color:#f92672">=</span>{bestFeatLabel:{}}
    <span style="color:#75715e">#删除已经使用的特征标签</span>
    <span style="color:#66d9ef">del</span>(labels[bestFeat])
    <span style="color:#75715e">#得到训练集中所有最优特征的属性值</span>
    featValues<span style="color:#f92672">=</span>[example[bestFeat] <span style="color:#66d9ef">for</span> example <span style="color:#f92672">in</span> dataSet]
    <span style="color:#75715e">#去掉重复的属性值</span>
    uniqueVls<span style="color:#f92672">=</span>set(featValues)
    <span style="color:#75715e">#遍历特征，创建决策树</span>
    <span style="color:#66d9ef">for</span> value <span style="color:#f92672">in</span> uniqueVls:
        myTree[bestFeatLabel][value]<span style="color:#f92672">=</span>createTree(splitDataSet(dataSet,bestFeat,value),
                                               labels,featLabels)
    <span style="color:#66d9ef">return</span> myTree

<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">函数说明：获取决策树叶子节点的数目
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Parameters：
</span><span style="color:#e6db74">    myTree：决策树
</span><span style="color:#e6db74">Returns：
</span><span style="color:#e6db74">    numLeafs：决策树的叶子节点的数目
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">getNumLeafs</span>(myTree):
    numLeafs<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>

      <span style="color:#75715e"># 获得myTree的第一个键值，即第一个特征，分割的标签 firstStr=next(iter(myTree))也可以</span>
    firstStr<span style="color:#f92672">=</span>list(myTree<span style="color:#f92672">.</span>keys())[<span style="color:#ae81ff">0</span>]
    <span style="color:#75715e"># 根据键值得到对应的值，即根据第一个特征分类的结果</span>
    secondDict<span style="color:#f92672">=</span>myTree[firstStr]

    <span style="color:#66d9ef">for</span> key <span style="color:#f92672">in</span> secondDict<span style="color:#f92672">.</span>keys():
        <span style="color:#66d9ef">if</span> type(secondDict[key])<span style="color:#f92672">.</span>__name__<span style="color:#f92672">==</span><span style="color:#e6db74">&#39;dict&#39;</span>:
            numLeafs<span style="color:#f92672">+=</span>getNumLeafs(secondDict[key])
        <span style="color:#66d9ef">else</span>: numLeafs<span style="color:#f92672">+=</span><span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">return</span> numLeafs

<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">函数说明:获取决策树的层数
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Parameters:
</span><span style="color:#e6db74">    myTree:决策树
</span><span style="color:#e6db74">Returns:
</span><span style="color:#e6db74">    maxDepth:决策树的层数
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">getTreeDepth</span>(myTree):
    maxDepth <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>                                                <span style="color:#75715e">#初始化决策树深度</span>
     firstStr<span style="color:#f92672">=</span>list(myTree<span style="color:#f92672">.</span>keys())[<span style="color:#ae81ff">0</span>]                                <span style="color:#75715e">#python3中myTree.keys()返回的是dict_keys,不在是list,所以不能使用myTree.keys()[0]的方法获取结点属性，可以使用list(myTree.keys())[0]</span>
    secondDict <span style="color:#f92672">=</span> myTree[firstStr]                                <span style="color:#75715e">#获取下一个字典</span>
    <span style="color:#66d9ef">for</span> key <span style="color:#f92672">in</span> secondDict<span style="color:#f92672">.</span>keys():
        <span style="color:#66d9ef">if</span> type(secondDict[key])<span style="color:#f92672">.</span>__name__<span style="color:#f92672">==</span><span style="color:#e6db74">&#39;dict&#39;</span>:                <span style="color:#75715e">#测试该结点是否为字典，如果不是字典，代表此结点为叶子结点</span>
            thisDepth <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> getTreeDepth(secondDict[key])
        <span style="color:#66d9ef">else</span>:   thisDepth <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
        <span style="color:#66d9ef">if</span> thisDepth <span style="color:#f92672">&gt;</span> maxDepth: maxDepth <span style="color:#f92672">=</span> thisDepth            <span style="color:#75715e">#更新层数</span>
    <span style="color:#66d9ef">return</span> maxDepth

<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">函数说明:绘制结点
</span><span style="color:#e6db74">  # annotate函数是为绘制图上指定的数据点xy添加一个nodeTxt注释
</span><span style="color:#e6db74">    # nodeTxt是给数据点xy添加一个注释，xy为数据点的开始绘制的坐标,位于节点的中间位置
</span><span style="color:#e6db74">    # xycoords设置指定点xy的坐标类型，xytext为注释的中间点坐标，textcoords设置注释点坐标样式
</span><span style="color:#e6db74">    # bbox设置装注释盒子的样式,arrowprops设置箭头的样式
</span><span style="color:#e6db74">    &#39;&#39;&#39;
</span><span style="color:#e6db74">    figure points:表示坐标原点在图的左下角的数据点
</span><span style="color:#e6db74">    figure pixels:表示坐标原点在图的左下角的像素点
</span><span style="color:#e6db74">    figure fraction：此时取值是小数，范围是([0,1],[0,1]),在图的左下角时xy是（0,0），最右上角是(1,1)
</span><span style="color:#e6db74">    其他位置是按相对图的宽高的比例取最小值
</span><span style="color:#e6db74">    axes points : 表示坐标原点在图中坐标的左下角的数据点
</span><span style="color:#e6db74">    axes pixels : 表示坐标原点在图中坐标的左下角的像素点
</span><span style="color:#e6db74">    axes fraction : 与figure fraction类似，只不过相对于图的位置改成是相对于坐标轴的位置
</span><span style="color:#e6db74">    &#39;&#39;&#39;
</span><span style="color:#e6db74">Parameters:
</span><span style="color:#e6db74">    nodeTxt - 结点名
</span><span style="color:#e6db74">    centerPt - 文本位置
</span><span style="color:#e6db74">    parentPt - 标注的箭头位置
</span><span style="color:#e6db74">    nodeType - 结点格式
</span><span style="color:#e6db74">Returns:
</span><span style="color:#e6db74">    无
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plotNode</span>(nodeTxt, centerPt, parentPt, nodeType):
    arrow_args <span style="color:#f92672">=</span> dict(arrowstyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&lt;-&#34;</span>)                                            <span style="color:#75715e">#定义箭头格式</span>
    font <span style="color:#f92672">=</span> FontProperties(fname<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;c:\windows\fonts\simsun.ttc&#34;</span>, size<span style="color:#f92672">=</span><span style="color:#ae81ff">14</span>)        <span style="color:#75715e">#设置中文字体</span>
    createPlot<span style="color:#f92672">.</span>ax1<span style="color:#f92672">.</span>annotate(nodeTxt, xy<span style="color:#f92672">=</span>parentPt,  xycoords<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;axes fraction&#39;</span>,    <span style="color:#75715e">#绘制结点</span>
        xytext<span style="color:#f92672">=</span>centerPt, textcoords<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;axes fraction&#39;</span>,
        va<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;center&#34;</span>, ha<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;center&#34;</span>, bbox<span style="color:#f92672">=</span>nodeType, arrowprops<span style="color:#f92672">=</span>arrow_args, FontProperties<span style="color:#f92672">=</span>font)

<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">函数说明:标注有向边属性值
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Parameters:
</span><span style="color:#e6db74">    cntrPt、parentPt - 用于计算标注位置
</span><span style="color:#e6db74">    txtString - 标注的内容
</span><span style="color:#e6db74">Returns:
</span><span style="color:#e6db74">    无
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plotMidText</span>(cntrPt, parentPt, txtString):
    xMid <span style="color:#f92672">=</span> (parentPt[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">-</span>cntrPt[<span style="color:#ae81ff">0</span>])<span style="color:#f92672">/</span><span style="color:#ae81ff">2.0</span> <span style="color:#f92672">+</span> cntrPt[<span style="color:#ae81ff">0</span>]                                            <span style="color:#75715e">#计算标注位置</span>
    yMid <span style="color:#f92672">=</span> (parentPt[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">-</span>cntrPt[<span style="color:#ae81ff">1</span>])<span style="color:#f92672">/</span><span style="color:#ae81ff">2.0</span> <span style="color:#f92672">+</span> cntrPt[<span style="color:#ae81ff">1</span>]
    createPlot<span style="color:#f92672">.</span>ax1<span style="color:#f92672">.</span>text(xMid, yMid, txtString, va<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;center&#34;</span>, ha<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;center&#34;</span>, rotation<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>)

<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">函数说明:绘制决策树
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Parameters:
</span><span style="color:#e6db74">    myTree - 决策树(字典)
</span><span style="color:#e6db74">    parentPt - 标注的内容
</span><span style="color:#e6db74">    nodeTxt - 结点名
</span><span style="color:#e6db74">Returns:
</span><span style="color:#e6db74">    无
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plotTree</span>(myTree, parentPt, nodeTxt):
    decisionNode <span style="color:#f92672">=</span> dict(boxstyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;sawtooth&#34;</span>, fc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;0.8&#34;</span>)                                        <span style="color:#75715e">#设置结点格式</span>
    leafNode <span style="color:#f92672">=</span> dict(boxstyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;round4&#34;</span>, fc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;0.8&#34;</span>)                                            <span style="color:#75715e">#设置叶结点格式</span>
    numLeafs <span style="color:#f92672">=</span> getNumLeafs(myTree)                                                          <span style="color:#75715e">#获取决策树叶结点数目，决定了树的宽度</span>
    depth <span style="color:#f92672">=</span> getTreeDepth(myTree)                                                            <span style="color:#75715e">#获取决策树层数</span>
     firstStr<span style="color:#f92672">=</span>list(myTree<span style="color:#f92672">.</span>keys())[<span style="color:#ae81ff">0</span>]                                                          <span style="color:#75715e">#下个字典</span>
    cntrPt <span style="color:#f92672">=</span> (plotTree<span style="color:#f92672">.</span>xOff <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">+</span> float(numLeafs))<span style="color:#f92672">/</span><span style="color:#ae81ff">2.0</span><span style="color:#f92672">/</span>plotTree<span style="color:#f92672">.</span>totalW, plotTree<span style="color:#f92672">.</span>yOff)    <span style="color:#75715e">#中心位置</span>
    plotMidText(cntrPt, parentPt, nodeTxt)                                                    <span style="color:#75715e">#标注有向边属性值</span>
    plotNode(firstStr, cntrPt, parentPt, decisionNode)                                        <span style="color:#75715e">#绘制结点</span>
    secondDict <span style="color:#f92672">=</span> myTree[firstStr]                                                            <span style="color:#75715e">#下一个字典，也就是继续绘制子结点</span>
    plotTree<span style="color:#f92672">.</span>yOff <span style="color:#f92672">=</span> plotTree<span style="color:#f92672">.</span>yOff <span style="color:#f92672">-</span> <span style="color:#ae81ff">1.0</span><span style="color:#f92672">/</span>plotTree<span style="color:#f92672">.</span>totalD                                        <span style="color:#75715e">#y偏移</span>
    <span style="color:#66d9ef">for</span> key <span style="color:#f92672">in</span> secondDict<span style="color:#f92672">.</span>keys():
        <span style="color:#66d9ef">if</span> type(secondDict[key])<span style="color:#f92672">.</span>__name__<span style="color:#f92672">==</span><span style="color:#e6db74">&#39;dict&#39;</span>:                                            <span style="color:#75715e">#测试该结点是否为字典，如果不是字典，代表此结点为叶子结点</span>
            plotTree(secondDict[key],cntrPt,str(key))                                        <span style="color:#75715e">#不是叶结点，递归调用继续绘制</span>
        <span style="color:#66d9ef">else</span>:                                                                                <span style="color:#75715e">#如果是叶结点，绘制叶结点，并标注有向边属性值</span>
            plotTree<span style="color:#f92672">.</span>xOff <span style="color:#f92672">=</span> plotTree<span style="color:#f92672">.</span>xOff <span style="color:#f92672">+</span> <span style="color:#ae81ff">1.0</span><span style="color:#f92672">/</span>plotTree<span style="color:#f92672">.</span>totalW
            plotNode(secondDict[key], (plotTree<span style="color:#f92672">.</span>xOff, plotTree<span style="color:#f92672">.</span>yOff), cntrPt, leafNode)
            plotMidText((plotTree<span style="color:#f92672">.</span>xOff, plotTree<span style="color:#f92672">.</span>yOff), cntrPt, str(key))
    plotTree<span style="color:#f92672">.</span>yOff <span style="color:#f92672">=</span> plotTree<span style="color:#f92672">.</span>yOff <span style="color:#f92672">+</span> <span style="color:#ae81ff">1.0</span><span style="color:#f92672">/</span>plotTree<span style="color:#f92672">.</span>totalD

<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">函数说明:创建绘制面板
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">Parameters:
</span><span style="color:#e6db74">    inTree - 决策树(字典)
</span><span style="color:#e6db74">Returns:
</span><span style="color:#e6db74">    无
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">createPlot</span>(inTree):
    fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>figure(<span style="color:#ae81ff">1</span>, facecolor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;white&#39;</span>)<span style="color:#75715e">#创建fig</span>
    fig<span style="color:#f92672">.</span>clf()<span style="color:#75715e">#清空fig</span>
    axprops <span style="color:#f92672">=</span> dict(xticks<span style="color:#f92672">=</span>[], yticks<span style="color:#f92672">=</span>[])
    createPlot<span style="color:#f92672">.</span>ax1 <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">111</span>, frameon<span style="color:#f92672">=</span>False, <span style="color:#f92672">**</span>axprops)<span style="color:#75715e">#去掉x、y轴</span>
    plotTree<span style="color:#f92672">.</span>totalW <span style="color:#f92672">=</span> float(getNumLeafs(inTree))<span style="color:#75715e">#获取决策树叶结点数目</span>
    plotTree<span style="color:#f92672">.</span>totalD <span style="color:#f92672">=</span> float(getTreeDepth(inTree))<span style="color:#75715e">#获取决策树层数</span>
    plotTree<span style="color:#f92672">.</span>xOff <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.5</span><span style="color:#f92672">/</span>plotTree<span style="color:#f92672">.</span>totalW; plotTree<span style="color:#f92672">.</span>yOff <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span><span style="color:#75715e">#x偏移</span>
    plotTree(inTree, (<span style="color:#ae81ff">0.5</span>,<span style="color:#ae81ff">1.0</span>), <span style="color:#e6db74">&#39;&#39;</span>)<span style="color:#75715e">#绘制决策树</span>
    plt<span style="color:#f92672">.</span>show()<span style="color:#75715e">#显示绘制结果</span>
<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">使用决策树进行分类
</span><span style="color:#e6db74">Parameters：
</span><span style="color:#e6db74">    inputTree；已经生成的决策树
</span><span style="color:#e6db74">    featLabels：存储选择的最优特征标签
</span><span style="color:#e6db74">    testVec：测试数据列表，顺序对应最优特征标签
</span><span style="color:#e6db74">Returns：
</span><span style="color:#e6db74">    classLabel：分类结果
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">classify</span>(inputTree,featLabels,testVec):
    <span style="color:#75715e">#获取决策树节点</span>
    firstStr<span style="color:#f92672">=</span>next(iter(inputTree))
    <span style="color:#75715e">#下一个字典</span>
    secondDict<span style="color:#f92672">=</span>inputTree[firstStr]
    featIndex<span style="color:#f92672">=</span>featLabels<span style="color:#f92672">.</span>index(firstStr)

    <span style="color:#66d9ef">for</span> key <span style="color:#f92672">in</span> secondDict<span style="color:#f92672">.</span>keys():
        <span style="color:#66d9ef">if</span> testVec[featIndex]<span style="color:#f92672">==</span>key:
            <span style="color:#66d9ef">if</span> type(secondDict[key])<span style="color:#f92672">.</span>__name__<span style="color:#f92672">==</span><span style="color:#e6db74">&#39;dict&#39;</span>:
                classLabel<span style="color:#f92672">=</span>classify(secondDict[key],featLabels,testVec)
            <span style="color:#66d9ef">else</span>: classLabel<span style="color:#f92672">=</span>secondDict[key]
    <span style="color:#66d9ef">return</span> classLabel


<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
    dataSet, labels <span style="color:#f92672">=</span> createDataSet()
    featLabels <span style="color:#f92672">=</span> []
    myTree <span style="color:#f92672">=</span> createTree(dataSet, labels, featLabels)
    <span style="color:#66d9ef">print</span>(myTree)
    createPlot(myTree)
<span style="color:#75715e">#测试数据</span>
    testVec<span style="color:#f92672">=</span>[<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>]
    result<span style="color:#f92672">=</span>classify(myTree,featLabels,testVec)

    <span style="color:#66d9ef">if</span> result<span style="color:#f92672">==</span><span style="color:#e6db74">&#39;yes&#39;</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;放贷&#39;</span>)
    <span style="color:#66d9ef">if</span> result<span style="color:#f92672">==</span><span style="color:#e6db74">&#39;no&#39;</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;不放贷&#39;</span>)


</code></pre></div><h3 id="3优缺点及适用场景-1">3、优缺点及适用场景</h3>
<p><strong>优点</strong></p>
<p>易于理解和解释，决策树可以可视化。
几乎不需要数据预处理。其他方法经常需要数据标准化，创建虚拟变量和删除缺失值。决策树还不支持缺失值。
使用树的花费（例如预测数据）是训练数据点(data points)数量的对数。
可以同时处理数值变量和分类变量。其他方法大都适用于分析一种变量的集合。
可以处理多值输出变量问题。
使用白盒模型。如果一个情况被观察到，使用逻辑判断容易表示这种规则。相反，如果是黑盒模型（例如人工神经网络），结果会非常难解释。
即使对真实模型来说，假设无效的情况下，也可以较好的适用。</p>
<p><strong>缺点</strong></p>
<p>决策树学习可能创建一个过于复杂的树，并不能很好的预测数据。也就是过拟合。修剪机制（现在不支持），设置一个叶子节点需要的最小样本数量，或者数的最大深度，可以避免过拟合。
决策树可能是不稳定的，因为即使非常小的变异，可能会产生一颗完全不同的树。这个问题通过decision trees with an ensemble来缓解。
学习一颗最优的决策树是一个NP-完全问题under several aspects of optimality and even for simple concepts。因此，传统决策树算法基于启发式算法，例如贪婪算法，即每个节点创建最优决策。这些算法不能产生一个全家最优的决策树。对样本和特征随机抽样可以降低整体效果偏差。
概念难以学习，因为决策树没有很好的解释他们，例如，XOR, parity or multiplexer problems.
如果某些分类占优势，决策树将会创建一棵有偏差的树。因此，建议在训练之前，先抽样使样本均衡。</p>
<h2 id="三朴素贝叶斯">三、朴素贝叶斯</h2>
<h3 id="1主要思想-2">1、主要思想</h3>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202181428301.png" alt="img"></p>
<p>监督学习分为生成模型 (generative model) 与判别模型 (discriminative model)，贝叶斯算法正是生成模型的代表 (还有隐马尔科夫模型)。贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。<strong>而朴素朴素贝叶斯分类是贝叶斯分类中最简单，也是常见的一种分类方法</strong>。</p>
<p>朴素贝叶斯是经典的机器学习算法之一，也是为数不多的基于概率论的分类算法。对于大多数的分类算法，在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。比如决策树，KNN，逻辑回归，支持向量机等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系，要么是决策函数，要么是条件分布。但是朴素贝叶斯却是生成方法，该算法原理简单，也易于实现。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202181457928.png" alt="image-20220218145714838"></p>
<p>贝叶斯公式中，P(A)称为&quot;先验概率&rdquo;（Prior probability），即在B事件发生之前，对A事件概率的一个判断。P(A|B)称为&quot;后验概率&rdquo;（Posterior probability），即在B事件发生之后，对A事件概率的重新评估。P(B|A)/P(B)称为&quot;可能性函数&rdquo;（Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。所以，条件概率可以理解成下面的式子：后验概率＝先验概率 ｘ 调整因子。</p>
<p>说到朴素贝叶斯算法，首先牵扯到的一个概念是<strong>判别式和生成式</strong>。</p>
<ul>
<li>判别式：就是直接学习出特征输出Y和特征X之间的关系，如决策函数Y=f(X),或者从概率论的角度，求出条件分布P(Y|X)。代表算法有<strong>决策树、KNN、逻辑回归、支持向量机、随机条件场CRF等</strong>。</li>
<li>生成式：就是直接找出特征输出Y和特征X的联合分布P(X,Y)，然后用P(Y|X)=P(X,Y)/P(X）得出。代表算法有<strong>朴素贝叶斯、隐式马尔可夫链</strong>等。</li>
</ul>
<p><strong>朴素贝叶斯法是基于贝叶斯定理与特征条件独立性假设的分类方法</strong>。对于给定的训练集，首先基于特征条件独立假设学习输入输出的联合概率分布（朴素贝叶斯法这种通过学习得到模型的机制，显然<strong>属于生成模型</strong>）；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出 y。</p>
<ul>
<li>
<p>贝叶斯定理的基础是<strong>先验概率+数据=后验概率</strong>。贝叶斯定理解决的是一些原因X无法直接观测、测量，而我们希望通过其结果Y来反推出原因X的问题，也就是知道一部分先验概率，来求后验概率的问题。</p>
<p>**P(Y)：先验概率。**先验概率（prior probability）是指事情还没有发生，求这件事情发生的可能性的大小，是先验概率。它往往作为&quot;由因求果&quot;问题中的&quot;因&quot;出现。</p>
<p>**P(Y∣X)：后验概率。**后验概率是指事情已经发生，求这件事情发生的原因是由某个因素引起的可能性的大小。后验概率的计算要以先验概率为基础</p>
<p><strong>P(X∣Y) ：条件概率</strong>。又叫似然概率，一般是通过历史数据统计得到。一般不把它叫做先验概率，但从定义上也符合先验定义。</p>
</li>
<li>
<p><strong>特征条件独立</strong>：特征条件独立假设X的n个特征在类确定的条件下都是条件独立的。大大简化了计算过程，但是因为这个假设太过严格，所以会相应牺牲一定的准确率。这也是为什么称呼为<strong>朴素的原因</strong>。</p>
</li>
</ul>
<p><strong>贝叶斯算法简介</strong></p>
<p>贝叶斯方法源域它生前为解决一个“逆概”问题写的一篇文章。其要解决的问题：</p>
<p>　　<strong>正向概率</strong>：假设袋子里面有N个白球，M个黑球，你伸手进去摸一把，摸出黑球的概率是多大</p>
<p>　　<strong>逆向概率</strong>：如果我们事先不知道袋子里面黑白球的比例，而是闭着眼睛摸出一个（或者好几个）球，观察这些取出来的球的颜色之后，那么我们可以就此对袋子里面的黑白球的比例做出什么样的推测。</p>
<p>　　那么什么是贝叶斯呢？</p>
<ul>
<li>1，现实世界本身就是不确定的，人类的观察能力是有局限性的</li>
<li>2，我们日常观察到的只是事物表明上的结果，因此我们需要提供一个猜测</li>
</ul>
<p>　　NaiveBayes算法，又称朴素贝叶斯算法。朴素：特征条件独立；贝叶斯：基于贝叶斯定理。属于监督学习的生成模型，实现监督，没有迭代，并有坚实的数学理论（即贝叶斯定理）作为支撑。在大量样本下会有较好的表现，不适用于输入向量的特征条件有关联的场景。</p>
<p>　　朴素贝叶斯会单独考量每一维独立特征被分类的条件概率，进而综合这些概率并对其所在的特征向量做出分类预测。因此，朴素贝叶斯的基本数据假设是：各个维度上的特征被分类的条件概率之间是相互独立的。它经常被用于文本分类中，包括互联网新闻的分类，垃圾邮件的筛选。</p>
<p>　　朴素贝叶斯分类时一种十分简单的分类算法，叫他朴素贝叶斯分类时因为这种方法的思想真的很朴素，朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，即认为此待分类项属于哪个类别。</p>
<p>一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯学派大胆的假设先验分布的模型，比如正态分布，beta分布等。这个假设一般没有特定的依据，因此一直被频率学派认为很荒谬。虽然难以从严密的数学逻辑推出贝叶斯学派的逻辑，但是在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类。</p>
<p><strong>贝叶斯算法的流程如下：</strong></p>
<p>1）设x={a1,a2,&hellip;,am}为待分类项，其中a为x的一个特征属性</p>
<p>2）类别集合为C={y1,y2,&hellip;,yn}</p>
<p>3）分别计算P(y1|x),P(y2|x),&hellip;.,P(yn|x)的值（贝叶斯公式）</p>
<p>4）如果P(yk|x)=max{P(y1|x),P(y2|x),&hellip;.,P(yn|x)},那么认为x为yk类型</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202192045028.png" alt="uwfjofzm6g"></p>
<p>朴素贝叶斯按照数据的先验概率的不同可以分为高斯朴素贝叶斯，伯努利朴素贝叶斯，多项式朴素贝叶斯。</p>
<p>高斯朴素贝叶斯是指当特征属性为连续值时，而且分布服从高斯分布，那么在计算P(x|y)的时候可以直接使用高斯分布的概率公式。 因此只需要计算出各个类别中此特征项划分的各个均值和标准差。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202192102459.png" alt="image-20220219210254415"></p>
<p>伯努利朴素贝叶斯是指当特征属性为连续值时，而且分布服从伯努利分布， 那么在计算P(x|y)的时候可以直接使用伯努利分布的概率公式：<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202192106883.png" alt="image-20220219210621827"></p>
<p>多项式朴素贝叶斯指当特征属性服从多项分布，从而，对于每个类别 y，参数为θy=(θy1,θy2,&hellip;,θyn)，其中n为特征属性数目，那么P(xi|y)的概率为θyi。</p>
<ul>
<li>高斯模型：处理特征是连续型变量的情况</li>
<li>多项式模型：最常见，要求特征是离散数据</li>
<li>伯努利模型：要求特征是离散的，且为布尔类型，即true和false，或者1和0</li>
</ul>
<p><strong>基于极大似然估计的朴素贝叶斯算法的结果差强人意</strong>。这是因为在利用基于极大似然估计的朴素贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，用极大似然估计可能会出现所要估计的概率为0的情况，所以使用贝叶斯估计。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202192206046.png" alt="image-20220219220648975"></p>
<p><strong>拉普拉斯平滑（Laplace smoothing）</strong>
也就是参数为1时的贝叶斯估计，当某个分量在总样本某个分类中（观察样本库/训练集）从没出现过，会导致整个实例的计算结果为0。为了解决这个问题，使用拉普拉斯平滑/加1平滑进行处理。
它的思想非常简单，就是对先验概率的分子（划分的计数）加1，分母加上类别数；对条件概率分子加1，分母加上对应特征的可能取值数量。这样在解决零概率问题的同时，也保证了概率和依然为1。</p>
<p><strong>下溢出问题</strong>：这是由于<strong>太多很小的数相乘</strong>造成的。由于大部分因子都非常小，所以程序会下溢出或者得到不正确的答案。 一种解决办法是<strong>对乘积取自然对数</strong>。在代数中有 <code>ln(A * B) = ln(A) + ln(B)</code>, 于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。</p>
<p><strong>三个阶段：</strong>
第一阶段——准备阶段， 根据具体情况确定特征属性， 对每个特征属性进行适当划分， 然后由人工对一部分待分类项进行分类，形成训练样本集合。这一阶段的输入是所有待分类数据，输出是特征属性和训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段， 其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。</p>
<p>第二阶段——分类器训练阶段， 这个阶段的任务就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计， 并将结果记录。 其输入是特征属性和训练样本，输出是分类器。这一阶段是机械性阶段， 根据前面讨论的公式可以由程序自动计算完成。</p>
<p>第三阶段——应用阶段。 这个阶段的任务是使用分类器对待分类项进行分类， 其输入是分类器和待分类项， 输出是待分类项与类别的映射关系。这一阶段也是机械性阶段， 由程序完成。</p>
<h3 id="2实现代码-2">2、实现代码</h3>
<p><strong>项目概述</strong>：使用朴素贝叶斯构建一个快速过滤器来屏蔽侮辱性文档。如果某篇文档使用了负面或者侮辱性的语言，那么就将该文档标识为侮辱性文档。对此问题建立两个类别: 侮辱类和非侮辱类，使用 1 和 0 分别表示。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

<span style="color:#75715e"># 先验概率</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">trainNB0</span>(trainMatrix, trainCategory):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Desc:
</span><span style="color:#e6db74">        返回每个类别对应的先验概率和条件概率
</span><span style="color:#e6db74">    Params:
</span><span style="color:#e6db74">        trainMatrix: 训练数据集，即各个文档对应的 0-1 序列
</span><span style="color:#e6db74">        trainCategory: 训练数据集的类别，即各个文档对应的分类
</span><span style="color:#e6db74">    Return:
</span><span style="color:#e6db74">        p0Vect: 去重词汇表中每个单词在侮辱性文档中出现的概率
</span><span style="color:#e6db74">        p1Vect：去重词汇表中每个单词在非侮辱性文档中出现的概率
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    numTrainDocs <span style="color:#f92672">=</span> len(trainMatrix) <span style="color:#75715e"># 文件数</span>
    numWords <span style="color:#f92672">=</span> len(trainMatrix[<span style="color:#ae81ff">0</span>]) <span style="color:#75715e"># 单词数</span>
    
    <span style="color:#75715e"># 先验概率 </span>
    pAbusive <span style="color:#f92672">=</span> sum(trainCategory) <span style="color:#f92672">/</span> float(numTrainDocs) <span style="color:#75715e"># 侮辱性文件的出现概率，即 trainCategory 中所有 1 的个数(0 1 相加即得 1 的个数)</span>
    
    
    <span style="color:#75715e"># 条件概率 </span>
    
    <span style="color:#75715e"># (非)侮辱性单词在每个文件中出现的次数列表</span>
    <span style="color:#75715e"># 比如说 p0Num = [1,3,12,....] 表示第 2 个文档中出现了 3 次非侮辱词汇</span>
    p0Num <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(numWords) <span style="color:#75715e"># [1,1,1,.....] 非侮辱性单词在每个文件中出现的次数列表</span>
    p1Num <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(numWords) <span style="color:#75715e"># [1,1,1,.....] 侮辱性单词出在每个文件中出现的次数列表</span>
    
    <span style="color:#75715e"># (非)侮辱性单词在(非)侮辱性文档出现的总数</span>
    p0Denom <span style="color:#f92672">=</span> <span style="color:#ae81ff">2.0</span> <span style="color:#75715e"># 0 非侮辱性词汇在所有非侮辱的文档的出现总数</span>
    p1Denom <span style="color:#f92672">=</span> <span style="color:#ae81ff">2.0</span> <span style="color:#75715e"># 1 侮辱性词汇在所有侮辱性的文档的出现总数</span>
    
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(numTrainDocs):
        <span style="color:#75715e"># 是否是侮辱性文件</span>
        <span style="color:#66d9ef">if</span> trainCategory[i] <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
            <span style="color:#75715e"># 如果是侮辱性文件，对侮辱性文件的向量进行相加，表示在所有侮辱性文件中，去重词汇表中各个词汇出现的次数</span>
            p1Num <span style="color:#f92672">+=</span>  trainMatrix[i]
            <span style="color:#75715e"># 对向量中的所有元素进行求和，表示在所有侮辱性文件中,去重词汇表中所有词汇出现的次数之和</span>
            p1Denom <span style="color:#f92672">+=</span> sum(trainMatrix[i])
        <span style="color:#66d9ef">else</span>:
            <span style="color:#75715e"># 如果是非侮辱性文件，对非侮辱性文件的向量进行相加，表示在所有非侮辱性文件中，去重词汇表中各个词汇出现的次数</span>
            p0Num <span style="color:#f92672">+=</span> trainMatrix[i]
            <span style="color:#75715e"># 对向量中的所有元素进行求和，表示在所有非侮辱性文件中去重词汇表中所有词汇出现的次数之和</span>
            p0Denom <span style="color:#f92672">+=</span> sum(trainMatrix[i])
            
    <span style="color:#75715e"># 在类别 1 即侮辱性文档的条件下，去重词汇表中每个单词出现的概率</span>
    p1Vect <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>log(p1Num <span style="color:#f92672">/</span> p1Denom)
    
    <span style="color:#75715e"># 在类别 0 即非侮辱性文档的条件下，去重词汇表中每个单词出现的概率</span>
    p0Vect <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>log(p0Num <span style="color:#f92672">/</span> p0Denom)
    
    <span style="color:#66d9ef">return</span> pAbusive, p0Vect, p1Vect

<span style="color:#75715e"># 朴素贝叶斯分类器</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">classifyNB</span>(vec2Classify, p0Vec, p1Vec, pClass1):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Desc:
</span><span style="color:#e6db74">        输入某篇文档的0-1序列 vec2Classify，输出该篇所属的类别（侮辱性文档 1，非侮辱性文档 0）
</span><span style="color:#e6db74">    Args:
</span><span style="color:#e6db74">        vec2Classify: 某篇文档的 0-1 序列
</span><span style="color:#e6db74">        p0Vec：在类别 0 即非侮辱性文档的条件下，去重词汇表中每个单词出现的概率(对数形式)
</span><span style="color:#e6db74">        p1Vec：在类别 1 即侮辱性文档的条件下，去重词汇表中每个单词出现的概率（对数形式）
</span><span style="color:#e6db74">        pClass1: 该篇文档是侮辱性文件的概率（先验概率），注意转换成对数形式
</span><span style="color:#e6db74">    Return:
</span><span style="color:#e6db74">        类别 1/0
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    p1 <span style="color:#f92672">=</span> sum(vec2Classify <span style="color:#f92672">*</span> p1Vec) <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>log(pClass1) 
    p0 <span style="color:#f92672">=</span> sum(vec2Classify <span style="color:#f92672">*</span> p0Vec) <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> pClass1) 
    <span style="color:#66d9ef">if</span> p1 <span style="color:#f92672">&gt;</span> p0:
        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">else</span>:
        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">0</span>
    
<span style="color:#75715e"># 测试朴素贝叶斯分类器</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test</span>(testDoc):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Args:
</span><span style="color:#e6db74">        testDoc: 测试文档
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#75715e"># 1. 加载数据集</span>
    postingList, classVec <span style="color:#f92672">=</span> loadDataSet()
    <span style="color:#75715e"># 2. 创建去重词汇表</span>
    vocabList <span style="color:#f92672">=</span> createVocabList(postingList) <span style="color:#75715e"># 去重后的词汇表</span>
    <span style="color:#75715e"># 3. 创建每篇文档对应的 0-1 序列</span>
    trainMat <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> doc <span style="color:#f92672">in</span> postingList:
        trainMat<span style="color:#f92672">.</span>append(setOfWords2Vec(vocabList, doc))
    <span style="color:#75715e"># 4. 计算条件概率和先验概率</span>
    pAbusive, p0Vect, p1Vect <span style="color:#f92672">=</span> trainNB0(trainMat, classVec)
    <span style="color:#75715e"># 5. 朴素贝叶斯分类器</span>
    thisDoc <span style="color:#f92672">=</span> setOfWords2Vec(vocabList, testDoc) <span style="color:#75715e"># 将测试文档转换成 0-1 序列</span>
    <span style="color:#66d9ef">print</span>(testDoc, <span style="color:#e6db74">&#39;classified as: &#39;</span>, classifyNB(thisDoc, p0Vect, p1Vect, pAbusive))

</code></pre></div><h3 id="3优缺点及适用场景-2">3、优缺点及适用场景</h3>
<p><strong>优点</strong></p>
<ol>
<li>朴素贝叶斯模型发源于古典数学理论，<strong>有稳定的分类效率</strong>。</li>
<li><strong>对小规模的数据表现很好</strong>，能处理多分类任务，适合<strong>增量式训练</strong>，尤其是数据量超出内存时，我们可以一批批的去增量训练。</li>
<li><strong>对缺失数据不太敏感</strong>，算法也比较简单，常用于文本分类。</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>朴素贝叶斯模型的<strong>特征条件独立假设在实际应用中往往是不成立的</strong>。</li>
<li>如果样本数据分布不能很好的代表样本空间分布，那先验概率容易测不准。</li>
<li>对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。</li>
</ol>
<h2 id="四逻辑回归">四、逻辑回归</h2>
<h3 id="1主要思想-3">1、主要思想</h3>
<p>Logistic Regression 虽然被称为回归，但其实际上是分类模型，并常用于二分类。Logistic Regression 因其简单、可并行化、可解释强深受工业界喜爱。Logistic 回归的本质是：假设数据服从这个分布，然后使用极大似然估计做参数的估计。</p>
<p>逻辑回归是用来做分类算法的，大家都熟悉线性回归，一般形式是Y=aX+b，y的取值范围是[-∞, +∞]，有这么多取值，怎么进行分类呢？不用担心，伟大的数学家已经为我们找到了一个方法。</p>
<p>也就是把Y的结果带入一个非线性变换的<strong>Sigmoid函数</strong>中，即可得到[0,1]之间取值范围的数S，S可以把它看成是一个概率值，如果我们设置概率阈值为0.5，那么S大于0.5可以看成是正样本，小于0.5看成是负样本，就可以进行分类了。</p>
<p>简单来说， 逻辑回归（Logistic Regression）是一种用于解决二分类（0 or 1）问题的机器学习方法，用于估计某种事物的可能性。比如某用户购买某商品的可能性，某病人患有某种疾病的可能性，以及某广告被用户点击的可能性等。 注意，这里用的是“可能性”，而非数学上的“概率”，logisitc回归的结果并非数学定义中的概率值，不可以直接当做概率值来用。该结果往往用于和其他特征值加权求和，而非直接相乘。</p>
<p>逻辑回归（Logistic Regression）与线性回归（Linear Regression）都是一种广义线性模型（generalized linear model）。逻辑回归假设因变量 y 服从伯努利分布，而线性回归假设因变量 y 服从高斯分布。 因此与线性回归有很多相同之处，去除Sigmoid映射函数的话，逻辑回归算法就是一个线性回归。可以说，逻辑回归是以线性回归为理论支持的，但是逻辑回归通过Sigmoid函数引入了非线性因素，因此可以轻松处理0/1分类问题。</p>
<p><strong>Sigmoid函数</strong></p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202201421832.png" alt="image-20220220142110830"></p>
<p>函数中t无论取什么值，其结果都在[0,-1]的区间内，回想一下，一个分类问题就有两种答案，一种是“是”，一种是“否”，那0对应着“否”，1对应着“是”，那又有人问了，你这不是[0,1]的区间吗，怎么会只有0和1呢？这个问题问得好，我们假设分类的<strong>阈值</strong>是0.5，那么超过0.5的归为1分类，低于0.5的归为0分类，这就达到了分类的目的。选择0.5作为阈值是一个一般的做法，实际应用时特定的情况可以选择不同阈值，如果对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。</p>
<p><strong>损失函数</strong></p>
<p>损失函数（Loss Function) 直接作用于单个样本，用来表达样本的误差</p>
<p>代价函数（Cost Function）是整个样本集的平均误差，对所有损失函数值的平均</p>
<p>目标函数（Object Function）是我们最终要优化的函数，也就是代价函数+正则化函数（经验风险+结构风险）</p>
<p>逻辑回归的损失函数是 <strong>log loss</strong>，也就是<strong>对数似然函数</strong>，函数公式如下：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202201426465.jpeg" alt="image"></p>
<p>公式中的 y=1 表示的是真实值为1时用第一个公式，真实 y=0 用第二个公式计算损失。为什么要加上log函数呢？可以试想一下，当真实样本为1是，但h=0概率，那么log0=∞，这就对模型最大的惩罚力度；当h=1时，那么log1=0，相当于没有惩罚，也就是没有损失，达到最优结果。所以数学家就想出了用log函数来表示损失函数。</p>
<p>最后按照梯度下降法一样，求解极小值点，得到想要的模型效果。</p>
<p><strong>代价函数</strong></p>
<p>概况来讲，任何能够衡量模型预测出来的值 h(θ) 与真实值 y 之间的差异的函数都可以叫做代价函数 C(θ) 如果有多个样本，则可以将所有代价函数的取值求均值，记做 J(θ) 。因此很容易就可以得出以下关于代价函数的性质：</p>
<ul>
<li>选择代价函数时，最好挑选对参数 θ 可微的函数（全微分存在，偏导数一定存在）</li>
<li>对于每种算法来说，代价函数不是唯一的；</li>
<li>代价函数是参数 θ 的函数；</li>
<li>总的代价函数 J(θ) 可以用来评价模型的好坏，代价函数越小说明模型和参数越符合训练样本（x,y）；</li>
<li>J(θ) 是一个标量；</li>
</ul>
<p>经过上面的描述，一个好的代价函数需要满足两个最基本的要求：能够评价模型的准确性，对参数 θ 可微。在线性回归中，最常用的是均方误差(Mean squared error)，即</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202042678.png" alt="image-20220220204254617"></p>
<p>在逻辑回归中，最常用的是代价函数是<strong>交叉熵</strong>(Cross Entropy)，交叉熵是一个常见的代价函数。<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202045846.png" alt="img"></p>
<p><strong>为什么LR模型损失函数使用交叉熵不用均方差</strong></p>
<p>逻辑回归用均方差求出来的函数如下</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202048557.png" alt="image-20220220204823509"></p>
<p>是一个凸函数，不易优化，容易陷入局部最小值。</p>
<p>如果使用交叉熵作为损失函数：<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202057203.png" alt="image-20220220205744149"></p>
<p>w的梯度和当前预测的实际值是相关的，没有受到sigmoid函数的影响，且真实值和预测值差别越大，梯度越大，更新的速度也越快。如果用的是均方差作为损失函数，求得的梯度受sigmoid函数的影响。</p>
<p><strong>交叉熵简介</strong></p>
<p>交叉熵是信息论中的一个重要概念，主要用于度量两个概率分布间的差异性，要理解交叉熵，需要先了解下面几个概念。</p>
<p><strong>信息量</strong></p>
<p>信息奠基人香农（Shannon）认为“信息是用来消除随机不确定性的东西”，也就是说衡量信息量的大小就是看这个信息消除不确定性的程度。“太阳从东边升起”，这条信息并没有减少不确定性，因为太阳肯定是从东边升起的，这是一句废话，信息量为0。”2018年中国队成功进入世界杯“，从直觉上来看，这句话具有很大的信息量。因为中国队进入世界杯的不确定性因素很大，而这句话消除了进入世界杯的不确定性，所以按照定义，这句话的信息量很大。</p>
<p>根据上述可总结如下：信息量的大小与信息发生的概率成反比。<strong>概率越大，信息量越小。概率越小，信息量越大。</strong></p>
<p>设某一事件发生的概率为P(x)，其信息量表示为：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202103737.png" alt="image-20220220210317691"></p>
<p>其中 I ( x ) 表示信息量，这里 log 表示以e为底的自然对数。</p>
<p><strong>相对熵（KL散度）</strong></p>
<p>如果对于同一个随机变量X XX有两个单独的概率分布 P(x) 和 Q(x)，则我们可以使用KL散度来衡量这两个概率分布之间的差异。</p>
<p>下面直接列出公式，再举例子加以说明。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202105519.png" alt="image-20220220210527472"></p>
<p>在机器学习中，常常使用 P(x) 来表示样本的真实分布，Q(x) 来表示模型所预测的分布，比如在一个三分类任务中（例如，猫狗马分类器）， x1,x2,x3 分别代表猫，狗，马，例如一张猫的图片真实分布 P(X)=[1,0,0] ，预测分布 Q(X)=[0.7,0.2,0.1] ，计算KL散度：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202105311.png" alt="image-20220220210541257"></p>
<p>KL散度越小，表示 P(x) 与 Q(x) 的分布更加接近，可以通过反复训练 Q(x) 来使 Q(x) 的分布逼近 P(x)。
<strong>交叉熵</strong></p>
<p>首先将KL散度公式拆开：<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202106491.png" alt="image-20220220210652428"></p>
<p>前者 <em>H</em>(<em>p</em>(<em>x</em>)) 表示信息熵，后者即为交叉熵，<strong>KL散度 = 交叉熵 - 信息熵</strong></p>
<p>交叉熵公式表示为：<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202107134.png" alt="image-20220220210751085"></p>
<p>在机器学习训练网络时，输入数据与标签常常已经确定，那么真实概率分布 P(x) 也就确定下来了，所以信息熵在这里就是一个常量。由于KL散度的值表示真实概率分布 P(x) 与预测概率分布 Q(x) 之间的差异，值越小表示预测的结果越好，所以需要最小化KL散度，而交叉熵等于KL散度加上一个常量（信息熵），且公式相比KL散度更加容易计算，所以在机器学习中常常使用交叉熵损失函数来计算loss就行了。</p>
<p><strong>多分类</strong></p>
<p>思路步骤如下:</p>
<p>1.将类型class1看作正样本，其他类型全部看作负样本，然后我们就可以得到样本标记类型为该类型的概率p1。</p>
<p>2.然后再将另外类型class2看作正样本，其他类型全部看作负样本，同理得到p2。</p>
<p>3.以此循环，我们可以得到该待预测样本的标记类型分别为类型class i时的概率pi，最后我们取pi中最大的那个概率对应的样本标记类型作为我们的待预测样本类型。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202201748644.jpeg" alt="image"></p>
<p>总之还是以二分类来依次划分，并求出最大概率结果。</p>
<p><strong>逻辑回归常用的优化方法</strong></p>
<p><strong>一阶方法</strong></p>
<p>梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。</p>
<p><strong>二阶方法：牛顿法、拟牛顿法</strong></p>
<p>这里详细说一下牛顿法的基本原理和牛顿法的应用方式。牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。</p>
<p>缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。</p>
<p>拟牛顿法： 不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。</p>
<p><strong>逻辑斯特回归为什么要对特征进行离散化</strong></p>
<ol>
<li>非线性！逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代；</li>
<li>速度快！稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</li>
<li>鲁棒性！离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</li>
<li>方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；</li>
<li>稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；</li>
<li>简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。</li>
</ol>
<p><strong>决策边界</strong></p>
<p>决策边界，也称为决策面，是用于在N维空间，将不同类别样本分开的平面或曲面。<strong>决策边界其实就是一个方程</strong>。用于标识出分类函数（模型）的分类边界。</p>
<p>对于线性边界的情况，边界形式如下</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202201939860.png" alt="image-20220220193923812"></p>
<p>构造预测函数为：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202201940783.png" alt="image-20220220194022735"></p>
<p>函数<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202201940858.png" alt="img">的值有特殊的含义，它表示结果取1的概率，因此对于输入x分类结果为类别1和类别0的概率分别为：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202201940611.png" alt="img"></p>
<p><strong>逻辑回归与线性回归的区别与联系</strong></p>
<p><strong>区别</strong></p>
<ol>
<li>线性回归假设响应变量服从正态分布，逻辑回归假设响应变量服从伯努利分布</li>
<li>线性回归优化的目标函数是均方差（最小二乘），而逻辑回归优化的是似然函数（交叉熵）</li>
<li>线性归回要求自变量与因变量呈线性关系，而逻辑回归没有要求</li>
<li>线性回归分析的是因变量自身与自变量的关系，而逻辑回归研究的是因变量取值的概率与自变量的概率</li>
<li>逻辑回归处理的是分类问题，线性回归处理的是回归问题，这也导致了两个模型的取值范围不同：0-1和实数域</li>
<li>参数估计上，都是用极大似然估计的方法估计参数（高斯分布导致了线性模型损失函数为均方差，伯努利分布导致逻辑回归损失函数为交叉熵）</li>
</ol>
<p><strong>联系</strong></p>
<ol>
<li>两个都是线性模型，线性回归是普通线性模型，逻辑回归是广义线性模型</li>
<li>表达形式上，逻辑回归是线性回归套上了一个Sigmoid函数</li>
</ol>
<p>Logistic回归与多重线性回归实际上有很多相同之处，最大的区别就在于它们的因变量不同，其他的基本都差不多。正是因为如此，这两种回归可以归于同一个家族，即广义线性模型（generalizedlinear model）。</p>
<p>这一家族中的模型形式基本上都差不多，不同的就是因变量不同。</p>
<ul>
<li>如果是连续的，就是多重线性回归；</li>
<li>如果是二项分布，就是Logistic回归；</li>
<li>如果是Poisson分布，就是Poisson回归；</li>
<li>如果是负二项分布，就是负二项回归。</li>
</ul>
<p><strong>过拟合问题</strong></p>
<p>对于线性回归或逻辑回归的损失函数构成的模型，可能会有些权重很大，有些权重很小，导致过拟合（就是过分拟合了训练数据），使得模型的复杂度提高，泛化能力较差（对未知数据的预测能力）。</p>
<p>下面左图即为欠拟合，中图为合适的拟合，右图为过拟合。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202035523.png" alt="image-20220220203557430"></p>
<p>过拟合问题往往源自过多的特征。<strong>解决方法</strong>：</p>
<p>1）减少特征数量（减少特征会失去一些信息，即使特征选的很好）</p>
<ul>
<li>可用人工选择要保留的特征；</li>
<li>模型选择算法；</li>
</ul>
<p>2）正则化（特征较多时比较有效）</p>
<ul>
<li>保留所有特征，但减少θ的大小</li>
<li>正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或惩罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化项就越大。</li>
</ul>
<h3 id="2实现代码-3">2、实现代码</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LogisticRegression  <span style="color:#75715e">#导入逻辑回归模型 </span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">Sigmoid</span>(x):
    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x))
 
x<span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#f92672">-</span><span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">0.1</span>)
h <span style="color:#f92672">=</span> Sigmoid(x)            <span style="color:#75715e">#Sigmoid函数</span>
plt<span style="color:#f92672">.</span>plot(x, h)
plt<span style="color:#f92672">.</span>axvline(<span style="color:#ae81ff">0.0</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;k&#39;</span>)   <span style="color:#75715e">#坐标轴上加一条竖直的线（0位置）</span>
plt<span style="color:#f92672">.</span>axhspan(<span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">1.0</span>, facecolor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;1.0&#39;</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;dotted&#39;</span>)  
plt<span style="color:#f92672">.</span>axhline(y<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;dotted&#39;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;k&#39;</span>) 
plt<span style="color:#f92672">.</span>yticks([<span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">1.0</span>])  <span style="color:#75715e">#y轴标度</span>
plt<span style="color:#f92672">.</span>ylim(<span style="color:#f92672">-</span><span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">1.1</span>)       <span style="color:#75715e">#y轴范围</span>
plt<span style="color:#f92672">.</span>show() 


clf <span style="color:#f92672">=</span> LogisticRegression()
<span style="color:#66d9ef">print</span> clf
clf<span style="color:#f92672">.</span>fit(train_feature,label)
predict[<span style="color:#e6db74">&#39;label&#39;</span>] <span style="color:#f92672">=</span> clf<span style="color:#f92672">.</span>predict(predict_feature)

LogisticRegression(C<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, class_weight<span style="color:#f92672">=</span>None, dual<span style="color:#f92672">=</span>False, fit_intercept<span style="color:#f92672">=</span>True,
          intercept_scaling<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, max_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, multi_class<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ovr&#39;</span>, n_jobs<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
          penalty<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;l2&#39;</span>, random_state<span style="color:#f92672">=</span>None, solver<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;liblinear&#39;</span>, tol<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0001</span>,
          verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, warm_start<span style="color:#f92672">=</span>False)
</code></pre></div><h3 id="3优缺点及适用场景-3">3、优缺点及适用场景</h3>
<p><strong>优点</strong></p>
<ul>
<li>直接对分类的可能性建模，无需事先假设数据分布，避免了假设分布不准确带来的问题</li>
<li>不仅预测出类别，还可得到近似概率预测</li>
<li>对率函数是任意阶可导凸函数，有很好得数学性质，很多数值优化算法可直接用于求取最优解</li>
<li>容易使用和解释，计算代价低</li>
<li>LR对时间和内存需求上相当高效</li>
<li>可应用于分布式数据，并且还有在线算法实现，用较小资源处理较大数据</li>
<li>对数据中小噪声鲁棒性很好，并且不会受到轻微多重共线性影响</li>
<li>因为结果是概率，可用作排序模型</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>容易欠拟合，分类精度不高</li>
<li>数据特征有缺失或特征空间很大时效果不好</li>
</ul>
<p><strong>应用场景</strong></p>
<ul>
<li>CTR预估/推荐系统的learning to rank/各种分类场景。</li>
<li>某搜索引擎厂的广告CTR预估基线版是LR。</li>
<li>某电商搜索排序/广告CTR预估基线版是LR。</li>
<li>某电商的购物搭配推荐用了大量LR。</li>
<li>某现在一天广告赚1000w+的新闻app排序基线是LR。</li>
</ul>
<h2 id="五支持向量机">五、支持向量机</h2>
<h3 id="1主要思想-4">1、主要思想</h3>
<p>SVM是一种二类分类模型，它的基本模型是的定义在特征空间上的<strong>间隔最大</strong>的线性分类器。S.M的学习策略就是间隔最大化，是一种用来解决二分类问题的机器学习算法，它通过在样本空间中找到一个划分超平面，将不同类别的样本分开，同时使得两个点集到此平面的最小距离最大，两个点集中的边缘点到此平面的距离最大。</p>
<p>先来看看下面这个图:</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202146884.png" alt="img"></p>
<p>图中有分别属于两类的一些二维数据点和三条直线。如果三条直线分别代表三个分类器的话，请问哪一个分类器比较好？</p>
<p>我们凭直观感受应该觉得答案是H3。首先H1不能把类别分开，这个分类器肯定是不行的；H2可以，但分割线与最近的数据点只有很小的间隔，如果测试数据有一些噪声的话可能就会被H2错误分类(即对噪声敏感、泛化能力弱)。H3以较大间隔将它们分开，这样就能容忍测试数据的一些噪声而正确分类，是一个泛化能力不错的分类器。</p>
<p>对于支持向量机来说，数据点若是p维向量，我们用p+1维的超平面来分开这些点。但是可能有许多超平面可以把数据分类。最佳超平面的一个合理选择就是以最大间隔把两个类分开的超平面。因此，SVM选择能够使离超平面最近的数据点的到超平面距离最大的超平面。之所以选择距离最大的超平面是因为距离最大的超平面具有最好的泛化性能。</p>
<p>以上介绍的SVM只能解决线性可分的问题，为了解决更加复杂的问题，支持向量机学习方法有一些由简至繁的模型:</p>
<ul>
<li>线性可分SVM</li>
</ul>
<blockquote>
<p>当训练数据线性可分时，通过硬间隔最大化可以学习得到一个线性分类器，即硬间隔SVM，如上图的的H3。</p>
</blockquote>
<ul>
<li>线性SVM</li>
</ul>
<blockquote>
<p>当训练数据不能线性可分但是可以近似线性可分时，通过软间隔最大化也可以学习到一个线性分类器，即软间隔SVM。</p>
</blockquote>
<ul>
<li>非线性SVM</li>
</ul>
<blockquote>
<p>当训练数据线性不可分时，通过使用核技巧(kernel trick)和软间隔最大化，可以学习到一个非线性SVM。</p>
</blockquote>
<p>一个超平面由法向量w和截距b决定,其方程为<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202209806.png" alt="image-20220220220915756">, 可以规定法向量指向的一侧为正类,另一侧为负类。</p>
<p>为了找到最大间隔超平面，我们可以先选择分离两类数据的两个平行超平面，使得它们之间的距离尽可能大。在这两个超平面范围内的区域称为“间隔(margin)”，最大间隔超平面是位于它们正中间的超平面。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202212110.png" alt="image-20220220221213060"></p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202215594.png" alt="image-20220220221518526"></p>
<p>通过求解上式即可得到最优超平面。这被称为原始问题。</p>
<p>可以应用拉格朗日乘子法构造拉格朗日函数(Lagrange function)再通过求解其对偶问题(dual problem)得到原始问题的最优解。转换为对偶问题来求解的原因是:</p>
<ul>
<li>
<p>对偶问题更易求解，由下文知对偶问题只需优化一个变量α且约束条件更简单；</p>
</li>
<li>
<p>能更加自然地引入核函数，进而推广到非线性问题。</p>
<p>具体推导细节可参见书籍《统计学习方法》</p>
</li>
</ul>
<p><strong>支持向量</strong></p>
<p>在线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的数据点称为支持向量。<strong>在决定最佳超平面时只有支持向量起作用，而其他数据点并不起作用</strong>。如果移动非支持向量，甚至删除非支持向量都不会对最优超平面产生任何影响。也即支持向量对模型起着决定性的作用，这也是“支持向量机”名称的由来。</p>
<p><strong>软间隔支持向量机</strong></p>
<p>间隔线性支持向量机是用来解决训练数据完全线性可分情况的算法，但由于我们实际获取的真实样本往往会存在噪声，使得训练数据不是清晰线性可分的，又或者即使我们找到了一个可以使不同类样本完全分开的超平面，也很难确定这个线性可分的结果是不是由于过拟合导致的，比如下图的情况，为此引入了“软间隔”（soft margin）的概念。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202220286.jpeg" alt="img"></p>
<p>具体来说，硬间隔支持向量机要求所有的样本均被最佳超平面正确划分，而软间隔支持向量机允许某些样本点不满足间隔大于等于1的条件，当然在最大化间隔的时候也要限制不满足间隔大于等于1的样本的个数使之尽可能的少。于是我们引入一个惩罚系数C&gt;0，并对每个样本点(xi,yi)引入一个松弛变量（slack variables）ξ≥0。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202222694.png" alt="image-20220220222216642"></p>
<p>C越大对误分类的惩罚越大，C越小对误分类的惩罚越小。上式是软间隔支持向量机的原始问题。可以证明w的解是唯一的，b的解不是唯一的，b的解是在一个区间内。对比软间隔支持向量机的对偶问题和硬间隔支持向量机的对偶问题可发现二者的唯一差别就在于对偶变量的约束不同，软间隔支持向量机对对偶变量的约束是0≤αi≤C，硬间隔支持向量机对对偶变量的约束是0≤αi，于是可采用和硬间隔支持向量机相同的解法求解。</p>
<p>前面介绍的都是线性问题，但是我们经常会遇到非线性的问题(例如异或问题)，此时就需要用到核技巧(kernel trick)将线性支持向量机推广到非线性支持向量机。需要注意的是，不仅仅是SVM，很多线性模型都可以用核技巧推广到非线性模型，例如核线性判别分析(KLDA)。</p>
<p><strong>核函数</strong></p>
<p>核技巧的基本思路分为两步:使用一个变换将原空间的数据映射到新空间(例如更高维甚至无穷维的空间)；然后在新空间里用线性方法从训练数据中学习得到模型。</p>
<p>*<em><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202225758.jpeg" alt="img"></em></p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202227316.png" alt="image-20220220222726223"></p>
<p>实际问题中一般使用已有的核函数，下面给出一些常用的核函数。目的就在于对非线性可分数据，简化高维空间的复杂运算，让数据进行维度转化，实质上就是将数据在低维进行了计算之后，再转换到高维空间，直接避免了在高维空间的复杂运算。</p>
<ul>
<li>多项式核函数(polynomial kernel function)</li>
</ul>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202228280.png" alt="image-20220220222859211"></p>
<ul>
<li>高斯核函数(Guassian kernel function)</li>
</ul>
<p>​	<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202229602.png" alt="image-20220220222917547"></p>
<p>​	LinearSVC：主要用于线性可分的情形。参数少，速度快，对于一般数据，分类效果已经很理想
​    RBF:主要用于线性不可分的情形。参数多，分类结果非常依赖于参数
​    polynomial:多项式函数,degree 表示多项式的程度&mdash;&ndash;支持非线性分类
​    Sigmoid：在生物学中常见的S型的函数，也称为S型生长曲线</p>
<p>图像分类，通常使用高斯径向基和函数，因为分类较为平滑，文字不适用高斯径向基和函数。没有标准的答案，可以尝试各种核函数，根据精确度判定。</p>
<p>利用核技巧可以很简单地把线性支持向量机扩展到非线性支持向量机，只需将线性支持向量机中的內积换成核函数即可。下面简述非线性支持向量机学习算法。</p>
<ul>
<li>首先选取适当的核函数<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202230310.png" alt="image-20220220223045099">和适当的参数<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202231090.png" alt="image-20220220223123016">，构造最优化问题</li>
</ul>
<p>​	<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202231033.png" alt="image-20220220223157970"></p>
<ul>
<li>再利用现成的二次规划问题求解算法或者SMO算法求得最优解<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202232780.png" alt="image-20220220223255725">,</li>
<li><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202233029.png" alt="image-20220220223331960"></li>
</ul>
<h3 id="2实现代码-4">2、实现代码</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> svm
<span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> model_selection
<span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> StandardScaler  <span style="color:#75715e"># 归一化</span>
<span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> accuracy_score

trd <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_excel(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;svmtrain.xlsx&#39;</span>)
height,width <span style="color:#f92672">=</span>trd<span style="color:#f92672">.</span>shape
<span style="color:#75715e">#将训练集转化为二维数组</span>
train <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((height,width))
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>,height):
	<span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>,width<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>): <span style="color:#75715e">#遍历的实际下标，即excel第一行</span>
		train[i][j<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> trd<span style="color:#f92672">.</span>iat[i,j<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]

y_train, x_train <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>split(train,            <span style="color:#75715e">#要切分的数组</span>
                (<span style="color:#ae81ff">3</span>,),                         <span style="color:#75715e">#沿轴切分的位置，第4列开始往后为x_train自变量</span>
                axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)                       <span style="color:#75715e">#按纵向进行分割</span>

<span style="color:#75715e">#将测试集转化为二维数组</span>
ted <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_excel(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;svmtest.xlsx&#39;</span>)
height1,width1 <span style="color:#f92672">=</span>ted<span style="color:#f92672">.</span>shape
test <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((height1,width1))
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>,height1):
	<span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>,width1<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>): <span style="color:#75715e">#遍历的实际下标，即excel第一行</span>
		test[i][j<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> ted<span style="color:#f92672">.</span>iat[i,j<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]

y_test, x_test <span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>split(test,            <span style="color:#75715e">#要切分的数组</span>
                (<span style="color:#ae81ff">3</span>,),                     <span style="color:#75715e">#沿轴切分的位置，第4列开始往后为x_test自变量</span>
                axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> )

<span style="color:#75715e"># #归一化</span>
scaler <span style="color:#f92672">=</span> StandardScaler()
x_train <span style="color:#f92672">=</span> scaler<span style="color:#f92672">.</span>fit_transform(x_train)
x_test <span style="color:#f92672">=</span> scaler<span style="color:#f92672">.</span>transform(x_test)


<span style="color:#75715e">#预测的前三列.按列单独提取前三列标签</span>
train1<span style="color:#f92672">=</span> y_train[:, <span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>flatten()
train2<span style="color:#f92672">=</span> y_train[:, <span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>flatten()
train3<span style="color:#f92672">=</span> y_train[:, <span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">3</span>]<span style="color:#f92672">.</span>flatten()

<span style="color:#75715e"># model_selection.train_test_split(x_train,</span>
<span style="color:#75715e">#                         train1,              #所要划分的样本结果</span>
<span style="color:#75715e">#                   random_state=1, #随机数种子确保产生的随机数组相同</span>
<span style="color:#75715e">#                  test_size=0.28)  #测试样本占比</span>

<span style="color:#75715e"># model_selection.train_test_split(x_train,</span>
<span style="color:#75715e">#                         train2,              #所要划分的样本结果</span>
<span style="color:#75715e">#                   random_state=1, #随机数种子确保产生的随机数组相同</span>
<span style="color:#75715e">#                  test_size=0.28)  #测试样本占比</span>

model_selection<span style="color:#f92672">.</span>train_test_split(x_train,
                        train3,              <span style="color:#75715e">#所要划分的样本结果</span>
                  random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, <span style="color:#75715e">#随机数种子确保产生的随机数组相同</span>
                 test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.28</span>)  <span style="color:#75715e">#测试样本占比</span>


<span style="color:#75715e">#**********************SVM分类器构建*************************</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">classifier</span>():
    clf <span style="color:#f92672">=</span> svm<span style="color:#f92672">.</span>SVC( C<span style="color:#f92672">=</span><span style="color:#ae81ff">2.5</span>,                         <span style="color:#75715e">#误差项惩罚系数,默认值是1</span>
                  kernel<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;rbf&#39;</span>,               <span style="color:#75715e">#线性核 kenrel=&#34;rbf&#34;:高斯核</span>
                  decision_function_shape<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ovr&#39;</span>) <span style="color:#75715e">#决策函数一个类别与其他类进行划分</span>
    <span style="color:#66d9ef">return</span> clf
clf <span style="color:#f92672">=</span> classifier()

<span style="color:#75715e">#***********************训练模型*****************************</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train</span>(clf,x_train,y_train):
    clf<span style="color:#f92672">.</span>fit(x_train,         <span style="color:#75715e">#训练集特征向量，fit表示输入数据开始拟合</span>
            y_train)

    <span style="color:#75715e"># 训练SVM模型1</span>
<span style="color:#75715e"># train(clf,x_train,train1)</span>
<span style="color:#75715e"># outcome1=clf.predict(x_test)</span>
<span style="color:#75715e"># print(outcome1)</span>

    <span style="color:#75715e"># 训练SVM模型2</span>
<span style="color:#75715e"># train(clf,x_train,train2)</span>
<span style="color:#75715e"># outcome2=clf.predict(x_test)</span>
<span style="color:#75715e"># print(outcome2)</span>
    <span style="color:#75715e"># 训练SVM模型3</span>
train(clf,x_train,train3)
outcome3<span style="color:#f92672">=</span>clf<span style="color:#f92672">.</span>predict(x_test)
<span style="color:#66d9ef">print</span>(outcome3)



<span style="color:#75715e"># 模型评估</span>
<span style="color:#75715e">#精确度第一列</span>
<span style="color:#75715e"># tra_label= clf.predict(x_train)</span>
<span style="color:#75715e"># print(&#34;第一列精确度：&#34;, accuracy_score(train1,tra_label) )</span>
<span style="color:#75715e">#精确度第二列</span>
<span style="color:#75715e"># tra_label= clf.predict(x_train)</span>
<span style="color:#75715e"># print(&#34;第二列精确度：&#34;, accuracy_score(train2,tra_label) )</span>
<span style="color:#75715e">#精确度第三列</span>
tra_label<span style="color:#f92672">=</span> clf<span style="color:#f92672">.</span>predict(x_train)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;第三列精确度：&#34;</span>, accuracy_score(train3,tra_label) )


</code></pre></div><h3 id="3优缺点及适用场景-4">3、优缺点及适用场景</h3>
<p><strong>优点</strong></p>
<ol>
<li>
<p>由于SVM是一个凸优化问题，所以求得的解一定是全局最优而不是局部最优。</p>
</li>
<li>
<p>不仅适用于线性线性问题还适用于非线性问题(用核技巧)。</p>
</li>
<li>
<p>拥有高维样本空间的数据也能用SVM，这是因为数据集的复杂度只取决于支持向量而不是数据集的维度，这在某种意义上避免了“维数灾难”。</p>
</li>
<li>
<p>理论基础比较完善(例如神经网络就更像一个黑盒子)。</p>
</li>
<li>
<p>有严格的数学理论支持，可解释性强，不依靠统计方法，从而简化了通常的分类和回归问题；</p>
</li>
<li>
<p>能找出对任务至关重要的关键样本（即：支持向量）；</p>
</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>只适用于二分类问题。(SVM的推广SVR也适用于回归问题；可以通过多个SVM的组合来解决多分类问题)</li>
<li>训练时间长。当采用 SMO 算法时，由于每次都需要挑选一对参数，因此时间复杂度为<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202241271.png" alt="image-20220220224123209"> ，其中 N 为训练样本的数量；</li>
<li>当采用核技巧时，如果需要存储核矩阵，则空间复杂度为 <img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202202241955.png" alt="">；</li>
<li>模型预测时，预测时间与支持向量的个数成正比。当支持向量的数量较大时，预测计算复杂度较高。</li>
</ol>
<p>因此支持向量机目前只适合小批量样本的任务，无法适应百万甚至上亿样本的任务</p>
<h2 id="六adaboost元算法">六、AdaBoost元算法</h2>
<h3 id="1主要思想及代码">1、主要思想及代码</h3>
<p>​    当我们在做决策时，通常会考虑多人的意见而不只是一个人的。机器学习在做决策时同样也可以如此，这就是元算法的核心思想。AdaBoost是一种最流行的元算法，它被认为是最好的监督学习算法。‘</p>
<p>不同的分类算法，如k-近邻算法、决策树、朴素贝叶斯、Logistic回归、支持向量机等，都有各自的优缺点。我们可以把不同的分类器组合起来，这种组合结果就被称为元算法。而且组合的方式很多，可以是不同算法的组合，也可以是同一算法在不同设置下的组合，还可以是数据集的不同部分分配给不同的分类器的组合。AdaBoost是boosting方法最流行的一种版本。那么什么是boosting呢？boosting使用的多个分类器的类型是一样的，但是不同的分类器是通过串行训练获得的，每个新分类器都根据已训练出的分类器的性能来进行训练。boosting是通过集中关注被已有分类器错分的数据来获得新的分类器。</p>
<p>　<strong>集成学习</strong>或者<strong>元算法</strong>的一般结构是：先产生一组**“个体学习器”**，再用某种策略将他们结合起来。**个体学习器**通常是由一个现有的学习算法从训练数据产生。</p>
<p>　　根据<strong>个体学习器的生成方式</strong>，目前的集成学习方法大致可分为<strong>两大类</strong>，即</p>
<p>　　1.个体学习器间存在强依赖关系、必须串行生成的<strong>序列化方法</strong>，典型的代表是<strong>Boosting</strong>，其中<strong>AdaBoost</strong>就是<strong>Boosting</strong>的最流行的一个版本</p>
<p>　　2.个体学习器间不存在强依赖关系、可同时生成的<strong>并行化方法</strong>，典型的代表是<strong>Bagging和“随机森林”（Random Forest）</strong></p>
<p>AdaBoost用来构建一个二值分类器，我们把两种类型分别表示为“+1”和“-1”。如果想要把它应用到多个类型的场合，那就需要对AdaBoost做一些修改。</p>
<p>AdaBoost使用弱分类器和多个实例来构建一个强分类器。这里的“弱”表示分类器的性能比随机猜测要好些，但也不会好很多。它的工作原理：训练数据集中的每个样本被赋予一个权重，构成权重向量D，这些权重一开始都被初始化为相等值。首先在训练数据集上训练出一个弱分类器并计算该分类器的错误率e，然后再同一数据集上再次训练弱分类器。当然，在第二次训练中，将会重新调整每个样本的权重，其中第一次分对的样本的权重降低，分错的样本的权重提高。为了从所有弱分类器中得到最终的分类结果，需要为每个弱分类器都分配一个权重值alpha。</p>
<p>错误率e的计算公式：<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211332521.png" alt="image-20220221133218392"></p>
<p>权重值alpha的计算公式：<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211333708.png" alt="image-20220221133321591"></p>
<p>在计算出alpha值之后，就可以对权重向量D进行更新，使得分对的样本的权重降低，分错的样本的权重升高，从而突出分错的数据。</p>
<p>如果某样本被分对，那么权重更改为：<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211334473.png" alt="image-20220221133417405"></p>
<p>如果某样本被分错，那么权重更改为：<img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211334925.png" alt="image-20220221133443850"></p>
<p>在计算出D之后，AdaBoost进入下一轮训练，得到新的弱分类器以及alpha值。不断地重复训练和求解alpha值，直到训练错误率为0或者弱分类器的数目得到用户指定最高值。</p>
<p>前面讲过boosting使用同一类型的弱分类器，所以Adaboost的弱分类器的类型是相同的。这里，我们基于单层决策树构建弱分类器。当然，也可以使用任意一种分类算法来构建弱分类器。单层决策树是一种简单的决策树，它只使用一个特征来做决策，即满足该特征的某个条件被分为“-1”，否则被分为“+1”。</p>
<p>下面重点来看一下如何基于AdaBoost构造一个强分类器。</p>
<p>首先，如何构造一个最优单层决策树？</p>
<p>构造一个单层决策树需要知道3个要素：用于决策的特征，基于该决策特征的分界值，小于等于分界值被分为“-1”还是大于分界值被分为“-1”。这3个要素需要通过三层循环来决定。</p>
<p>第一层循环是关于数据特征的循环，有多少特征就需要循环多少次。第二层循环是关于特征的取值的循环，获得数据集中该特征的最大值max和最小值min。我们可以规定步数step，然后就可以求出步长size=(max-min)/step。使用一个整数j从0循环到step，每次特征的取值为(min+j*size)，共step+1次循环。第三层循环是关于小于等于某特征取值被分为“-1”还是大于特征取值被分为“-1”，共2次循环。第三层循环可以得到一个分类结果，因为第一层循环确定了特征，第二层循环确定了该特征的分解值，第三层循环可以通过小于等于或大于该值给出所有数据分类，从而得到分类结果。将错分的数据的权重相加得到权重错误值。通过比较每次循环的权重错误值，得到最小的权重错误值，此情况就是所要构造的最优单层决策树。</p>
<p>记录下该单层决策树的3个要素：决策特征，特征分界值，不等号（小于等于或大于）以及最小的权重错误值和分类结果（使用该单层决策树分类的结果）。</p>
<p>然后，如何基于单层决策树进行AdaBoost训练？</p>
<p>AdaBoost训练的过程就是构造一个个弱分类器以及求解每个弱分类器权重值alpha的过程。训练过程是一个不断循环的过程，它的终止条件是训练错误率为0或者弱分类器的数目得到用户指定最高值。所以我们就需要设置弱分类器数目的最高值num并作为循环的次数，循环的过程中如果训练错误率为0，那么就直接跳出循环。在循环的过程中，我们需要计算弱分类器的alpha值、调整权重向量D以及求解训练错误率。</p>
<p>在循环开始前，需要初始化D中的每个权重为某个相同的值，并且规定初始化的D的所有权重的总和为1。根据调整的规则可以知道调整后的D的所有权重的总和肯定为1。这就使得在构造单层决策树时，求得的权重错误值即为权重错误率。</p>
<p>在每次循环的过程中，通过上述构造单层决策树的方式构造出一个弱分类器，并且得到该分类器的权重错误率e以及分类结果。根据权重错误率e可以求出alpha值，并且根据求出的alpha值调整权重向量D。</p>
<p>在循环前初始化一个0向量Error，在每次循环中，累加alpha值乘以分类结果所得的向量。用sign函数作用到向量Error的每个元素上，就得到了由已有的弱分类器构造的分类器的分类结果，统计出分错的样本的个数，然后除以所有样本的数目，得到训练错误率。当在某个次循环中，训练错误率为0，则跳出循环。每次的循环必须要保存弱分类器的3个要素以及alpha值。</p>
<p>注：sign函数，当自变量大于0时取值1，当自变量小于0时取值-1。</p>
<p>最后，如何基于弱分类器分类或构造强分类器？</p>
<p>用上述方法求得所有的弱分类器以及每个分类器的alpha值。已知某数据的所有特征值，那么，如何求得各个弱分类器的分类结果呢？任意一个弱分类器实际上就是一个单层决策树。假设某个单层决策树的3个要素分别是：第m个特征，特征分界值为k，不等号为小于等于。找到该待分类数据第m个特征的值x，如果x小于等于k，则被分为“-1”类，否则被分为“+1”类。通过这种方式就可以求得所有的弱分类器的分类结果，再分别乘以它们对应的alpha值，然后求和得到一个数值，最后用sign函数作用到这个数值上得出该数据的分类。</p>
<p>有一个误区值得大家的注意。并不是弱分类器的数目越多，测试错误率就越低。在有些情况下，随着弱分类器的数目的增加，测试错误率先下降到一个最小值之后再上升。这种现象称为过拟合。当然，如果对于“表现好”的数据集，AdaBoost的测试错误率就会达到一个稳定值，并不会随着分类器数目的增加而上升。</p>
<p>总的来说，AdaBoost是监督机器学习中最强大的算法之一，它能够快速地处理其它分类器很难处理的数据集。</p>
<p><strong>bagging：基于数据随机重抽样的分类器构建方法</strong></p>
<p><strong>自举汇聚法（bootstrap aggregating）</strong>，也称为bagging方法，它直接基于自助采样法（bootstrap samping）。</p>
<p>给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过m次随机采样操作，我们得到了含m个样本的采样集。这样从原始数据集选择T次后得到T个新数据集，且每个新数据集的大小和原数据集的大小相等。在T个新数据集建好之后，将某个学习算法分别作用于每个数据集就得到了T个分类器。当我们要对新数据集进行分类时，就可以应用这T个分类器进行分类。与此同时，选择分类器投票结果中最多的类别作为最后的分类结果（权重相等）。</p>
<p><strong>Boosting</strong></p>
<p>boosting是一种和bagging很类似的技术。其使用的多个分类器的类型都是一致的。</p>
<p>在boosting中，不同的分类器是通过串行训练而获得的，每个新分类器都根据已训练出的分类器的性能来进行训练。boosting是通过集中关注被已有分类器错分的那些数据来获得新的分类器。</p>
<p>boosting分类的结果是基于所有分类器的加权求和结果的，在bagging中的分类器权重是相等的，而boosting中的分类器权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。</p>
<p><strong>AdaBoost的一般流程：</strong></p>
<p>（1）收集数据：可以使用任意方法</p>
<p>（2）准备数据：依赖于所使用的弱分类器类型，本次使用的是单层决策树，这种分类器可以处理任何数据类型。还可以使用任意分类器作为弱分类器。其中，kNN，决策树，朴素贝叶斯，logistic回归，支持向量机任一分类器都可以充当分类器。作为弱分类器，简单分类器的效果会更好。</p>
<p>（3）分析数据：可以使用任意方法</p>
<p>（4）训练算法：AdaBoost的大部分时间都在训练上，分类器将多次在同一数据集上训练弱分类器。</p>
<p>（5）测试算法：计算分类的错误率</p>
<p>（6）使用算法：同SVM一样，AdaBoost预测两个类别中的一个。如果想把它应用到多个类别的场合，那么就要像多类SVM中的做法一样，对AdaBoost代码进行修改。</p>
<p><strong>弱分类器</strong>的“弱”意味着分类器的性能比随机猜测要略好，但是也不会好太多。这就是说，在二分类情况下，弱分类器的错误率会高于50%，而强分类器的错误率会低很多。</p>
<p><strong>AdaBoost运行过程：</strong></p>
<ol>
<li>训练数据中的每个样本，赋予一个权重，这些权重构成了向量D。</li>
<li>一开始，这些权重都初始化成相等值，首先在训练数据上训练出一个弱分类器的并计算该分类器的错误率，然后在同一数据集上再次训练弱分类器。</li>
<li>在分类器第二次训练当中，将会重新调整每个样本的权重，其中第一次分对的样本的权重会降低，而第一次分错的样本的权重将会提高。</li>
<li>为了从所有弱分类器中得到最终的分类结果，AdaBoost为每个分类器都配了一个权重值alpha，这些alpha值是基于每个弱分类器的错误率进行计算的。</li>
</ol>
<p>其中，错误率 ε 定义为：<strong>ε = 未正确分类的样本数目 / 所有的样本数目</strong></p>
<p>其中，alpha的公式如下<strong>a = ln(1-ε\ε) * 1\2</strong></p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211622854.jpeg" alt="img"></p>
<p>计算出alpha 之后，可以对权重向量D进行更新，使得正确分类的样本权重降低而错分样本的权重升高。</p>
<p>如果某个样本被正确分类，那么该样本的权重更改为：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211826960.png" alt="img"></p>
<p>如果某个样本被错误分类，那么该样本的权重更改为：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211826366.png" alt="img"></p>
<p>计算出D之后，AdaBoost算法会进行下一次迭代。算法不断重复训练和调整权重的过程，直到训练错误率为0或者弱分类的数目达到用户的指定值为止。</p>
<p><strong>基于单层决策树构建弱分类器</strong></p>
<p>单层决策树（decision stump，也称之为决策树桩）是一种简单的决策树。这个单层决策树仅基于单个特征的来做决策。</p>
<p>我们将使用多套代码来构建单层决策树：</p>
<p>1.第一个函数将用于测试是否有某个值小于或者大于我们正在测试的阈值。</p>
<p>2.第二个函数在一个加权平均数据集中循环，并找到具有最低错误率的单层决策树。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loadSimpData</span>():
   datMat <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>matrix(
           [[ <span style="color:#ae81ff">1.</span> , <span style="color:#ae81ff">2.1</span>],
            [ <span style="color:#ae81ff">2.</span> , <span style="color:#ae81ff">1.1</span>],
            [ <span style="color:#ae81ff">1.3</span>, <span style="color:#ae81ff">1.</span> ],
            [ <span style="color:#ae81ff">1.</span> , <span style="color:#ae81ff">1.</span> ],
            [ <span style="color:#ae81ff">2.</span> , <span style="color:#ae81ff">1.</span> ]])
   classLabels <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1.0</span> , <span style="color:#ae81ff">1.0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">1.0</span>]
   <span style="color:#66d9ef">return</span> datMat, classLabels
</code></pre></div><p>第二个函数的伪代码如下：</p>
<p>&quot;&rdquo;&rdquo;
将最小错误率minError设为 +inf 大
对数据集中的每一个特征(第一层循环)：
对每个步长(第二层循环)：
对每个不等号(第三层循环)：
建立一棵单层决策树并利用加权数据集对它进行测试
如果错误率地域minError，则将当前的单层决策树设为最佳单位决策树
返回最佳单层决策树
&quot;&rdquo;&rdquo;</p>
<pre><code>#==============================================================================
# 单层决策树生成函数
def stumpClassify(dataMatrix, dimen, threshVal, threshIneq):
   retArray = np.ones((np.shape(dataMatrix)[0],1))
   if threshIneq == 'lt':
       retArray[dataMatrix[:, dimen] &lt;= threshVal] = -1.0
   else:
       retArray[dataMatrix[:, dimen] &gt; threshVal] = 1.0
   return retArray

def buildStump(dataArr, classLabels, D):
   dataMatrix = np.mat(dataArr); labelMat = np.mat(classLabels).T
   m, n = np.shape(dataMatrix)                    
   numSteps = 10.0; bestStump = {}; bestClasEst = np.mat(np.zeros((m, 1)))
   minError = inf
   for i in range(n):
       rangeMin = dataMatrix[:, i].min();rangeMax = dataMatrix[:, 1].max()
       stepSize = (rangeMax - rangeMin)/numSteps
       for j in range(-1, int(numSteps) + 1):
           for inequal in ['lt', 'gt']:
               threshVal = (rangeMin + float(j) * stepSize)
               predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)
               errArr = np.mat(np.ones((m, 1)))
               errArr[predictedVals == labelMat] = 0
               weightedError = D.T * errArr
               print &quot;split: dim %d, thresh %.2f, thresh inequal: %s, the weighted error is %.3f&quot; %(i ,threshVal, inequal,weightedError)
               if weightedError &lt; minError:
                   minError = weightedError
                   bestClasEst = predictedVals.copy()
                   bestStump['dim'] = i
                   bestStump['thresh'] = threshVal
                   bestStump['ineq'] = inequal
   return bestStump, minError, bestClasEst
</code></pre><p>至此，我们构架了一个基于加权输入值进行决策的分类器。</p>
<p>1。第一个函数stumpClassify()是通过阈值比较对数据进行分类的。所有在阈值一边的数据会分到类别-1，而在另一边的数据会分到类别+1 ，。该函数可以通过数组过滤来实现，，将返回的数组的全部元素设置为+1，然后将所有不满足不等式要求的元素设置为-1 。</p>
<p>2.第二个函数buildStump() 将遍历buildStump()函数所有可能输入的值，并找到数据集上最佳的单层决策树。（“最佳”是基于权重向量D来定义的），bestStump这个空字典集用于存储给定权重向量D所得到的最佳单层决策树的相关信息。minError 正无穷大，用于寻找可能的最小概率。</p>
<p>3.三层嵌套的for循环是程序的最主要的部分。第一层for循环在数据集的所有特征上遍历。通过计算最小值和最大值来了解需要最大的步长。第二层for循环在这些特征的值上进行遍历。最后一个for循环是在大于和小于之间切换不等式。</p>
<p>4.在三层的for循环之内，我们在数据集及三个循环变量上调用stumpClassify()函数。构建的列向量errArr，若predictedVals中的值不等于labelMat中的真正类别的标签值，errArr = 1。</p>
<p>errArr 和权重向量D乘积相应元素求和，得到weightedError。</p>
<p>5.我们是基于权重向量D而不是其他错误计算指标来评价分类器的。</p>
<p>上述的单层决策树的生成函数是决策树的一个简化版本。也就是弱分类算法。</p>
<p><strong>完整的AdaBoos 算法实现</strong></p>
<p>我们将利用构建的单层决策树来实现这个完整算法。</p>
<p>&quot;&rdquo;&rdquo;
对每次迭代：
利用buildStump()函数找到最佳的单层决策树
将最佳单层决策树加入到单层决策树数组
计算alpha
计算新的权重向量D
更新累计类别估计值
如果错误率为0.0 ， 则退出循环
&quot;&rdquo;&rdquo;</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#基于单层决策树的AdaBoost训练过程</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">adaboostTrainDS</span>(dataArr, classLabels, numIt <span style="color:#f92672">=</span> <span style="color:#ae81ff">40</span>):
   <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">   数据集，类别标签，迭代次数
</span><span style="color:#e6db74">   &#34;&#34;&#34;</span>
   weakClassArr <span style="color:#f92672">=</span> []
   m <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>shape(dataArr)[<span style="color:#ae81ff">0</span>]
   D <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mat(np<span style="color:#f92672">.</span>ones((m, <span style="color:#ae81ff">1</span>))<span style="color:#f92672">/</span>m)
   aggClassEst <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mat(np<span style="color:#f92672">.</span>zeros((m, <span style="color:#ae81ff">1</span>)))
   <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(numIt):
       bestStump, error, classEst <span style="color:#f92672">=</span> buildStump(dataArr, classLabels, D)
       <span style="color:#66d9ef">print</span> <span style="color:#e6db74">&#34;D:&#34;</span>, D<span style="color:#f92672">.</span>T
       alpha <span style="color:#f92672">=</span> float(<span style="color:#ae81ff">0.5</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>log((<span style="color:#ae81ff">1.0</span><span style="color:#f92672">-</span>error)<span style="color:#f92672">/</span>max(error, <span style="color:#ae81ff">1e-16</span>)))
       bestStump[<span style="color:#e6db74">&#39;alphas&#39;</span>] <span style="color:#f92672">=</span> alpha
       weakClassArr<span style="color:#f92672">.</span>append(bestStump)
       <span style="color:#66d9ef">print</span> <span style="color:#e6db74">&#34;classEst:&#34;</span>,classEst<span style="color:#f92672">.</span>T
       expon <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>multiply(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span><span style="color:#f92672">*</span>alpha<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>mat(classLabels)<span style="color:#f92672">.</span>T, classEst)
       D <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>multiply(D,np<span style="color:#f92672">.</span>exp(expon))
       D <span style="color:#f92672">=</span> D<span style="color:#f92672">/</span>D<span style="color:#f92672">.</span>sum()
       aggClassEst <span style="color:#f92672">+=</span> alpha<span style="color:#f92672">*</span>classEst
       <span style="color:#66d9ef">print</span> <span style="color:#e6db74">&#34;aggClassEst: &#34;</span>,aggClassEst<span style="color:#f92672">.</span>T
       aggErrors <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>multiply(np<span style="color:#f92672">.</span>sign(aggClassEst) <span style="color:#f92672">!=</span> np<span style="color:#f92672">.</span>mat(classLabels)<span style="color:#f92672">.</span>T, np<span style="color:#f92672">.</span>ones((m, <span style="color:#ae81ff">1</span>)))
       ErrorRate <span style="color:#f92672">=</span> aggErrors<span style="color:#f92672">.</span>sum()<span style="color:#f92672">/</span>m
       <span style="color:#66d9ef">print</span> <span style="color:#e6db74">&#34;total error:&#34;</span>,ErrorRate,<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>
       <span style="color:#66d9ef">if</span> ErrorRate <span style="color:#f92672">==</span><span style="color:#ae81ff">0.0</span>:
           <span style="color:#66d9ef">break</span>
   <span style="color:#66d9ef">return</span> weakClassArr
</code></pre></div><p>1.函数名称尾部的DS代表的就是单层决策树，是adaboost中最流行的弱分类器。</p>
<p>2.向量D非常重要，包含了每个数据点的权重。这些权重都赋予了相等的值，后续的迭代中，adaboost算法会增加错分数据的权重同时，降低正确分类的数据的权重。</p>
<p>3.adaboost 算法的核心在于for循环，该循环运行numTt次或者直到训练错误率为0为止。</p>
<p>4.alpha值会告诉分类器本次单层决策树输出结果的权重。其中max(error, 1e-16)用于确保没有错误的时候不会发生除零溢出。</p>
<p>观察classifierArray 的值,字典中包含了分类需要的所有信息。</p>
<p><strong>测试算法：基于AdaBoost 的分类</strong></p>
<p>每个弱分类器的结果以其对应的alpha值作为权重，所有的弱分类器的结果加权求和就得到了最后的结果。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># AdaBoost分类函数</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">adaClassify</span>(datToClass, classifierArr):
   dataMatrix <span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>mat(datToClass)
   m <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>shape(dataMatrix)[<span style="color:#ae81ff">0</span>]
   aggClassEst <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mat(np<span style="color:#f92672">.</span>zeros((m, <span style="color:#ae81ff">1</span>)))
   <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(classifierArr)):
       classEst <span style="color:#f92672">=</span> stumpClassify(dataMatrix,classifierArr[i][<span style="color:#e6db74">&#39;dim&#39;</span>],classifierArr[i][<span style="color:#e6db74">&#39;thresh&#39;</span>],classifierArr[i][<span style="color:#e6db74">&#39;ineq&#39;</span>])
       aggClassEst <span style="color:#f92672">+=</span> classifierArr[i][<span style="color:#e6db74">&#39;alphas&#39;</span>]<span style="color:#f92672">*</span>classEst
       <span style="color:#66d9ef">print</span> aggClassEst
   <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>sign(aggClassEst) 
</code></pre></div><p>ad随着迭代的进行，数据点[0,0]的分类结果越来越强。aClassify()函数就是利用训练出的多个弱分类器进行分类的函数。这两个点的分类结果也会随着迭代的进行而越来越强。</p>
<p><strong>示例：在一个难数据集上应用AdaBoost</strong></p>
<p>我们现在想利用多个单层决策树和AdaBoost来预测马疝病死亡率。</p>
<p>示例：在一个难数据集上的AdaBoost应用</p>
<p>1.收集数据：提供的文本文件</p>
<p>2.准备数据：确保类别标签是+1和-1 而不是0和1</p>
<p>3.分析数据：手工检查数据</p>
<p>4.训练算法：在数据上，利用adaBoostTrain() 函数训练出一系列的分类器</p>
<p>5.测试算法：我们拥有两个数据集。在不采用随机抽样的方法下，我们就会对AdaBoost和Logistic回归的结果进行完全对等的比较。</p>
<p>6.使用算法：观察该例子上的错误率。不过，也可构建一个Web网站，让驯马师输入马的病症然后预测马是否会死去。</p>
<p>我们给出一个向文件中加载数据的方法。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#960050;background-color:#1e0010">自适应数据加载函数</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loadDataSet</span>(fileName):
<span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">函数能够自检出特征的数目
</span><span style="color:#e6db74">&#34;&#34;&#34;</span>
   numFeat <span style="color:#f92672">=</span> len(open(fileName)<span style="color:#f92672">.</span>readline()<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#39;</span>))
   dataMat <span style="color:#f92672">=</span> []; labelMat <span style="color:#f92672">=</span> []
   fr <span style="color:#f92672">=</span> open(fileName)
   <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> fr<span style="color:#f92672">.</span>readlines():
       lineArr <span style="color:#f92672">=</span> []
       curLine <span style="color:#f92672">=</span> line<span style="color:#f92672">.</span>strip()<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#39;</span>)
       <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(numFeat <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>):
           lineArr<span style="color:#f92672">.</span>append(float(curLine[i]))
       dataMat<span style="color:#f92672">.</span>append(lineArr)
       labelMat<span style="color:#f92672">.</span>append(float(curLine[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]))
   <span style="color:#66d9ef">return</span> dataMat, labelMat
</code></pre></div><p>上述函数能够自检出特征的数目，函数还能假定最后一个特征是类别标签。</p>
<p>将弱分类器的数目设定为1到10000之间的几个不同的数字，运行上述过程。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211842181.jpeg" alt="img"></p>
<p>观察上表，我们发现错率达到一个最小值后又开始上升，这称之为 <strong>过拟合</strong>。</p>
<p><strong>非均衡分类问题</strong></p>
<p>在前几张的分类器构造里面，我们最终讨论的都是错误率。如果有人牵过来一匹马，让我们预测他是否会生存，我们说马会死，可能马很可能被实施安乐死。我们的预测也许是错误的，马也许本来可以继续活着。但是坦白的说，大多数情况下不同类别的分类代价并不相等。</p>
<p><strong>其他分类器性能度量指标：*正确率、召回率、以及ROC曲线*</strong></p>
<p>在之前的讨论中，我们都是基于错误率来衡量分类器任务的成功程度的。错误率指的是在所有的测试样本样例中错分的样本的比例。实际上这样的度量错误的掩饰了样例如何被分错误的事实。</p>
<p>在机器学习中的，有一个普遍称之为混淆矩阵的工具，可以帮助人们更好的了解分类中的错误，</p>
<p>在下面这个二类问题中，如果将一个正例判别为正例，那么就认为产生了一个真正例(True , Positive ，TP , 也称之为真阳)；若对一个反例正确的判为反例，就认为产生了一个真反例(True ， Negitive，也称之为真阴)，另外两只情况分别称伪反例(FN ， 假阴)和伪正例(FP ，假阳)。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211844067.png" alt="img"></p>
<p>在分类中，当某个指标的重要性高于其他类别时候，我们就可以利用上述定义来定义出多个比错误率更好的新指标。</p>
<p>第一个指标是<strong>正确率</strong>（precision），它等于 TP/( TP + FP ) ，给出的是预测能力为正例的样本中的真正正例的比例。</p>
<p>第二个是<strong>召回率</strong> （Recall），它等于 TP/( TP + FN ) ， 给出的是预测能力为正例的真实比例占所有真实比例的比例。</p>
<p>在召回率很大的f分类器中，真正判错正例的数目并不多。</p>
<p>另一个用于度量分类中的非均衡性的工具是ROC曲线(ROC curce)，ROC代表<strong>接收者操作特征（receiver operating characterristic）</strong>。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211844658.jpeg" alt="img"></p>
<p>上图的RCO曲线中，给出了一条虚线和一条实线。虚线给出的是随机猜测的结果曲线。</p>
<p>横坐标轴是伪正例的比例（假阳率=FP/(FP + TN)），而纵轴是真正例的比例（真阳率 = TP/(TP + FN)）。ROC曲线给出了 当阈值变化时假阳率和真阳率的变化情况。</p>
<p>左下角的点所对应的是将所有的样例判为反例的情况，右上角的点对应的则是将所有样例判别为正例的情况。</p>
<p>ROC曲线可以用于比较分类器，还可以基于成本效益分析来做出决策。</p>
<p>在理想的情况下，最佳的分类器应该尽可能的处于左下角，这也意味着分类器在假阳率很低的同时也获得了很高的真阳率。例如，在垃圾邮件过滤中，这相当于过滤了所有的垃圾邮件，但是没有将任何的合法邮件表示为垃圾邮件。</p>
<p>对于不同ROC曲线进行比较的一个指标是<strong>曲线下的面积</strong>（area unser curve ，<strong>AUC</strong>）。AUC给出的是分类器的平均性能值，并不能完全代替对整条曲线的观察。一个完美分类器的AUC为1.0 ，随机猜测的AUC为0.5 。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python"><span style="color:#75715e"># ROC曲线的绘制以及AUC计算函数</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plotROC</span>(predStrengths,classLabels):
   cur <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">1.0</span>)
   ySum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
   numPosClas <span style="color:#f92672">=</span> sum(np<span style="color:#f92672">.</span>array(classLabels) <span style="color:#f92672">==</span> <span style="color:#ae81ff">1.0</span>)
   yStep <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>float(numPosClas)
   xStep <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>float(len(classLabels) <span style="color:#f92672">-</span> numPosClas)
   sortedIndicies <span style="color:#f92672">=</span> predStrengths<span style="color:#f92672">.</span>argsort()
   fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>figure()
   fig<span style="color:#f92672">.</span>clf()
   ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">111</span>)
   <span style="color:#66d9ef">for</span> index <span style="color:#f92672">in</span> sortedIndicies<span style="color:#f92672">.</span>tolist()[<span style="color:#ae81ff">0</span>]:
       <span style="color:#66d9ef">if</span> classLabels[index] <span style="color:#f92672">==</span> <span style="color:#ae81ff">1.0</span>:
           delX <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; delY <span style="color:#f92672">=</span> yStep
       <span style="color:#66d9ef">else</span>:
           delX <span style="color:#f92672">=</span> xStep; delY <span style="color:#f92672">=</span> yStep; ySum <span style="color:#f92672">+=</span> cur[<span style="color:#ae81ff">1</span>]
       ax<span style="color:#f92672">.</span>plot([cur[<span style="color:#ae81ff">0</span>], cur[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">-</span>delX], [cur[<span style="color:#ae81ff">1</span>], cur[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">-</span>delY], c <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;b&#39;</span>)
       cur <span style="color:#f92672">=</span> (cur[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">-</span> delX, cur[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">-</span> delY)
   ax<span style="color:#f92672">.</span>plot([<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>], [<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>], <span style="color:#e6db74">&#39;b--&#39;</span>)
   plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;False Positive Rate&#39;</span>); plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;True positive Rate&#39;</span>)
   plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;ROC curve for AdaBoost Horse colic Detection System&#39;</span>)
   ax<span style="color:#f92672">.</span>axis([<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>])
   plt<span style="color:#f92672">.</span>show()
   <span style="color:#66d9ef">print</span> <span style="color:#e6db74">&#34;the Area Under the Curve is:&#34;</span>,ySum<span style="color:#f92672">*</span>xStep
</code></pre></div><p>1.参数predStrengths 是numpy数组或行向量矩阵参数代表分类器的预测强度。</p>
<p>2.为了计算AUC，我们对多个小矩形的面积进行累加。这些矩形的宽度都是xStep，我们对所有的矩形的高度进行累加，最后乘以xStep得到总面积。</p>
<p><strong>基于代价函数的分类器决策控制</strong></p>
<p>除了调节分类器的阈值之外，还有可用于处理非均衡分类的代价方法，其中一种称之为，<strong>代价敏感学习 。</strong></p>
<p>下表一的代价矩阵，给出了目前为止分类器的代价矩阵（代价不是0就是1），基于该代价算计总代价：TP * 0 + FN * 1 + FP * 1 + TN * 0 。</p>
<p>下表二该代价矩阵的分类代价计算公式为：</p>
<blockquote>
<p>TP * (-5) + FN * 1 + FP * 50 + TN * 0 。</p>
</blockquote>
<p>若在构建分类器时，挚爱这些代价之，那么就可以选择付出最小代价的分类器。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211846026.jpeg" alt="img"></p>
<p>在分类算法中，我们有很多方法可以来引入代价信息。</p>
<p>在adaboost中，可以基于代价函数来调整错误权重向量D；</p>
<p>在朴素贝叶斯中，可以选择具有最小期望代价而不是最大概率的类别作为最后的结果 ；</p>
<p>在SVM中，可以在代价函数中对于不同的类别选择不同的参数C 。</p>
<p><strong>处理非均衡问题的数据抽样方法</strong></p>
<p>另一种针对非均衡问题调节分类器的方法，就是对分类器的训练数据进行改造，这样就可以通过欠抽样或者过抽样来实现。</p>
<p>过抽样意味着复制样例，欠抽样意味着删除样例。</p>
<p>无论采取哪种形式，数据都会从原始形式改造为新形式。</p>
<p>抽样过程可以用随机方式或者某个预定的方式来实现。</p>
<p><strong>小结</strong></p>
<p>集成方法通过组合多个分类器的分类结果，获得较简单的单分类器更好的分类结果。</p>
<p>本章以单层决策树作为弱分类器构建了adaboost分类器。adaboost函数可以应用于任意的分类器，只要该分类器可以处理加权数据即可。</p>
<h3 id="2优缺点及适用场景">2、优缺点及适用场景</h3>
<p><strong>优点</strong>：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整</p>
<p><strong>缺点</strong>：对离群点敏感</p>
<p>使用数据类型：数值型和标称型数据</p>
<h2 id="七线性回归">七、线性回归</h2>
<h3 id="1主要思想-5">1、主要思想</h3>
<p><strong>线性与非线性</strong></p>
<p><strong>线性</strong>：两个变量之间的关系是一次函数关系的——图象是直线，叫做线性。</p>
<p>注意：线性是指广义的线性，也就是数据与数据之间的关系。</p>
<p><strong>非线性</strong>：两个变量之间的关系不是一次函数关系的——图象不是直线，叫做非线性。</p>
<p>那到底什么时候可以使用线性回归呢？统计学家安斯库姆给出了四个数据集，被称为安斯库姆四重奏。</p>
<p>从这四个数据集的分布可以看出，并不是所有的数据集都可以用一元线性回归来建模。现实世界中的问题往往更复杂，变量几乎不可能非常理想化地符合线性模型的要求。因此使用线性回归，需要遵守下面几个假设：</p>
<ul>
<li>线性回归是一个回归问题。</li>
<li>要预测的变量 y 与自变量 x 的关系是线性的（图2 是一个非线性）。</li>
<li>各项误差服从正太分布，均值为0，与 x 同方差（图4 误差不是正太分布）。</li>
<li>变量 x 的分布要有变异性。</li>
<li>多元线性回归中不同特征之间应该相互独立，避免线性相关。</li>
</ul>
<p><strong>回归问题与分类问题</strong></p>
<p>与回归相对的是分类问题（classification），分类问题要预测的变量y输出集合是有限的，预测值只能是有限集合内的一个。当要预测的变量y输出集合是无限且连续，我们称之为回归。比如，天气预报预测明天是否下雨，是一个二分类问题；预测明天的降雨量多少，就是一个回归问题。</p>
<p><strong>变量之间是线性关系</strong></p>
<p>线性通常是指变量之间保持等比例的关系，从图形上来看，变量之间的形状为直线，斜率是常数。这是一个非常强的假设，数据点的分布呈现复杂的曲线，则不能使用线性回归来建模。可以看出，四重奏右上角的数据就不太适合用线性回归的方式进行建模。</p>
<p><strong>误差服从均值为零的正太分布</strong></p>
<p>前面最小二乘法求解过程已经提到了误差的概念，误差可以表示为误差 = 实际值 - 预测值。</p>
<p>可以这样理解这个假设：线性回归允许预测值与真实值之间存在误差，随着数据量的增多，这些数据的误差平均值为0；从图形上来看，各个真实值可能在直线上方，也可能在直线下方，当数据足够多时，各个数据上上下下相互抵消。如果误差不服从均值为零的正太分布，那么很有可能是出现了一些异常值，数据的分布很可能是安斯库姆四重奏右下角的情况。</p>
<p>这也是一个非常强的假设，如果要使用线性回归模型，那么必须假设数据的误差均值为零的正太分布。</p>
<p><strong>变量 x 的分布要有变异性</strong></p>
<p>线性回归对变量 x也有要求，要有一定变化，不能像安斯库姆四重奏右下角的数据那样，绝大多数数据都分布在一条竖线上。</p>
<p><strong>多元线性回归不同特征之间相互独立</strong></p>
<p>如果不同特征不是相互独立，那么可能导致特征间产生共线性，进而导致模型不准确。举一个比较极端的例子，预测房价时使用多个特征：房间数量，房间数量*2，-房间数量等，特征之间是线性相关的，如果模型只有这些特征，缺少其他有效特征，虽然可以训练出一个模型，但是模型不准确，预测性差。</p>
<p><strong>多重共线性</strong>
在多元线性回归模型经典假设中，其重要假定之一是回归模型的解释变量之间不存在线性关系，也就是说，解释变量X1，X2，……，Xk中的任何一个都不能是其他解释变量的线性组合。如果违背这一假定，即线性回归模型中某一个解释变量与其他解释变量间存在线性关系，就称线性回归模型中存在多重共线性。严重的多重共线性可能会产生问题，因为它可以增大回归系数的方差，使它们变得不稳定。以下是不稳定系数导致的一些后果：</p>
<p>即使预测变量和响应之间存在显著关系，系数也可能看起来并不显著。</p>
<p>高度相关的预测变量的系数在样本之间差异很大。</p>
<p>从模型中去除任何高度相关的项都将大幅影响其他高度相关项的估计系数。高度相关项的系数甚至会包含错误的符号。</p>
<p><strong>共线性出现的原因</strong></p>
<p>多重共线性问题就是指一个解释变量的变化引起另一个解释变量地变化。</p>
<p>原本自变量应该是各自独立的，根据回归分析结果，能得知哪些因素对因变量Y有显著影响，哪些没有影响。如果各个自变量x之间有很强的线性关系，就无法固定其他变量，也就找不到x和y之间真实的关系了。除此以外，多重共线性的原因还可能包括：</p>
<ul>
<li>
<p>数据不足。在某些情况下，收集更多数据可以解决共线性问题。</p>
</li>
<li>
<p>错误地使用虚拟变量。（比如，同时将男、女两个虚拟变量都放入模型，此时必定出现共线性，称为完全共线性）</p>
</li>
</ul>
<p><strong>共线性的判别指标</strong></p>
<p>有多种方法可以检测多重共线性，较常使用的是回归分析中的VIF值，VIF值越大，多重共线性越严重。一般认为VIF大于10时（严格是5），代表模型存在严重的共线性问题。有时候也会以容差值作为标准，容差值=1/VIF，所以容差值大于0.1则说明没有共线性(严格是大于0.2)，VIF和容差值有逻辑对应关系，两个指标任选其一即可。</p>
<p>除此之外，直接对自变量进行相关分析，查看相关系数和显著性也是一种判断方法。如果一个自变量和其他自变量之间的相关系数显著，则代表可能存在多重共线性问题。</p>
<p>如存在严重的多重共线性问题，可以考虑使用以下几种方法处理：</p>
<p>（1）手动移除出共线性的变量</p>
<p>先做下相关分析，如果发现某两个自变量X（解释变量）的相关系数值大于0.7，则移除掉一个自变量（解释变量），然后再做回归分析。此方法是最直接的方法，但有的时候我们不希望把某个自变量从模型中剔除，这样就要考虑使用其他方法。</p>
<p>（2）逐步回归法</p>
<p>让系统自动进行自变量的选择剔除，使用逐步回归将共线性的自变量自动剔除出去。此种解决办法有个问题是，可能算法会剔除掉本不想剔除的自变量，如果有此类情况产生，此时最好是使用岭回归进行分析。</p>
<p>（3）增加样本容量</p>
<p>增加样本容量是解释共线性问题的一种办法，但在实际操作中可能并不太适合，原因是样本量的收集需要成本时间等。</p>
<p>（4）岭回归</p>
<p>上述第1和第2种解决办法在实际研究中使用较多，但问题在于，如果实际研究中并不想剔除掉某些自变量，某些自变量很重要，不能剔除。此时可能只有岭回归最为适合了。岭回归是当前解决共线性问题最有效的解释办法。</p>
<p><strong>常用的回归模型评估指标</strong></p>
<ul>
<li>解释方差（ Explained variance score）</li>
<li>绝对平均误差（Mean absolute error）</li>
<li>均方误差（Mean squared error）</li>
<li>决定系数（R² score）</li>
</ul>
<h3 id="2实现代码-5">2、实现代码</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
 
x<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">5</span>],dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float)
y<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">3.0</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">5</span>])
plt<span style="color:#f92672">.</span>scatter(x,y)
 
x_mean<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>mean(x)
y_mean<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>mean(y)
num<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>
d<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>
<span style="color:#66d9ef">for</span> x_i,y_i <span style="color:#f92672">in</span> zip(x,y):
    num<span style="color:#f92672">+=</span>(x_i<span style="color:#f92672">-</span>x_mean)<span style="color:#f92672">*</span>(y_i<span style="color:#f92672">-</span>y_mean)
    d<span style="color:#f92672">+=</span>(x_i<span style="color:#f92672">-</span>x_mean)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>
    a<span style="color:#f92672">=</span>num<span style="color:#f92672">/</span>d
    b<span style="color:#f92672">=</span>y_mean<span style="color:#f92672">-</span>a<span style="color:#f92672">*</span>x_mean
y_hat<span style="color:#f92672">=</span>a<span style="color:#f92672">*</span>x<span style="color:#f92672">+</span>b
 
plt<span style="color:#f92672">.</span>figure(<span style="color:#ae81ff">2</span>)
plt<span style="color:#f92672">.</span>scatter(x,y)
plt<span style="color:#f92672">.</span>plot(x,y_hat,c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r&#39;</span>)
x_predict<span style="color:#f92672">=</span><span style="color:#ae81ff">4.8</span>
y_predict<span style="color:#f92672">=</span>a<span style="color:#f92672">*</span>x_predict<span style="color:#f92672">+</span>b
<span style="color:#66d9ef">print</span>(y_predict)
plt<span style="color:#f92672">.</span>scatter(x_predict,y_predict,c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;b&#39;</span>,marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;+&#39;</span>)
</code></pre></div><p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211900706.png" alt="image-20220221190003528"></p>
<h3 id="3优缺点及适用场景-5">3、优缺点及适用场景</h3>
<p>　<strong>优点：</strong></p>
<p>　			（1）思想简单，实现容易。建模迅速，对于小数据量、简单的关系很有效；</p>
<p>　　　　（2）是许多强大的非线性模型的基础。</p>
<p>　　　　（3）线性回归模型十分容易理解，结果具有很好的可解释性，有利于决策分析。</p>
<p>　　　　（4）蕴含机器学习中的很多重要思想。</p>
<p>　　　　（5）能解决回归问题。
<strong>缺点：</strong></p>
<p>​				（1）对于非线性数据或者数据特征间具有相关性多项式回归难以建模.</p>
<p>　　　　（2）难以很好地表达高度复杂的数据。</p>
<h2 id="八树回归">八、树回归</h2>
<h3 id="1主要思想及代码-1">1、主要思想及代码</h3>
<p><strong>CART</strong></p>
<p>CART是一种二分递归分割的技术，分割方法采用基于最小距离的基尼指数估计函数，将当前的样本集分为两个子样本集，使得生成的的每个非叶子节点都有两个分支。因此，CART算法生成的决策树是结构简洁的二叉树。</p>
<p>分类树是针对目标变量是离散型变量，通过二叉树将数据进行分割成离散类的方法。而回归树则是针对目标变量是连续性的变量，通过选取最优分割特征的某个值，然后数据根据大于或者小于这个值进行划分进行树分裂最终生成回归树。</p>
<p><strong>特征和最佳分割点的选取</strong></p>
<p>在使用决策树解决回归问题中我们需要不断的选取某一特征的一个值作为分割点来生成子树。选取的标准就是使得被分割的两部分数据能有最好的纯度。</p>
<ul>
<li>对于离散型数据我们可以通过计算分割两部分数据的基尼不纯度的变化来判定最有分割点；</li>
<li>对于连续性变量我们通过计算最小平方残差，也就是选择使得分割后数据方差变得最小的特征和分割点。直观的理解就是使得分割的两部分数据能够有最相近的值。</li>
</ul>
<p><strong>树分裂的终止条件</strong></p>
<p>有了选取分割特征和最佳分割点的方法，树便可以依此进行分裂，但是分裂的终止条件是什么呢?</p>
<ol>
<li><strong>节点中所有目标变量的值相同</strong>, 既然都已经是相同的值了自然没有必要在分裂了，直接返回这个值就好了.</li>
<li>树的深度达到了预先指定的最大值</li>
<li><strong>不纯度的减小量小于预先定好的阈值</strong>,也就是之进一步的分割数据并不能更好的降低数据的不纯度的时候就可以停止树分裂了。</li>
<li>节点的数据量小于预先定好的阈值</li>
</ol>
<p>首先是加载数据的部分，这里的所有测试数据我均使用的《Machine Learning in Action》中的数据，格式比较规整加载方式也比较一致, 这里由于做树回归，自变量和因变量都放在同一个二维数组中:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_data</span>(filename):
    <span style="color:#e6db74">&#39;&#39;&#39; 加载文本文件中的数据.
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    dataset <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">with</span> open(filename, <span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> f:
        <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> f:
            line_data <span style="color:#f92672">=</span> [float(data) <span style="color:#66d9ef">for</span> data <span style="color:#f92672">in</span> line<span style="color:#f92672">.</span>split()]
            dataset<span style="color:#f92672">.</span>append(line_data)
    <span style="color:#66d9ef">return</span> dataset
</code></pre></div><p>树回归中再找到分割特征和分割值之后需要将数据进行划分以便构建子树或者叶子节点:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">split_dataset</span>(dataset, feat_idx, value):
    <span style="color:#e6db74">&#39;&#39;&#39; 根据给定的特征编号和特征值对数据集进行分割
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    ldata, rdata <span style="color:#f92672">=</span> [], []
    <span style="color:#66d9ef">for</span> data <span style="color:#f92672">in</span> dataset:
        <span style="color:#66d9ef">if</span> data[feat_idx] <span style="color:#f92672">&lt;</span> value:
            ldata<span style="color:#f92672">.</span>append(data)
        <span style="color:#66d9ef">else</span>:
            rdata<span style="color:#f92672">.</span>append(data)
    <span style="color:#66d9ef">return</span> ldata, rdata
</code></pre></div><p>然后就是重要的选取最佳分割特征和分割值了，这里我们通过找打使得分割后的方差最小的分割点最为最佳分割点:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">choose_best_feature</span>(dataset, fleaf, ferr, opt):
    <span style="color:#e6db74">&#39;&#39;&#39; 选取最佳分割特征和特征值
</span><span style="color:#e6db74">    dataset: 待划分的数据集
</span><span style="color:#e6db74">    fleaf: 创建叶子节点的函数
</span><span style="color:#e6db74">    ferr: 计算数据误差的函数
</span><span style="color:#e6db74">    opt: 回归树参数.
</span><span style="color:#e6db74">        err_tolerance: 最小误差下降值;
</span><span style="color:#e6db74">        n_tolerance: 数据切分最小样本数
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    dataset <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(dataset)
    m, n <span style="color:#f92672">=</span> dataset<span style="color:#f92672">.</span>shape
    err_tolerance, n_tolerance <span style="color:#f92672">=</span> opt[<span style="color:#e6db74">&#39;err_tolerance&#39;</span>], opt[<span style="color:#e6db74">&#39;n_tolerance&#39;</span>]
    err <span style="color:#f92672">=</span> ferr(dataset)
    best_feat_idx, best_feat_val, best_err <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, float(<span style="color:#e6db74">&#39;inf&#39;</span>)
    <span style="color:#75715e"># 遍历所有特征</span>
    <span style="color:#66d9ef">for</span> feat_idx <span style="color:#f92672">in</span> range(n<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>):
        values <span style="color:#f92672">=</span> dataset[:, feat_idx]
        <span style="color:#75715e"># 遍历所有特征值</span>
        <span style="color:#66d9ef">for</span> val <span style="color:#f92672">in</span> values:
            <span style="color:#75715e"># 按照当前特征和特征值分割数据</span>
            ldata, rdata <span style="color:#f92672">=</span> split_dataset(dataset<span style="color:#f92672">.</span>tolist(), feat_idx, val)
            <span style="color:#66d9ef">if</span> len(ldata) <span style="color:#f92672">&lt;</span> n_tolerance <span style="color:#f92672">or</span> len(rdata) <span style="color:#f92672">&lt;</span> n_tolerance:
                <span style="color:#75715e"># 如果切分的样本量太小</span>
                <span style="color:#66d9ef">continue</span>
            <span style="color:#75715e"># 计算误差</span>
            new_err <span style="color:#f92672">=</span> ferr(ldata) <span style="color:#f92672">+</span> ferr(rdata)
            <span style="color:#66d9ef">if</span> new_err <span style="color:#f92672">&lt;</span> best_err:
                best_feat_idx <span style="color:#f92672">=</span> feat_idx
                best_feat_val <span style="color:#f92672">=</span> val
                best_err <span style="color:#f92672">=</span> new_err
    <span style="color:#75715e"># 如果误差变化并不大归为一类</span>
    <span style="color:#66d9ef">if</span> abs(err <span style="color:#f92672">-</span> best_err) <span style="color:#f92672">&lt;</span> err_tolerance:
        <span style="color:#66d9ef">return</span> None, fleaf(dataset)
    <span style="color:#75715e"># 检查分割样本量是不是太小</span>
    ldata, rdata <span style="color:#f92672">=</span> split_dataset(dataset<span style="color:#f92672">.</span>tolist(), best_feat_idx, best_feat_val)
    <span style="color:#66d9ef">if</span> len(ldata) <span style="color:#f92672">&lt;</span> n_tolerance <span style="color:#f92672">or</span> len(rdata) <span style="color:#f92672">&lt;</span> n_tolerance:
        <span style="color:#66d9ef">return</span> None, fleaf(dataset)
    <span style="color:#66d9ef">return</span> best_feat_idx, best_feat_val
</code></pre></div><p>其中，停止选取的条件有两个: 一个是当分割的子数据集的大小小于一定值；一个是当选取的最佳分割点分割的数据的方差减小量小于一定的值。</p>
<p><code>fleaf</code>是创建叶子节点的函数引用，不同的树结构此函数也是不同的，例如本部分的回归树，创建叶子节点就是根据分割后的数据集平均值，而对于模型树来说，此函数返回值是根据数据集得到的回归系数。<code>ferr</code>是计算数据集不纯度的函数，不同的树模型该函数也会不同，对于回归树，此函数计算数据集的方差来判定数据集的纯度，而对于模型树来说我们需要计算线性模型拟合程度也就是线性模型的残差平方和。</p>
<p>然后就是最主要的回归树的生成函数了，树结构肯定需要通过递归创建的，选不出新的分割点的时候就触底：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_tree</span>(dataset, fleaf, ferr, opt<span style="color:#f92672">=</span>None):
    <span style="color:#e6db74">&#39;&#39;&#39; 递归创建树结构
</span><span style="color:#e6db74">    dataset: 待划分的数据集
</span><span style="color:#e6db74">    fleaf: 创建叶子节点的函数
</span><span style="color:#e6db74">    ferr: 计算数据误差的函数
</span><span style="color:#e6db74">    opt: 回归树参数.
</span><span style="color:#e6db74">        err_tolerance: 最小误差下降值;
</span><span style="color:#e6db74">        n_tolerance: 数据切分最小样本数
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    <span style="color:#66d9ef">if</span> opt <span style="color:#f92672">is</span> None:
        opt <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;err_tolerance&#39;</span>: <span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#39;n_tolerance&#39;</span>: <span style="color:#ae81ff">4</span>}
    <span style="color:#75715e"># 选择最优化分特征和特征值</span>
    feat_idx, value <span style="color:#f92672">=</span> choose_best_feature(dataset, fleaf, ferr, opt)
    
    <span style="color:#75715e"># 触底条件</span>
    <span style="color:#66d9ef">if</span> feat_idx <span style="color:#f92672">is</span> None:
        <span style="color:#66d9ef">return</span> value
    <span style="color:#75715e"># 创建回归树</span>
    tree <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;feat_idx&#39;</span>: feat_idx, <span style="color:#e6db74">&#39;feat_val&#39;</span>: value}
    <span style="color:#75715e"># 递归创建左子树和右子树</span>
    ldata, rdata <span style="color:#f92672">=</span> split_dataset(dataset, feat_idx, value)
    ltree <span style="color:#f92672">=</span> create_tree(ldata, fleaf, ferr, opt)
    rtree <span style="color:#f92672">=</span> create_tree(rdata, fleaf, ferr, opt)
    tree[<span style="color:#e6db74">&#39;left&#39;</span>] <span style="color:#f92672">=</span> ltree
    tree[<span style="color:#e6db74">&#39;right&#39;</span>] <span style="color:#f92672">=</span> rtree
    <span style="color:#66d9ef">return</span> tree
</code></pre></div><p><strong>使用回归树对数据进行回归</strong></p>
<p>这里使用了现成的分段数据作为训练数据生成回归树:</p>
<p><strong>可视化数据点</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">dataset <span style="color:#f92672">=</span> load_data(<span style="color:#e6db74">&#39;ex0.txt&#39;</span>)
dataset <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(dataset)
<span style="color:#75715e"># 绘制散点</span>
plt<span style="color:#f92672">.</span>scatter(dataset[:, <span style="color:#ae81ff">0</span>], dataset[:, <span style="color:#ae81ff">1</span>])
</code></pre></div><p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211908489.png" alt="img"></p>
<p><strong>创建回归树并可视化</strong></p>
<p>看到这种分段的数据，回归树拟合它可是最合适不过了，我们创建回归树:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">tree <span style="color:#f92672">=</span> create_tree(dataset, fleaf, ferr, opt<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;n_tolerance&#39;</span>: <span style="color:#ae81ff">4</span>,
                                              <span style="color:#e6db74">&#39;err_tolerance&#39;</span>: <span style="color:#ae81ff">1</span>})
</code></pre></div><p>通过Python字典表示的回归树结构:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">{<span style="color:#e6db74">&#39;feat_idx&#39;</span>: <span style="color:#ae81ff">0</span>,
 <span style="color:#e6db74">&#39;feat_val&#39;</span>: <span style="color:#ae81ff">0.40015800000000001</span>,
 <span style="color:#e6db74">&#39;left&#39;</span>: {<span style="color:#e6db74">&#39;feat_idx&#39;</span>: <span style="color:#ae81ff">0</span>,
          <span style="color:#e6db74">&#39;feat_val&#39;</span>: <span style="color:#ae81ff">0.20819699999999999</span>,
          <span style="color:#e6db74">&#39;left&#39;</span>: <span style="color:#f92672">-</span><span style="color:#ae81ff">0.023838155555555553</span>,
          <span style="color:#e6db74">&#39;right&#39;</span>: <span style="color:#ae81ff">1.0289583666666666</span>},
 <span style="color:#e6db74">&#39;right&#39;</span>: {<span style="color:#e6db74">&#39;feat_idx&#39;</span>: <span style="color:#ae81ff">0</span>,
           <span style="color:#e6db74">&#39;feat_val&#39;</span>: <span style="color:#ae81ff">0.609483</span>,
           <span style="color:#e6db74">&#39;left&#39;</span>: <span style="color:#ae81ff">1.980035071428571</span>,
           <span style="color:#e6db74">&#39;right&#39;</span>: {<span style="color:#e6db74">&#39;feat_idx&#39;</span>: <span style="color:#ae81ff">0</span>,
                     <span style="color:#e6db74">&#39;feat_val&#39;</span>: <span style="color:#ae81ff">0.81674199999999997</span>,
                     <span style="color:#e6db74">&#39;left&#39;</span>: <span style="color:#ae81ff">2.9836209534883724</span>,
                     <span style="color:#e6db74">&#39;right&#39;</span>: <span style="color:#ae81ff">3.9871631999999999</span>}}}
</code></pre></div><p>这里我还是使用Graphviz来可视化回归树，类似之前决策树做分类的文章中的<code>dotify</code>函数，这里稍微修改下叶子节点的label，我们便可以递归得到决策树对应的dot文件,然后获取树结构图:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">datafile <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;ex0.txt&#39;</span>
dotfile <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;{}.dot&#39;</span><span style="color:#f92672">.</span>format(datafile<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;.&#39;</span>)[<span style="color:#ae81ff">0</span>])
<span style="color:#66d9ef">with</span> open(dotfile, <span style="color:#e6db74">&#39;w&#39;</span>) <span style="color:#66d9ef">as</span> f:
    content <span style="color:#f92672">=</span> dotify(tree)
    f<span style="color:#f92672">.</span>write(content)
</code></pre></div><p><strong>绘制回归树回归曲线</strong></p>
<p>有了回归树，我们便可以绘制回归树回归曲线，看看它对于分段数据是否能有较好的回归效果：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># 绘制回归曲线</span>
x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">50</span>)
y <span style="color:#f92672">=</span> [tree_predict([i], tree) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> x]
plt<span style="color:#f92672">.</span>plot(x, y, c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r&#39;</span>)
plt<span style="color:#f92672">.</span>show()
</code></pre></div><p><strong>树剪枝</strong>
在介绍树剪枝之前先使用上一部分的代码对两组类似的数据进行回归，可视化后的数据以及回归曲线如下(数据文件左&amp;数据文件右):</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211914837.png" alt="image-20220221191450542"></p>
<p>左右两边的数据的分布基本相同但是使用相同的参数得到的回归树却完全不同左边的回归树只有两个分支，而右边的分支则有很多，甚至有时候会为所有的数据点得到一个分支，这样回归树将会非常的庞大, 如下是可视化得到的两个回归树:</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211915130.jpeg" alt="img"></p>
<p>如果一棵树的节点过多则表明该模型可能对数据进行了“过拟合”。那么我们需要降低决策树的复杂度来避免过拟合，此过程就是<strong>剪枝</strong>。剪枝技术又分为<strong>预剪枝</strong>和<strong>后剪枝</strong>。</p>
<p><strong>预剪枝</strong></p>
<p>预剪枝是在生成决策树之前通过改变参数然后在树生成的过程中进行的。比如在上文中我们创建回归树的函数中有个<code>opt</code>参数，其中包含<code>n_tolerance</code>和<code>err_tolerance</code>，他们可以控制何时停止树的分裂，当增大叶子节点的最小数据量以及增大误差容忍度，树的分裂也会越提前的终止。当我们把误差变化容忍度增加到2000的时候得到的回归树以及回归曲线可视化如下:</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211915865.jpeg" alt="img"></p>
<p><strong>后剪枝</strong></p>
<p>预剪枝技术需要用于预先指定参数，但是后剪枝技术则是通过测试数据来自动进行剪枝不需要用户干预因此是一种更理想的剪枝技术，但是我们需要写剪枝函数来处理。</p>
<blockquote>
<p>后剪枝的大致思想就是我们针对一颗子树，尝试将其左右子树(节点)合并，通过测试数据计算合并前后的方差，如果合并后的方差比合并前的小，这说明可以合并此子树。</p>
</blockquote>
<p>对树进行塌陷处理: 我们对一棵树进行塌陷处理，就是递归将这棵树进行合并返回这棵树的平均值。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">collapse</span>(tree):
    <span style="color:#e6db74">&#39;&#39;&#39; 对一棵树进行塌陷处理, 得到给定树结构的平均值
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    <span style="color:#66d9ef">if</span> not_tree(tree):
        <span style="color:#66d9ef">return</span> tree
    ltree, rtree <span style="color:#f92672">=</span> tree[<span style="color:#e6db74">&#39;left&#39;</span>], tree[<span style="color:#e6db74">&#39;right&#39;</span>]
    <span style="color:#66d9ef">return</span> (collapse(ltree) <span style="color:#f92672">+</span> collapse(rtree))<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>
</code></pre></div><p>后剪枝的Python实现:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">postprune</span>(tree, test_data):
    <span style="color:#e6db74">&#39;&#39;&#39; 根据测试数据对树结构进行后剪枝
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    <span style="color:#66d9ef">if</span> not_tree(tree):
        <span style="color:#66d9ef">return</span> tree
    <span style="color:#75715e"># 若没有测试数据则直接返回树平均值</span>
    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> test_data:
        <span style="color:#66d9ef">return</span> collapse(tree)
    ltree, rtree <span style="color:#f92672">=</span> tree[<span style="color:#e6db74">&#39;left&#39;</span>], tree[<span style="color:#e6db74">&#39;right&#39;</span>]
    <span style="color:#66d9ef">if</span> not_tree(ltree) <span style="color:#f92672">and</span> not_tree(rtree):
        <span style="color:#75715e"># 分割数据用于测试</span>
        ldata, rdata <span style="color:#f92672">=</span> split_dataset(test_data, tree[<span style="color:#e6db74">&#39;feat_idx&#39;</span>], tree[<span style="color:#e6db74">&#39;feat_val&#39;</span>])
        <span style="color:#75715e"># 分别计算合并前和合并后的测试数据误差</span>
        err_no_merge <span style="color:#f92672">=</span> (np<span style="color:#f92672">.</span>sum((np<span style="color:#f92672">.</span>array(ldata) <span style="color:#f92672">-</span> ltree)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>) <span style="color:#f92672">+</span>
                        np<span style="color:#f92672">.</span>sum((np<span style="color:#f92672">.</span>array(rdata) <span style="color:#f92672">-</span> rtree)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>))
        err_merge <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum((np<span style="color:#f92672">.</span>array(test_data) <span style="color:#f92672">-</span> (ltree <span style="color:#f92672">+</span> rtree)<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
        <span style="color:#66d9ef">if</span> err_merge <span style="color:#f92672">&lt;</span> err_no_merge:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;merged&#39;</span>)
            <span style="color:#66d9ef">return</span> (ltree <span style="color:#f92672">+</span> rtree)<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>
        <span style="color:#66d9ef">else</span>:
            <span style="color:#66d9ef">return</span> tree
    tree[<span style="color:#e6db74">&#39;left&#39;</span>] <span style="color:#f92672">=</span> postprune(tree[<span style="color:#e6db74">&#39;left&#39;</span>], test_data)
    tree[<span style="color:#e6db74">&#39;right&#39;</span>] <span style="color:#f92672">=</span> postprune(tree[<span style="color:#e6db74">&#39;right&#39;</span>], test_data)
    <span style="color:#66d9ef">return</span> tree
</code></pre></div><p>我们看一下不对刚才的树进行预剪枝而是使用测试数据进行后剪枝的效果:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">data_test <span style="color:#f92672">=</span> load_data(<span style="color:#e6db74">&#39;ex2test.txt&#39;</span>)
pruned_tree <span style="color:#f92672">=</span> postprune(tree, data_test)
</code></pre></div><p>通过输出可以看到总共进行了8次剪枝操作，通过把剪枝前和剪枝后的树可视化对比下看看:</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211917474.jpeg" alt="img"></p>
<p>树的规模的确是减小了。</p>
<p><strong>模型树</strong></p>
<p>上一部分叶子节点上放的是分割后数据的平均值并以他作为满足条件的样本的预测值，本部分我们将在叶子节点上放一个线性模型来做预测。也就是指我们的树是由多个线性模型组成的，显然会比强行用平均值来建模更有优势。</p>
<ul>
<li>模型树使用多个线性函数来做回归比用多个平均值组成一棵大树的模型更有可解释性</li>
<li>而且线性模型的使用可以使树的规模减小，毕竟平均值的覆盖范围只是局部的，而线性模型可以覆盖所有具有线性关系的数据。</li>
<li>模型树也具有更高的预测准确度</li>
</ul>
<p><strong>创建模型树</strong></p>
<p>模型树和回归树的思想是完全一致的，只是在生成叶子节点的方法以及计算数据误差(不纯度)的方式不同。在模型树里针对一个叶子节点我们需要使用分割到的数据进行线性回归得到线性回归系数而不是简单的计算数据的平均值。不纯度的计算也不是简单的计算数据的方差，而是计算线性模型的残差平方和。</p>
<p>为了能为叶子节点计算线性模型，我们还需要实现一个标准线性回归函数<code>linear_regression</code>, 相应模型树的<code>ferr</code>和<code>fleaf</code>的Python实现。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">linear_regression</span>(dataset):
    <span style="color:#e6db74">&#39;&#39;&#39; 获取标准线性回归系数
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    dataset <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>matrix(dataset)
    <span style="color:#75715e"># 分割数据并添加常数列</span>
    X_ori, y <span style="color:#f92672">=</span> dataset[:, :<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], dataset[:, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
    X_ori, y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>matrix(X_ori), np<span style="color:#f92672">.</span>matrix(y)
    m, n <span style="color:#f92672">=</span> X_ori<span style="color:#f92672">.</span>shape
    X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>matrix(np<span style="color:#f92672">.</span>ones((m, n<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)))
    X[:, <span style="color:#ae81ff">1</span>:] <span style="color:#f92672">=</span> X_ori
    <span style="color:#75715e"># 回归系数</span>
    w <span style="color:#f92672">=</span> (X<span style="color:#f92672">.</span>T<span style="color:#f92672">*</span>X)<span style="color:#f92672">.</span>I<span style="color:#f92672">*</span>X<span style="color:#f92672">.</span>T<span style="color:#f92672">*</span>y
    <span style="color:#66d9ef">return</span> w, X, y
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fleaf</span>(dataset):
    <span style="color:#e6db74">&#39;&#39;&#39; 计算给定数据集的线性回归系数
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    w, _, _ <span style="color:#f92672">=</span> linear_regression(dataset)
    <span style="color:#66d9ef">return</span> w
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">ferr</span>(dataset):
    <span style="color:#e6db74">&#39;&#39;&#39; 对给定数据集进行回归并计算误差
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    w, X, y <span style="color:#f92672">=</span> linear_regression(dataset)
    y_prime <span style="color:#f92672">=</span> X<span style="color:#f92672">*</span>w
    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>var(y_prime <span style="color:#f92672">-</span> y)
</code></pre></div><p><strong>在分段线性数据上应用模型树</strong></p>
<p>本部分使用了事先准备好的分段线性数据来构建模型树，数据点可视化如下:</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211919483.png" alt="img"></p>
<p>现在我们使用这些数据构建一个模型树:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">tree <span style="color:#f92672">=</span> create_tree(dataset, fleaf, ferr, opt<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;err_tolerance&#39;</span>: <span style="color:#ae81ff">0.1</span>, <span style="color:#e6db74">&#39;n_tolerance&#39;</span>: <span style="color:#ae81ff">4</span>})
</code></pre></div><p>得到的树结构：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">{<span style="color:#e6db74">&#39;feat_idx&#39;</span>: <span style="color:#ae81ff">0</span>,
 <span style="color:#e6db74">&#39;feat_val&#39;</span>: <span style="color:#ae81ff">0.30440099999999998</span>,
 <span style="color:#e6db74">&#39;left&#39;</span>: matrix([[ <span style="color:#ae81ff">3.46877936</span>],
                 [ <span style="color:#ae81ff">1.18521743</span>]]),
 <span style="color:#e6db74">&#39;right&#39;</span>: matrix([[  <span style="color:#ae81ff">1.69855694e-03</span>],
                  [  <span style="color:#ae81ff">1.19647739e+01</span>]])}
</code></pre></div><p>可视化:</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211922357.png" alt="img"></p>
<p>绘制回归曲线:</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211922058.png" alt="img"></p>
<p>可以通过模型树看到对于此数据只需要两个分支，数的深度也只有2层。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211922908.png" alt="image-20220221192230789"></p>
<p><strong>回归树与线性回归的对比</strong></p>
<p>本部分我们使用标准线性回归和回归树分别对同一组数据进行回归，并使用同一组测试数据计算相关系数(Correlation Coefficient)对两种模型的回归效果进行对比。</p>
<p>数据我还是使用《Machinie Learning in Action》中的现成数据，数据可视化如下:</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211923407.png" alt="img"></p>
<p>现在我们分别使用标准线性回归和回归树对该数据进行回归，并计算模型预测值和测试样本的相关系数R2R2</p>
<p>相关系数计算:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_corrcoef</span>(X, Y):
    <span style="color:#75715e"># X Y 的协方差</span>
    cov <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(X<span style="color:#f92672">*</span>Y) <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>mean(X)<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>mean(Y)
    <span style="color:#66d9ef">return</span> cov<span style="color:#f92672">/</span>(np<span style="color:#f92672">.</span>var(X)<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>var(Y))<span style="color:#f92672">**</span><span style="color:#ae81ff">0.5</span>
</code></pre></div><p>获得的相关系数:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">linear regression correlation coefficient: <span style="color:#ae81ff">0.9434684235674773</span>
regression tree correlation coefficient: <span style="color:#ae81ff">0.9780307932704089</span>
</code></pre></div><p>绘制线性回归和树回归的回归曲线(黄色会树回归曲线，红色会线性回归):</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211924162.png" alt="img"></p>
<p>可见树回归方法在预测复杂数据的时候会比简单的线性模型更有效。</p>
<h3 id="2优缺点及适用场景-1">2、优缺点及适用场景</h3>
<p><strong>优点：</strong></p>
<p>具有很高的复杂度和高度的非线性关系，比多项式拟合拥有更好的效果；</p>
<p>模型容易理解和阐述，训练过程中的决策边界容易实践和理解。</p>
<p><strong>缺点：</strong></p>
<p>由于决策树有过拟合的倾向，完整的决策树模型包含很多过于复杂和非必须的结构。但可以通过扩大随机森林或者剪枝的方法来缓解这一问题；</p>
<p>较大的随机数表现很好，但是却带来了运行速度慢和内存消耗高的问题。</p>
<h2 id="九k均值聚类">九、K均值聚类</h2>
<h3 id="1主要思想-6">1、主要思想</h3>
<p>　　k-means算法是一种简单的迭代型聚类算法，采用距离作为相似性指标，从而发现给定数据集中的K个类，且每个类的中心是根据类中所有值的均值得到，每个类用聚类中心来描述。对于给定的一个包含n个d维数据点的数据集X以及要分得的类别K,选取欧式距离作为相似度指标，聚类目标是使得各类的聚类平方和最小，即最小化。结合最小二乘法和拉格朗日原理，聚类中心为对应类别中各数据点的平均值，同时为了使得算法收敛，在迭代过程中，应使最终的聚类中心尽可能的不变。</p>
<p><strong>算法实现一般流程</strong></p>
<p>1、首先确定一个k值，即我们希望将数据集经过聚类得到k个集合。</p>
<p>2、从数据集中随机选择k个数据点作为质心。</p>
<p>3、对数据集中每一个点，计算其与每一个质心的距离（如欧式距离），离哪个质心近，就划分到那个质心所属的集合。</p>
<p>4、把所有数据归好集合后，一共有k个集合。然后重新计算每个集合的质心。</p>
<p>5、如果新计算出来的质心和原来的质心之间的距离小于某一个设置的阈值（表示重新计算的质心的位置变化不大，趋于稳定，或者说收敛），我们可以认为聚类已经达到期望的结果，算法终止。</p>
<p>6、如果新质心和原质心距离变化很大，需要迭代3~5步骤。</p>
<p><strong>算法步骤图解</strong></p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211942351.png" alt="image-20220221194227200"></p>
<p>上图a表达了初始的数据集，假设k=2。在图b中，我们随机选择了两个k类所对应的类别质心，即图中的红色质心和蓝色质心，然后分别求样本中所有点到这两个质心的距离，并标记每个样本的类别为和该样本距离最小的质心的类别，如图c所示，经过计算样本和红色质心和蓝色质心的距离，我们得到了所有样本点的第一轮迭代后的类别。此时我们对我们当前标记为红色和蓝色的点分别求其新的质心，如图d所示，新的红色质心和蓝色质心的位置已经发生了变动。图e和图f重复了我们在图c和图d的过程，即将所有点的类别标记为距离最近的质心的类别并求新的质心。最终我们得到的两个类别如图f。</p>
<p>簇：所有数据的点集合，簇中的对象是相似的。</p>
<p>质心：簇中所有点的中心（计算所有点的中心而来）。</p>
<p><strong>在使用K均值聚类时需要注意的地方</strong></p>
<p><strong>1. 输入数据一般需要做缩放，如标准化</strong>。原因很简单，K均值是建立在距离度量上的，因此不同变量间如果维度差别过大，可能会造成少数变量“施加了过高的影响而造成垄断”。</p>
<p><strong>2. 如果输入数据的变量类型不同，部分是数值型（numerical），部分是分类变量（categorical），需要做特别处理</strong>。方法1是将分类变量转化为数值型，但缺点在于如果使用独热编码（one hot encoding）可能会导致数据维度大幅度上升，如果使用标签编码（label encoding）无法很好的处理数据中的顺序（order）。方法2是对于数值型变量和分类变量分开处理，并将结果结合起来，具体可以参考Python的实现，如K-mode和K-prototype。</p>
<p><strong>3. 输出结果非固定，多次运行结果可能不同</strong>。首先要意识到K-means中是有随机性的，从初始化到收敛结果往往不同。一种看法是强行固定随机性，比如设定sklearn中的random state为固定值。另一种看法是，如果你的K均值结果总在大幅度变化，比如不同簇中的数据量在多次运行中变化很大，那么K均值不适合你的数据，不要试图稳定结果 。</p>
<p><strong>4. 运行时间往往可以得到优化，选择最优的工具库</strong>。基本上现在的K均值实现都是K-means++，速度都不错。但当数据量过大时，依然可以使用其他方法，如MiniBatchKMeans。上百万个数据点往往可以在数秒钟内完成聚类，推荐Sklearn的实现。</p>
<p><strong>5. 高维数据上的有效性有限</strong>。建立在距离度量上的算法一般都有类似的问题，那就是在高维空间中距离的意义有了变化，且并非所有维度都有意义。这种情况下，K均值的结果往往不好，而通过划分子空间的算法（sub-spacing method）效果可能更好。</p>
<p><strong>6. 运行效率与性能之间的取舍</strong>。但数据量上升到一定程度时，如&gt;10万条数据，那么很多算法都不能使用。最近读到的一篇对比不同算法性能随数据量的变化很有意思。在作者的数据集上，当数据量超过一定程度时仅K均值和HDBSCAN可用。</p>
<p>因此不难看出，K均值算法最大的优点就是运行速度快，能够处理的数据量大，且易于理解。但缺点也很明显，就是算法性能有限，在高维上可能不是最佳选项。</p>
<p>一个比较粗浅的结论是，在数据量不大时，可以优先尝试其他算法。仅当数据量巨大，且无法降维或者降低数量时，再尝试使用K均值。</p>
<p>一个显著的问题信号是，如果多次运行K均值的结果都有很大差异，那么有很高的概率K均值不适合当前数据，要对结果谨慎的分析。</p>
<h3 id="2实现代码-6">2、实现代码</h3>
<p>以鸢尾花分类为例</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> math
<span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> defaultdict
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
dataname <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;data.txt&#34;</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loadIRISdata</span>(filename):
    data <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">with</span> open(filename, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;r&#34;</span>, encoding<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;utf-8&#34;</span>) <span style="color:#66d9ef">as</span> rf:
        <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> rf:
            <span style="color:#66d9ef">if</span> line <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>:
                <span style="color:#66d9ef">continue</span>
            data<span style="color:#f92672">.</span>append(list(map(float, line<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34; &#34;</span>))))
    <span style="color:#66d9ef">return</span> data
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">generateCenters</span>(data):
    <span style="color:#e6db74">&#39;&#39;&#39;求解初始聚类中心&#39;&#39;&#39;</span>
    centers <span style="color:#f92672">=</span> []
    <span style="color:#e6db74">&#39;&#39;&#39;已知维度为4&#39;&#39;&#39;</span>
    <span style="color:#e6db74">&#39;&#39;&#39;分三类，取第0，50，100的三个向量作为分界&#39;&#39;&#39;</span>
    centers<span style="color:#f92672">.</span>append(data[<span style="color:#ae81ff">0</span>])
    centers<span style="color:#f92672">.</span>append(data[<span style="color:#ae81ff">50</span>])
    centers<span style="color:#f92672">.</span>append(data[<span style="color:#ae81ff">100</span>])
    <span style="color:#66d9ef">return</span> centers
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">distance</span>(a ,b):
    <span style="color:#e6db74">&#39;&#39;&#39;欧式距离&#39;&#39;&#39;</span>
    sum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">4</span>):
        sq <span style="color:#f92672">=</span> (a[i]<span style="color:#f92672">-</span>b[i])<span style="color:#f92672">*</span>(a[i]<span style="color:#f92672">-</span>b[i])
        sum <span style="color:#f92672">+=</span> sq
    <span style="color:#66d9ef">return</span> math<span style="color:#f92672">.</span>sqrt(sum)
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">point_avg</span>(points):
    <span style="color:#e6db74">&#39;&#39;&#39;对维度求平均值&#39;&#39;&#39;</span>
    new_center <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">4</span>):
        sum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> points:
            sum <span style="color:#f92672">+=</span> p[i]
        new_center<span style="color:#f92672">.</span>append(float(<span style="color:#e6db74">&#34;</span><span style="color:#e6db74">%.8f</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (sum<span style="color:#f92672">/</span>float(len(points)))))
    <span style="color:#66d9ef">return</span> new_center
 
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">updataCenters</span>(data, assigments):
    new_means <span style="color:#f92672">=</span> defaultdict(list)
    centers <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> assigment, point <span style="color:#f92672">in</span> zip(assigments, data):
        new_means[assigment]<span style="color:#f92672">.</span>append(point)
        <span style="color:#e6db74">&#39;&#39;&#39;将同一类的数据进行整合&#39;&#39;&#39;</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">3</span>):
        points <span style="color:#f92672">=</span> new_means[i]
        centers<span style="color:#f92672">.</span>append(point_avg(points))
    <span style="color:#66d9ef">return</span> centers
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">assignment</span>(data, centers):
    assignments <span style="color:#f92672">=</span> []
    <span style="color:#e6db74">&#39;&#39;&#39;对应位置显示对应类群&#39;&#39;&#39;</span>
    <span style="color:#66d9ef">for</span> point <span style="color:#f92672">in</span> data:
        <span style="color:#e6db74">&#39;&#39;&#39;遍历所有数据&#39;&#39;&#39;</span>
        shortest <span style="color:#f92672">=</span> float(<span style="color:#e6db74">&#39;inf&#39;</span>)
        shortestindex <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">3</span>):
            <span style="color:#e6db74">&#39;&#39;&#39;遍历三个中心向量，与哪个类中心欧氏距离最短就将其归为哪类&#39;&#39;&#39;</span>
            value <span style="color:#f92672">=</span> distance(point, centers[i])
            <span style="color:#66d9ef">if</span> value <span style="color:#f92672">&lt;</span> shortest:
                shortest <span style="color:#f92672">=</span> value
                shortestindex <span style="color:#f92672">=</span> i
        assignments<span style="color:#f92672">.</span>append(shortestindex)
    <span style="color:#66d9ef">return</span> assignments
 
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">kmeans</span>(data):
    k_data <span style="color:#f92672">=</span> generateCenters(data)
    assigments <span style="color:#f92672">=</span> assignment(data, k_data)
    old_assigments <span style="color:#f92672">=</span> None
    <span style="color:#66d9ef">while</span> assigments <span style="color:#f92672">!=</span> old_assigments:
        new_centers <span style="color:#f92672">=</span> updataCenters(data, assigments)
        old_assigments <span style="color:#f92672">=</span> assigments
        assigments <span style="color:#f92672">=</span> assignment(data, new_centers)
    result <span style="color:#f92672">=</span> list(zip(assigments, data))
    <span style="color:#66d9ef">return</span> result
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">acc</span>(result):
    sum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    all <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">50</span>):
        <span style="color:#66d9ef">if</span> result[i][<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            sum <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
        all <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">50</span>):
        <span style="color:#66d9ef">if</span> result[i<span style="color:#f92672">+</span><span style="color:#ae81ff">50</span>][<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
            sum <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
        all <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">50</span>):
        <span style="color:#66d9ef">if</span> result[i<span style="color:#f92672">+</span><span style="color:#ae81ff">100</span>][<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> <span style="color:#ae81ff">2</span>:
            sum <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
        all <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;sum:&#39;</span>, sum, <span style="color:#e6db74">&#39;all:&#39;</span>, all)
    <span style="color:#66d9ef">return</span> sum, all
 
<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
    data <span style="color:#f92672">=</span> loadIRISdata(dataname)
    result <span style="color:#f92672">=</span> kmeans(data)
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">3</span>):
        tag <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;第</span><span style="color:#e6db74">%d</span><span style="color:#e6db74">类数据有：&#34;</span> <span style="color:#f92672">%</span> (i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>))
        <span style="color:#66d9ef">for</span> tuple <span style="color:#f92672">in</span> range(len(result)):
            <span style="color:#66d9ef">if</span>(result[tuple][<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> i):
                <span style="color:#66d9ef">print</span>(tuple, end<span style="color:#f92672">=</span><span style="color:#e6db74">&#39; &#39;</span>)
                tag <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
            <span style="color:#66d9ef">if</span> tag <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">20</span> :
                <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)
                tag <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    <span style="color:#75715e">#print(result)</span>
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)
    sum, all <span style="color:#f92672">=</span> acc(result)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;c-means准确度为:</span><span style="color:#e6db74">%2f%%</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> ((sum<span style="color:#f92672">/</span>all)<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>))
</code></pre></div><h3 id="3优缺点及适用场景-6">3、优缺点及适用场景</h3>
<p><strong>优点：</strong></p>
<p>1、原理比较简单，实现也是很容易，收敛速度快。</p>
<p>2、当结果簇是密集的，而簇与簇之间区别明显时, 它的效果较好。</p>
<p>3、主要需要调参的参数仅仅是簇数k。</p>
<p><strong>缺点：</strong></p>
<p>1、K值需要预先给定，很多情况下K值的估计是非常困难的。</p>
<p>2、K-Means算法对初始选取的质心点是敏感的，不同的随机种子点得到的聚类结果完全不同 ，对结果影响很大。</p>
<p>3、对噪音和异常点比较的敏感。用来检测异常值。</p>
<p>4、采用迭代方法，可能只能得到局部的最优解，而无法得到全局的最优解。</p>
<h2 id="十apriori算法">十、Apriori算法</h2>
<h3 id="1主要思想-7">1、主要思想</h3>
<p>Apriori算法是一种挖掘关联规则的频繁项集算法，其核心思想是通过候选集生成和情节的向下封闭检测两个阶段来挖掘频繁项集。 Apriori（先验的，推测的）算法应用广泛，可用于消费市场价格分析，猜测顾客的消费习惯；网络安全领域中的入侵检测技术；可用在用于高校管理中，根据挖掘规则可以有效地辅助学校管理部门有针对性的开展贫困助学工作；也可用在移动通信领域中，指导运营商的业务运营和辅助业务提供商的决策制定。</p>
<p><strong>挖掘步骤：</strong></p>
<p>1.依据支持度找出所有频繁项集（频度）</p>
<p>2.依据置信度产生关联规则（强度）</p>
<p><strong>基本概念</strong></p>
<p>1、支持度</p>
<p>关联规则A-&gt;B的支持度support=P(AB)，指的是事件A和事件B同时发生的概率。</p>
<p>2、置信度</p>
<p>置信度confidence=P(B|A)=P(AB)/P(A),指的是发生事件A的基础上发生事件B的概率。比如说在规则Computer =&gt; antivirus_software , 其中 support=2%, confidence=60%中，就表示的意思是所有的商品交易中有2%的顾客同时买了电脑和杀毒软件，并且购买电脑的顾客中有60%也购买了杀毒软件。</p>
<p>3、k项集</p>
<p>如果事件A中包含k个元素，那么称这个事件A为k项集，并且事件A满足最小支持度阈值的事件称为频繁k项集。</p>
<p>4、由频繁项集产生强关联规则</p>
<p>1）K维数据项集LK是频繁项集的必要条件是它所有K-1维子项集也为频繁项集，记为LK-1　</p>
<p>2）如果K维数据项集LK的任意一个K-1维子集Lk-1，不是频繁项集，则K维数据项集LK本身也不是最大数据项集。</p>
<p>3）Lk是K维频繁项集，如果所有K-1维频繁项集合Lk-1中包含LK的K-1维子项集的个数小于K，则Lk不可能是K维最大频繁数据项集。</p>
<p>4）同时满足最小支持度阀值和最小置信度阀值的规则称为强规则。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211953812.png" alt="image-20220221195352704"></p>
<p>Apriori算法过程分为两个步骤：</p>
<p>第一步通过迭代，检索出事务数据库中的所有频繁项集，即支持度不低于用户设定的阈值的项集；</p>
<p>第二步利用频繁项集构造出满足用户最小信任度的规则。</p>
<p>具体做法就是：</p>
<p>首先找出频繁1-项集，记为L1；然后利用L1来产生候选项集C2，对C2中的项进行判定挖掘出L2，即频繁2-项集；不断如此循环下去直到无法发现更多的频繁k-项集为止。每挖掘一层Lk就需要扫描整个数据库一遍。算法利用了一个性质：</p>
<p><strong>Apriori 性质</strong>：任一频繁项集的所有非空子集也必须是频繁的。意思就是说，生成一个k-itemset的候选项时，如果这个候选项有子集不在(k-1)-itemset(已经确定是frequent的)中时，那么这个候选项就不用拿去和支持度判断了，直接删除。具体而言：</p>
<p>1） 连接步</p>
<p>为找出Lk（所有的频繁k项集的集合），通过将Lk-1（所有的频繁k-1项集的集合）与自身连接产生候选k项集的集合。候选集合记作Ck。设l1和l2是Lk-1中的成员。记li[j]表示li中的第j项。假设Apriori算法对事务或项集中的项按字典次序排序，即对于（k-1）项集li，li[1]i<!-- raw HTML omitted -->[2]&lt;……….i<!-- raw HTML omitted -->[k-1]。将Lk-1与自身连接，如果(l1[1]=l2[1])&amp;&amp;( l1[2]=l2[2])&amp;&amp;……..&amp;&amp; (l1[k-2]=l2[k-2])&amp;&amp;(l1[k-1]2<!-- raw HTML omitted -->[k-1])，那认为l1和l2是可连接。连接l1和l2 产生的结果是{l1[1],l1[2],……,l1[k-1],l2[k-1]}。</p>
<p>2） 剪枝步</p>
<p>CK是LK的超集，也就是说，CK的成员可能是也可能不是频繁的。通过扫描所有的事务（交易），确定CK中每个候选的计数，判断是否小于最小支持度计数，如果不是，则认为该候选是频繁的。为了压缩Ck,可以利用Apriori性质：任一频繁项集的所有非空子集也必须是频繁的，反之，如果某个候选的非空子集不是频繁的，那么该候选肯定不是频繁的，从而可以将其从CK中删除。</p>
<p>实例一：下面以图例的方式说明该算法的运行过程： 假设有一个数据库D，其中有4个事务记录，分别表示为：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211956894.png" alt="img"></p>
<p>这里预定最小支持度minSupport=2,下面用图例说明算法运行的过程：</p>
<p>1、扫描D，对每个候选项进行支持度计数得到表C1:</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211956751.png" alt="img"></p>
<p>2、比较候选项支持度计数与最小支持度minSupport，产生1维最大项目集L1：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211956100.png" alt="img"></p>
<p>3、由L1产生候选项集C2：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211956047.png" alt="img"></p>
<p>4、扫描D，对每个候选项集进行支持度计数:</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211956249.png" alt="img"></p>
<p>5、比较候选项支持度计数与最小支持度minSupport，产生2维最大项目集L2：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211956560.png" alt="img"></p>
<p>6、由L2产生候选项集C3：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211956703.png" alt="img"></p>
<p>7、比较候选项支持度计数与最小支持度minSupport，产生3维最大项目集L3：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211956775.png" alt="img"></p>
<p>算法终止。</p>
<p>实例二：下图从整体同样的能说明此过程：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202211956307.png" alt="img"></p>
<p>此例的分析如下：</p>
<p>1 . 连接：C3=L2 L2= {{A,C},{B,C},{B,E}{C,E}} {{A,C},{B,C},{B,E}{C,E}} = {{A,B,C},{A,C,E},{B,C,E}}</p>
<p>2．使用Apriori性质剪枝：频繁项集的所有子集必须是频繁的，对候选项C3，我们可以删除其子集为非频繁的选项：</p>
<p>{A,B,C}的2项子集是{A,B},{A,C},{B,C}，其中{A,B}不是L2的元素，所以删除这个选项；</p>
<p>{A,C,E}的2项子集是{A,C},{A,E},{C,E}，其中{A,E} 不是L2的元素，所以删除这个选项；</p>
<p>{B,C,E}的2项子集是{B,C},{B,E},{C,E}，它的所有2－项子集都是L2的元素，因此保留这个选项。</p>
<p>3．这样，剪枝后得到C3={{B,C,E}}</p>
<h3 id="2实现代码-7">2、实现代码</h3>
<p>例：交易数据D = {ABCD, BCE, ABCE, BDE, ABCD}找出其中频繁项集：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> copy

minsupport <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>  <span style="color:#75715e"># 最小支持度的频数</span>


<span style="color:#75715e"># 除去子集不是频繁项集的候选项集</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">subset</span>(sen_c_items, item):
    sub_set <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> sen_c_items:
        candidate <span style="color:#f92672">=</span> set(i)  <span style="color:#75715e"># 将K-1项集中的候选项变成集合形式</span>
        t_record <span style="color:#f92672">=</span> set(item)  <span style="color:#75715e"># 将交易数据库中的数据项变成集合形式</span>
        <span style="color:#66d9ef">if</span> candidate<span style="color:#f92672">.</span>issubset(t_record):
            sub_set<span style="color:#f92672">.</span>append(i)  <span style="color:#75715e"># 保留子集部分</span>
    <span style="color:#66d9ef">return</span> sub_set


<span style="color:#75715e"># 找出K项候选集</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">apriori_gen</span>(sub_k_items, n):
    sen_k_items <span style="color:#f92672">=</span> set([])
    <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> range(len(sub_k_items)):
        <span style="color:#66d9ef">for</span> q <span style="color:#f92672">in</span> range(p <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, len(sub_k_items)):
            flag <span style="color:#f92672">=</span> True
            p_item <span style="color:#f92672">=</span> list(sub_k_items[p])
            q_item <span style="color:#f92672">=</span> list(sub_k_items[q])
            <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(n):
                <span style="color:#66d9ef">if</span> p_item[n <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>] <span style="color:#f92672">==</span> q_item[n <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>]:
                    flag <span style="color:#f92672">=</span> False
                    <span style="color:#66d9ef">break</span>
                <span style="color:#66d9ef">elif</span> p_item[i] <span style="color:#f92672">!=</span> q_item[i] <span style="color:#f92672">and</span> i <span style="color:#f92672">&lt;</span> n<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>:
                    flag <span style="color:#f92672">=</span> False
            <span style="color:#66d9ef">if</span> flag:
                c <span style="color:#f92672">=</span> list(sub_k_items[p] <span style="color:#f92672">+</span> q_item[n <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>])  <span style="color:#75715e"># 将q中的最后一项添加到p中</span>
                c<span style="color:#f92672">.</span>sort()
                <span style="color:#66d9ef">if</span> has_frequent_subset(c, sub_k_items, n):
                    c <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;</span><span style="color:#f92672">.</span>join(c)
                    sen_k_items<span style="color:#f92672">.</span>add(c)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;候选&#34;</span>, n <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#34;-项集：&#34;</span>, sorted(sen_k_items))
    <span style="color:#66d9ef">return</span> sen_k_items


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">has_frequent_subset</span>(c, freq_items, n):
    num <span style="color:#f92672">=</span> len(c) <span style="color:#f92672">*</span> n
    count <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    flag <span style="color:#f92672">=</span> False
    <span style="color:#66d9ef">for</span> c_item <span style="color:#f92672">in</span> freq_items:
        <span style="color:#66d9ef">if</span> set(c_item)<span style="color:#f92672">.</span>issubset(set(c)):
            count <span style="color:#f92672">+=</span> n
    <span style="color:#66d9ef">if</span> count <span style="color:#f92672">==</span> num:
        flag <span style="color:#f92672">=</span> True
    <span style="color:#66d9ef">return</span> flag


<span style="color:#75715e"># 剔除不频繁的项集</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">op_freq_item</span>(d):
    <span style="color:#66d9ef">for</span> item <span style="color:#f92672">in</span> list(d<span style="color:#f92672">.</span>keys()):
        <span style="color:#66d9ef">if</span> d[item] <span style="color:#f92672">&lt;</span> minsupport:
            <span style="color:#66d9ef">del</span> d[item]
    l <span style="color:#f92672">=</span> sorted((d<span style="color:#f92672">.</span>keys()))
    <span style="color:#66d9ef">return</span> l


<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
    dataset <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;ABCD&#34;</span>, <span style="color:#e6db74">&#34;BCE&#34;</span>, <span style="color:#e6db74">&#34;ABCE&#34;</span>, <span style="color:#e6db74">&#34;BDE&#34;</span>, <span style="color:#e6db74">&#34;ABCD&#34;</span>, ]
    d <span style="color:#f92672">=</span> {}
    <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> dataset:  <span style="color:#75715e"># 挑选频繁1项集</span>
        <span style="color:#66d9ef">for</span> index <span style="color:#f92672">in</span> range(len(t)):
            <span style="color:#66d9ef">if</span> t[index] <span style="color:#f92672">in</span> d:
                d[t[index]] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
            <span style="color:#66d9ef">else</span>:
                d[t[index]] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
    l1 <span style="color:#f92672">=</span> op_freq_item(d)  <span style="color:#75715e"># 剔除不符合的1项集</span>
    sub_c_items <span style="color:#f92672">=</span> l1
    c_items_freq <span style="color:#f92672">=</span> {}  <span style="color:#75715e"># 用于存放K-1候选项集，并统计其出现的频数</span>
    freq_items <span style="color:#f92672">=</span> [sub_c_items]  <span style="color:#75715e"># 保存频繁项集</span>
    n <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    <span style="color:#66d9ef">while</span> True:
        n <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;寻找&#34;</span>, n <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#34;-候选项集&#34;</span>)
        sen_c_items <span style="color:#f92672">=</span> list(apriori_gen(sub_c_items, n))  <span style="color:#75715e"># 找到K项集 sen_c_items</span>
        <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> dataset:
            k_subset <span style="color:#f92672">=</span> sorted(subset(sen_c_items, t))  <span style="color:#75715e"># 获取K项集包含候选项集的item的子集 new_c_item</span>
            <span style="color:#66d9ef">print</span>(t, <span style="color:#e6db74">&#34;的子集：&#34;</span>, k_subset)
            <span style="color:#66d9ef">for</span> c_item <span style="color:#f92672">in</span> k_subset:
                <span style="color:#66d9ef">if</span> c_item <span style="color:#f92672">in</span> sen_c_items <span style="color:#f92672">and</span> c_item <span style="color:#f92672">in</span> c_items_freq:
                    c_items_freq[c_item] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>  <span style="color:#75715e"># 找到了加1</span>
                <span style="color:#66d9ef">else</span>:
                    c_items_freq[c_item] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>  <span style="color:#75715e"># 未找到则创建并初始数量为1</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;待挑选的频繁：&#34;</span>, c_items_freq)
        <span style="color:#66d9ef">if</span> len(op_freq_item(c_items_freq)):
            sub_c_items <span style="color:#f92672">=</span> op_freq_item(c_items_freq)  <span style="color:#75715e"># 选出K-1候选项集</span>
            freq_items<span style="color:#f92672">.</span>append(sub_c_items)  <span style="color:#75715e"># 将频繁K项集加入频繁项集freq_items中</span>
            <span style="color:#66d9ef">print</span>(sub_c_items)
        <span style="color:#66d9ef">else</span>:
            <span style="color:#66d9ef">break</span>
        c_items_freq<span style="color:#f92672">.</span>clear()  <span style="color:#75715e"># 清楚字典里的元素</span>
        <span style="color:#66d9ef">print</span>()
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;频繁项集：&#34;</span>, freq_items)



</code></pre></div><h3 id="3优缺点及适用场景-7">3、优缺点及适用场景</h3>
<p><strong>优点：</strong></p>
<p>1)Aprioi算法采用逐层搜索的迭代方法，算法简单明了，没有复杂的理论推导，也易于实现。</p>
<p>2）数据采用水平组织方式</p>
<p>3)采用Apriori 优化方法</p>
<p>4)适合事务数据库的关联规则挖掘。</p>
<p>5)适合稀疏数据集:根据以往的研究,该算法只能适合稀疏数据集的关联规则挖掘，也就是频繁项目集的长度稍小的数据集。</p>
<p><strong>缺陷：</strong></p>
<ol>
<li>对数据库的扫描次数过多。</li>
<li>Apion算法可能产生大量的候选项集。</li>
<li>在频繁项目集长度变大的情况下,运算时间显著增加。</li>
<li>采用唯一支持度,没有考虑各个属性重要程度的不同。</li>
<li>算法的适应面窄。</li>
</ol>
<h2 id="十一fp-growth算法">十一、FP-growth算法</h2>
<h3 id="1主要思想-8">1、主要思想</h3>
<p>FP-growth(Frequent Pattern Tree, 频繁模式树),是韩家炜老师提出的挖掘频繁项集的方法，是将数据集存储在一个特定的称作FP树的结构之后发现频繁项集或频繁项对，即常在一块出现的元素项的集合FP树。P代表频繁模式（Frequent Pattern）。FP树通过链接（link）来连接相似元素，被连起来的元素项可以看成一个链表。
FP-growth算法比Apriori算法效率更高，在整个算法执行过程中，只需遍历数据集2次，就能够完成频繁模式发现，其发现频繁项集的基本过程如下：
（1）构建FP树
（2）从FP树中挖掘频繁项集</p>
<p>FP树的一个例子如下</p>
<p><img src="https://pic2.zhimg.com/80/v2-42b2a5e0c720a1311827cae0f55857f1_720w.jpg" alt="img"></p>
<p>FP-growth算法虽然能高效地发现频繁项集，但是不能用于发现关联规则。FP-growth算法的执行速度快于Apriori算法，通常性能要好两个数量级以上。</p>
<p><strong>FP-growth算法只需要对数据集扫描两次</strong>，它发现频繁项集的过程如下：</p>
<ul>
<li>构建FP树</li>
<li>从FP树中挖掘频繁项集</li>
</ul>
<ol>
<li><strong>构建FP树</strong></li>
</ol>
<p><strong>步骤：</strong></p>
<ol>
<li>扫描数据集，对所有元素项的出现次数进行计数，去掉不满足最小支持度的元素项；</li>
<li>对每个集合进行过滤和排序，过滤是去掉不满足最小支持度的元素项，排序基于元素项的绝对出现频率来进行；</li>
<li>创建只包含空集合的根节点，将过滤和排序后的每个项集依次添加到树中，如果树中已经存在该路径，则增加对应元素上的值。如果该路径不存在，则创建一条新路径。</li>
</ol>
<p><strong>带头指针的FP树</strong></p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212006591.jpeg" alt="img"></p>
<ol start="2">
<li><strong>从一棵FP树中挖掘频繁项集</strong></li>
</ol>
<p><strong>步骤：</strong></p>
<ol>
<li>从FP树中获得条件模式基；</li>
<li>利用条件模式基，构建一个条件FP树；</li>
<li>迭代重复步骤1-2，直到树包含一个元素项为止。</li>
</ol>
<p><strong>抽取条件模式基</strong></p>
<p>条件模式基（conditional pattern base）是以所查询元素项为结尾的路径集合。<strong>每条路径其实都是一条前缀路径</strong>（prefix path）。总之，一条前缀路径是介于所查询元素项与树根节点之间的所有内容。</p>
<p><strong>通过创建的头指针表来得到前缀路径</strong>。头指针表包含相同类型元素链表的起始指针。一旦到达了每一个元素项，就可以上溯这棵树直到根节点为止。</p>
<p>例如下图，元素&quot;r&quot;的前缀路径是{z}、{z,x,y}和{x,s}。同时，每一个路径要与起始元素的计数值关联 。</p>
<p><img src="https://pic3.zhimg.com/80/v2-1cb47bdb49d909954880dbd694036c46_720w.jpg" alt="img"></p>
<h3 id="2实现代码-8">2、实现代码</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">treeNode</span>:
    <span style="color:#e6db74">&#34;&#34;&#34;FP树的类定义&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">def</span> __init__(self,nameValue,numOccur,parentNode):
        self<span style="color:#f92672">.</span>name <span style="color:#f92672">=</span> nameValue  <span style="color:#75715e"># 存放节点的名字</span>
        self<span style="color:#f92672">.</span>count <span style="color:#f92672">=</span> numOccur  <span style="color:#75715e"># 计数值</span>
        self<span style="color:#f92672">.</span>nodeLink <span style="color:#f92672">=</span> None   <span style="color:#75715e"># 用于链接相似的元素项</span>
        self<span style="color:#f92672">.</span>parent <span style="color:#f92672">=</span> parentNode  <span style="color:#75715e"># 指向当前节点的父节点</span>
        self<span style="color:#f92672">.</span>children <span style="color:#f92672">=</span> {}   <span style="color:#75715e"># 当前节点的子节点</span>

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">inc</span>(self,numOccur):
        <span style="color:#e6db74">&#34;&#34;&#34;对count变量增加给定值&#34;&#34;&#34;</span>
        self<span style="color:#f92672">.</span>count <span style="color:#f92672">+=</span> numOccur

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">displayFPTree</span>(self,ind<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
        <span style="color:#e6db74">&#34;&#34;&#34;将树以文本形式显示出来&#34;&#34;&#34;</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;  &#34;</span><span style="color:#f92672">*</span>ind,self<span style="color:#f92672">.</span>name,<span style="color:#e6db74">&#34;  &#34;</span>,self<span style="color:#f92672">.</span>count)
        <span style="color:#66d9ef">for</span> child <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>children<span style="color:#f92672">.</span>values():
            child<span style="color:#f92672">.</span>displayFPTree(ind<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)

<span style="color:#75715e"># FP树构建函数</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">createTree</span>(dataSet,minsup<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
    <span style="color:#e6db74">&#34;&#34;&#34;构建FP树，其中输入的数据集dataSet是字典类型&#34;&#34;&#34;</span>
    <span style="color:#75715e"># 对数据集进行第一次扫描，统计每个元素项出现的频率，将结果存在头指针表中</span>
    headerTable <span style="color:#f92672">=</span> {}
    <span style="color:#66d9ef">for</span> trans <span style="color:#f92672">in</span> dataSet:
        <span style="color:#66d9ef">for</span> item <span style="color:#f92672">in</span> trans:
            headerTable[item] <span style="color:#f92672">=</span> headerTable<span style="color:#f92672">.</span>get(item,<span style="color:#ae81ff">0</span>) <span style="color:#f92672">+</span> dataSet[trans]
    <span style="color:#75715e"># 去掉头指针表中出现次数小于最小支持度阈值的项</span>
    <span style="color:#66d9ef">for</span> item <span style="color:#f92672">in</span> list(headerTable<span style="color:#f92672">.</span>keys()):
        <span style="color:#66d9ef">if</span> headerTable[item] <span style="color:#f92672">&lt;</span> minsup:
            <span style="color:#66d9ef">del</span> (headerTable[item])
    freqItemSet <span style="color:#f92672">=</span> set(headerTable<span style="color:#f92672">.</span>keys())  <span style="color:#75715e"># 获取频繁项</span>
    <span style="color:#75715e"># 如果没有元素项满足要求，则退出</span>
    <span style="color:#66d9ef">if</span> len(freqItemSet) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        <span style="color:#66d9ef">return</span> None,None

    <span style="color:#75715e"># 对头指针表进行扩展，以便可以保存计数值和指向每种类型第一个元素项的指针</span>
    <span style="color:#66d9ef">for</span> item <span style="color:#f92672">in</span> headerTable:
        headerTable[item] <span style="color:#f92672">=</span> [headerTable[item], None]
    <span style="color:#75715e"># 创建只包含空集合的根节点</span>
    retTree <span style="color:#f92672">=</span> treeNode(<span style="color:#e6db74">&#39;Null Set&#39;</span>, <span style="color:#ae81ff">1</span>, None)
    <span style="color:#75715e"># 对数据集进行第二次扫描，构建FP树</span>
    <span style="color:#66d9ef">for</span> trans,count <span style="color:#f92672">in</span> dataSet<span style="color:#f92672">.</span>items():
        localD <span style="color:#f92672">=</span> {}
        <span style="color:#75715e"># 对该项集进行过滤</span>
        <span style="color:#66d9ef">for</span> item <span style="color:#f92672">in</span> trans:
            <span style="color:#66d9ef">if</span> item <span style="color:#f92672">in</span> freqItemSet: <span style="color:#75715e"># 仅考虑频繁项</span>
                localD[item] <span style="color:#f92672">=</span> headerTable[item][<span style="color:#ae81ff">0</span>]
        <span style="color:#75715e"># 对该项集进行排序，按元素的频率来排序,如果两元素在频次相同按字母顺序排序</span>
        <span style="color:#66d9ef">if</span> len(localD) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#75715e"># ord() 函数是以一个字符（长度为1的字符串）作为参数，返回对应的十进制ASCII数值，比如：ord(&#39;a&#39;) 返回97  ord(&#39;b&#39;) 返回98</span>
            <span style="color:#75715e"># 如果都是数字 -ord(p[0]) 改为 int(p[0])</span>
            orderedItems <span style="color:#f92672">=</span> [v[<span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> v <span style="color:#f92672">in</span> sorted(localD<span style="color:#f92672">.</span>items(), key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> p: (p[<span style="color:#ae81ff">1</span>], <span style="color:#f92672">-</span>ord(p[<span style="color:#ae81ff">0</span>])), reverse<span style="color:#f92672">=</span>True)]
            updateTree(orderedItems,retTree,headerTable,count)
    <span style="color:#66d9ef">return</span> retTree,headerTable


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">updateTree</span>(items,inTree,headerTable,count):
    <span style="color:#e6db74">&#34;&#34;&#34;更新FP树，让FP树生长&#34;&#34;&#34;</span>
    <span style="color:#75715e"># 判断项集中第一个元素项是否作为子节点存在，如果存在，更新该元素项的计数</span>
    <span style="color:#66d9ef">if</span> items[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">in</span> inTree<span style="color:#f92672">.</span>children:
        inTree<span style="color:#f92672">.</span>children[items[<span style="color:#ae81ff">0</span>]]<span style="color:#f92672">.</span>inc(count)
    <span style="color:#66d9ef">else</span>:  <span style="color:#75715e"># 不存在，则创建新的节点，并作为子节点添加到树中</span>
        inTree<span style="color:#f92672">.</span>children[items[<span style="color:#ae81ff">0</span>]] <span style="color:#f92672">=</span> treeNode(items[<span style="color:#ae81ff">0</span>],count,inTree)
        <span style="color:#75715e"># 更新头指针表，以指向新的节点</span>
        <span style="color:#66d9ef">if</span> headerTable[items[<span style="color:#ae81ff">0</span>]][<span style="color:#ae81ff">1</span>] <span style="color:#f92672">==</span> None:
            headerTable[items[<span style="color:#ae81ff">0</span>]][<span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> inTree<span style="color:#f92672">.</span>children[items[<span style="color:#ae81ff">0</span>]]
        <span style="color:#66d9ef">else</span>:
            updateHeadera(headerTable[items[<span style="color:#ae81ff">0</span>]][<span style="color:#ae81ff">1</span>], inTree<span style="color:#f92672">.</span>children[items[<span style="color:#ae81ff">0</span>]])
    <span style="color:#75715e"># 不断调用自身函数，每次调用会去掉列表中第一个元素</span>
    <span style="color:#66d9ef">if</span> len(items) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>:
        updateTree(items[<span style="color:#ae81ff">1</span>:],inTree<span style="color:#f92672">.</span>children[items[<span style="color:#ae81ff">0</span>]],headerTable,count)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">updateHeadera</span>(nodeToTest,targetNode):
    <span style="color:#e6db74">&#34;&#34;&#34;确保节点链接指向树中该元素项的每一个实例&#34;&#34;&#34;</span>
    <span style="color:#75715e"># 从头指针表的nodeLink开始，一直沿着nodeLink直到到达链表末尾</span>
    <span style="color:#66d9ef">while</span> (nodeToTest<span style="color:#f92672">.</span>nodeLink <span style="color:#f92672">!=</span> None):
        nodeToTest <span style="color:#f92672">=</span> nodeToTest<span style="color:#f92672">.</span>nodeLink
    nodeToTest<span style="color:#f92672">.</span>nodeLink <span style="color:#f92672">=</span> targetNode
    
<span style="color:#75715e"># 发现以给定元素项结尾的所有路径函数</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">ascendTree</span>(leafNode,prefixPath):
    <span style="color:#66d9ef">if</span> leafNode<span style="color:#f92672">.</span>parent <span style="color:#f92672">!=</span> None:
        prefixPath<span style="color:#f92672">.</span>append(leafNode<span style="color:#f92672">.</span>name)
        ascendTree(leafNode<span style="color:#f92672">.</span>parent,prefixPath)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">findPrefixPath</span>(treeNode):
    condPats <span style="color:#f92672">=</span> {}
    <span style="color:#66d9ef">while</span> treeNode <span style="color:#f92672">!=</span> None:
        prefixPath <span style="color:#f92672">=</span> []
        ascendTree(treeNode,prefixPath)  <span style="color:#75715e"># 迭代上溯整棵树</span>
        <span style="color:#66d9ef">if</span> len(prefixPath) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>:
            <span style="color:#75715e"># 前缀路径对应的计数值</span>
            condPats[frozenset(prefixPath[<span style="color:#ae81ff">1</span>:])] <span style="color:#f92672">=</span> treeNode<span style="color:#f92672">.</span>count
        treeNode <span style="color:#f92672">=</span> treeNode<span style="color:#f92672">.</span>nodeLink
    <span style="color:#66d9ef">return</span> condPats
    
    <span style="color:#75715e"># 递归查找频繁项集的mineTree</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">mineTree</span>(inTree,headerTable,minsup,preFix,freqItemList):
    <span style="color:#75715e"># 从头指针表的低端开始</span>
    <span style="color:#75715e"># headerTable.items():类似[(&#39;z&#39;, [5, None]），(&#39;r&#39;, [3,None]]格式，所以按p[1][0]排序</span>
    bigL <span style="color:#f92672">=</span> [v[<span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> v <span style="color:#f92672">in</span> sorted(headerTable<span style="color:#f92672">.</span>items(),key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> p:p[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">0</span>])]

    <span style="color:#66d9ef">for</span> basePat <span style="color:#f92672">in</span> bigL:
        newFreqSet <span style="color:#f92672">=</span> preFix<span style="color:#f92672">.</span>copy()
        newFreqSet<span style="color:#f92672">.</span>add(basePat)  <span style="color:#75715e"># set类型的用add方法添加</span>
        freqItemList<span style="color:#f92672">.</span>append(newFreqSet)
        <span style="color:#75715e"># 从条件模式基中构建条件FP树</span>
        condPattBase <span style="color:#f92672">=</span> findPrefixPath(headerTable[basePat][<span style="color:#ae81ff">1</span>])
        myCondTree, myHead <span style="color:#f92672">=</span> createTree(condPattBase,minsup)
        <span style="color:#75715e"># 挖掘条件FP树</span>
        <span style="color:#66d9ef">if</span> myHead <span style="color:#f92672">!=</span> None:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;conditional tree for :&#39;</span>,newFreqSet)
            myCondTree<span style="color:#f92672">.</span>displayFPTree()
            mineTree(myCondTree,myHead,minsup,newFreqSet,freqItemList)
            
 <span style="color:#75715e"># 简单数据和数据包装器</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loadData</span>():
    simpDat <span style="color:#f92672">=</span> [[<span style="color:#e6db74">&#39;r&#39;</span>,<span style="color:#e6db74">&#39;z&#39;</span>,<span style="color:#e6db74">&#39;h&#39;</span>,<span style="color:#e6db74">&#39;j&#39;</span>,<span style="color:#e6db74">&#39;p&#39;</span>],
               [<span style="color:#e6db74">&#39;z&#39;</span>,<span style="color:#e6db74">&#39;y&#39;</span>,<span style="color:#e6db74">&#39;x&#39;</span>,<span style="color:#e6db74">&#39;w&#39;</span>,<span style="color:#e6db74">&#39;v&#39;</span>,<span style="color:#e6db74">&#39;u&#39;</span>,<span style="color:#e6db74">&#39;t&#39;</span>,<span style="color:#e6db74">&#39;s&#39;</span>],
               [<span style="color:#e6db74">&#39;z&#39;</span>],
               [<span style="color:#e6db74">&#39;r&#39;</span>,<span style="color:#e6db74">&#39;x&#39;</span>,<span style="color:#e6db74">&#39;n&#39;</span>,<span style="color:#e6db74">&#39;o&#39;</span>,<span style="color:#e6db74">&#39;s&#39;</span>],
               [<span style="color:#e6db74">&#39;y&#39;</span>,<span style="color:#e6db74">&#39;r&#39;</span>,<span style="color:#e6db74">&#39;x&#39;</span>,<span style="color:#e6db74">&#39;z&#39;</span>,<span style="color:#e6db74">&#39;q&#39;</span>,<span style="color:#e6db74">&#39;t&#39;</span>,<span style="color:#e6db74">&#39;p&#39;</span>],
               [<span style="color:#e6db74">&#39;y&#39;</span>,<span style="color:#e6db74">&#39;z&#39;</span>,<span style="color:#e6db74">&#39;x&#39;</span>,<span style="color:#e6db74">&#39;e&#39;</span>,<span style="color:#e6db74">&#39;q&#39;</span>,<span style="color:#e6db74">&#39;s&#39;</span>,<span style="color:#e6db74">&#39;t&#39;</span>,<span style="color:#e6db74">&#39;m&#39;</span>]]
    <span style="color:#66d9ef">return</span> simpDat

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">createInitSet</span>(dataSet):
    retDict <span style="color:#f92672">=</span> {}
    <span style="color:#66d9ef">for</span> trans <span style="color:#f92672">in</span> dataSet:
        retDict[frozenset(trans)] <span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">return</span> retDict

dataSet <span style="color:#f92672">=</span> loadData()
initSet <span style="color:#f92672">=</span> createInitSet(dataSet)
minsup<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>
myFPTree,myheaderTable <span style="color:#f92672">=</span> createTree(initSet, minsup)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;构建的FP树：&#39;</span>)
myFPTree<span style="color:#f92672">.</span>displayFPTree()
freqItems <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;显示所有的条件树&#34;</span>)
mineTree(myFPTree,myheaderTable,minsup,set([]),freqItems)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;频繁项集：</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>,freqItems)


</code></pre></div><h3 id="3优缺点及适用场景-8">3、优缺点及适用场景</h3>
<p><strong>优点：</strong>
FP-tree是一个高度压缩的结构，它存储了用于挖掘频繁项集的全部信息。
<strong>缺点：</strong>
树的子节点过多，例如生成了只包含前缀的树，那么也会导致算法效率大幅度下降。FP-Growth算法需要递归生成条件数据库和条件FP-tree,所以内存开销大，而且只能用于挖掘单维的布尔关联规则。</p>
<h2 id="十二gbdt算法">十二、GBDT算法</h2>
<h3 id="1主要思想-9">1、主要思想</h3>
<p>GBDT属于Boosting算法，它是利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART回归树。GBDT的会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树，而不是分类树（尽管GBDT调整后也可以用于分类但不代表GBDT的树为分类树）</p>
<p>Boosting 是一族可将弱学习器提升为强学习器的算法，属于集成学习（ensemble learning）的范畴。Boosting 方法基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断要好。通俗地说，就是&quot;三个臭皮匠顶个诸葛亮&quot;的道理。</p>
<p>基于梯度提升算法的学习器叫做 GBM(Gradient Boosting Machine)。理论上，GBM 可以选择各种不同的学习算法作为基学习器。GBDT 实际上是 GBM 的一种情况。</p>
<p>为什么梯度提升方法倾向于选择决策树作为基学习器呢？(也就是 GB 为什么要和 DT 结合，形成 GBDT) 决策树可以认为是 if-then 规则的集合，易于理解，可解释性强，预测速度快。同时，决策树算法相比于其他的算法需要更少的特征工程，比如可以不用做特征标准化，可以很好的处理字段缺失的数据，也可以不用关心特征间是否相互依赖等。决策树能够自动组合多个特征。</p>
<p>不过，单独使用决策树算法时，有容易过拟合缺点。所幸的是，通过各种方法，抑制决策树的复杂性，降低单颗决策树的拟合能力，再通过梯度提升的方法集成多个决策树，最终能够很好的解决过拟合的问题。由此可见，梯度提升方法和决策树学习算法可以互相取长补短，是一对完美的搭档。</p>
<p>至于抑制单颗决策树的复杂度的方法有很多，比如限制树的最大深度、限制叶子节点的最少样本数量、限制节点分裂时的最少样本数量、吸收 bagging 的思想对训练样本采样（subsample），在学习单颗决策树时只使用一部分训练样本、借鉴随机森林的思路在学习单颗决策树时只采样一部分特征、在目标函数中添加正则项惩罚复杂的树结构等。</p>
<p>考虑一个简单的例子来演示 GBDT 算法原理。</p>
<p>下面是一个二分类问题，1 表示可以考虑的相亲对象，0 表示不考虑的相亲对象。</p>
<p>特征维度有 3 个维度，分别对象 身高，金钱，颜值</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212034242.jpeg" alt="img"></p>
<p>对应这个例子，训练结果是 perfect 的，全部正确， 特征权重可以看出，对应这个例子训练结果颜值的重要度最大，看一下训练得到的树。</p>
<p><strong>Tree 0：</strong></p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212034081.jpeg" alt="img"></p>
<p><strong>Tree 1：</strong></p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212034742.jpeg" alt="img"></p>
<p><strong>原理推导</strong></p>
<p><strong>目标函数</strong></p>
<p>监督学习的关键概念：模型（model）、参数（parameters）、目标函数（objective function）</p>
<p>模型就是所要学习的条件概率分布或者决策函数，它决定了在给定特征向量时如何预测出目标。</p>
<p>参数就是我们要从数据中学习得到的内容。模型通常是由一个参数向量决定的函数。</p>
<p>目标函数通常定义为如下形式：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212034351.jpeg" alt="img"></p>
<p>其中，L 是损失函数，用来衡量模型拟合训练数据的好坏程度；Ω称之为正则项，用来衡量学习到的模型的复杂度。</p>
<p>对正则项的优化鼓励算法学习到较简单的模型，简单模型一般在测试样本上的预测结果比较稳定、方差较小（奥坎姆剃刀原则）。也就是说，优化损失函数尽量使模型走出欠拟合的状态，优化正则项尽量使模型避免过拟合。</p>
<p><strong>加法模型</strong></p>
<p>GBDT 算法可以看成是由 K 棵树组成的加法模型：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212034207.png" alt="img"></p>
<p>如何来学习加法模型呢？</p>
<p>解这一优化问题，可以用前向分布算法（forward stagewise algorithm）。因为学习的是加法模型，如果能够从前往后，每一步只学习一个基函数及其系数（结构），逐步逼近优化目标函数，那么就可以简化复杂度。这一学习过程称之为 Boosting。具体地，我们从一个常量预测开始，每次学习一个新的函数，过程如下：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212034922.png" alt="img"></p>
<p>在第 t 步，这个时候目标函数可以写为：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212034053.png" alt="img"></p>
<p>举例说明，假设损失函数为平方损失（square loss），则目标函数为：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212034763.png" alt="img"></p>
<p>其中，称</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212034938.png" alt="img"></p>
<p>之为残差（residual）。因此，使用平方损失函数时，GBDT 算法的每一步在生成决策树时只需要拟合前面的模型的残差。</p>
<p><strong>泰勒公式</strong></p>
<p>定义：</p>
<p><img src="https://pic1.zhimg.com/80/v2-485bd82b00f9152e1c10abc9199eb568_720w.png" alt="img"></p>
<p>泰勒公式简单的理解，就是函数某个点的取值可以用参考点取值和 n+1 阶导数的来表示，而且这个公式是有规律的比较好记。</p>
<p>根据泰勒公式把函数</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035100.png" alt="img"></p>
<p>在</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035166.png" alt="img"></p>
<p>点处二阶展开，可得到如下等式：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035586.png" alt="img"></p>
<p>则等式(1) 可转化为：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035867.png" alt="img"></p>
<p>假设损失函数为平方损失函数，把对应的一阶导数和二阶导数代入等式(4) 即得等式(2)。</p>
<p>由于函数中的常量在函数最小化的过程中不起作用，因此我们可以从等式(4) 中移除掉常量项，得：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035117.png" alt="img"></p>
<p><strong>GBDT 算法</strong></p>
<p>一颗生成好的决策树，假设其叶子节点个数为</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035736.png" alt="img"></p>
<p>，</p>
<p>决策树的复杂度可以由正则项</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035966.png" alt="img"></p>
<p>来定义，即决策树模型的复杂度由生成的树的叶子节点数量和叶子节点对应的值向量的 L2 范数决定。</p>
<p>定义集合</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035088.png" alt="img"></p>
<p>为所有被划分到叶子节点的训练样本的集合。等式(5) 可以根据树的叶子节点重新组织为 T 个独立的二次函数的和：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035868.png" alt="img"></p>
<p>定义</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035373.png" alt="img"></p>
<p>，则等式(6) 可写为：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035812.png" alt="img"></p>
<p>因为一元二次函数最小值处，一阶导数等于 0:</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035196.png" alt="img"></p>
<p>此时，目标函数的值为</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035627.png" alt="img"></p>
<p>综上，为了便于理解，单颗决策树的学习过程可以大致描述为： 1. 枚举所有可能的树结构 q 2. 用等式(8) 为每个 q 计算其对应的分数 Obj，分数越小说明对应的树结构越好 3. 根据上一步的结果，找到最佳的树结构，用等式(7) 为树的每个叶子节点计算预测值</p>
<p>然而，可能的树结构数量是无穷的，所以实际上我们不可能枚举所有可能的树结构。通常情况下，我们采用贪心策略来生成决策树的每个节点。</p>
<p>\1. 从深度为 0 的树开始，对每个叶节点枚举所有的可用特征 2. 针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益） 3. 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该节点生长出左右两个新的叶节点，并为每个新节点关联对应的样本集 4. 回到第 1 步，递归执行到满足特定条件为止</p>
<p><strong>收益的计算</strong></p>
<p>如何计算每次分裂的收益呢？假设当前节点记为 C,分裂之后左孩子节点记为 L，右孩子节点记为 R，则该分裂获得的收益定义为当前节点的目标函数值减去左右两个孩子节点的目标函数值之和：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035782.png" alt="img"></p>
<p>根据等式(8) 可得：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035068.png" alt="img"></p>
<p>其中，</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035704.png" alt="img"></p>
<p>项表示因为增加了树的复杂性（该分裂增加了一个叶子节点）带来的惩罚。</p>
<p>最后，总结一下 GBDT 的学习算法：</p>
<ol>
<li>算法每次迭代生成一颗新的决策树 ;</li>
<li>在每次迭代开始之前，计算损失函数在每个训练样本点的一阶导数和二阶导数 ;</li>
<li>通过贪心策略生成新的决策树，通过等式(7) 计算每个叶节点对应的预测值</li>
<li>把新生成的决策树</li>
</ol>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035386.png" alt="img"></p>
<p>添加到模型中：</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212035288.png" alt="img"></p>
<h3 id="2实现代码-9">2、实现代码</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
<span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> GradientBoostingClassifier
<span style="color:#f92672">from</span> sklearn.externals <span style="color:#f92672">import</span> joblib


data <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;./data_train.csv&#34;</span>)
x_columns <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> data<span style="color:#f92672">.</span>columns:
    <span style="color:#66d9ef">if</span> x <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> [<span style="color:#e6db74">&#39;id&#39;</span>, <span style="color:#e6db74">&#39;label&#39;</span>]:
        x_columns<span style="color:#f92672">.</span>append(x)
X <span style="color:#f92672">=</span> data[x_columns]
y <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#39;label&#39;</span>]
x_train, x_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(X, y)

<span style="color:#75715e"># 模型训练，使用GBDT算法</span>
gbr <span style="color:#f92672">=</span> GradientBoostingClassifier(n_estimators<span style="color:#f92672">=</span><span style="color:#ae81ff">3000</span>, max_depth<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, min_samples_split<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)
gbr<span style="color:#f92672">.</span>fit(x_train, y_train<span style="color:#f92672">.</span>ravel())
joblib<span style="color:#f92672">.</span>dump(gbr, <span style="color:#e6db74">&#39;train_model_result4.m&#39;</span>)   <span style="color:#75715e"># 保存模型</span>

y_gbr <span style="color:#f92672">=</span> gbr<span style="color:#f92672">.</span>predict(x_train)
y_gbr1 <span style="color:#f92672">=</span> gbr<span style="color:#f92672">.</span>predict(x_test)
acc_train <span style="color:#f92672">=</span> gbr<span style="color:#f92672">.</span>score(x_train, y_train)
acc_test <span style="color:#f92672">=</span> gbr<span style="color:#f92672">.</span>score(x_test, y_test)
<span style="color:#66d9ef">print</span>(acc_train)
<span style="color:#66d9ef">print</span>(acc_test)
</code></pre></div><h3 id="3优缺点及适用场景-9">3、优缺点及适用场景</h3>
<p>GBDT主要的优点有：</p>
<p>　1) 可以灵活处理各种类型的数据，包括连续值和离散值。</p>
<p>　2) 在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。</p>
<p>　3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。</p>
<p>GBDT的主要缺点有：</p>
<p>　1)由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。</p>
<h2 id="十三xgboost算法">十三、XGBOOST算法</h2>
<h3 id="1主要思想-10">1、主要思想</h3>
<p>XGBoost算法的基本思想与GBDT类似，不断地地进行特征分裂来生长一棵树，每一轮学习一棵树，其实就是去拟合上一轮模型的预测值与实际值之间的残差。当我们训练完成得到k棵树时，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数，最后只需将每棵树对应的分数加起来就是该样本的预测值。</p>
<p><strong>1. Boosted trees</strong></p>
<p>Boosted trees是一种集成方法，Boosting算法是一种加法模型(additive training)，定义如下：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13256158-40b5abf90351aeae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/785/format/webp" alt="img"></p>
<p>这里K是树的棵数，f(x)是函数空间中的一个函数：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13256158-d435f0afa9586cb1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/427/format/webp" alt="img"></p>
<p>q(x)表示将样本x分到了某个叶子节点上，w是叶子节点的分数（leaf score）</p>
<p>下面通过一个具体的例子来说明：预测一个人是否喜欢电脑游戏，下图表明小男孩更喜欢打游戏。</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13256158-4fe54693f4edbd5b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/938/format/webp" alt="img"></p>
<p>image</p>
<p><strong>2. 目标函数正则化</strong></p>
<p>XGBoost使用的目标函数如下：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13256158-7e522b5959dfa6d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/572/format/webp" alt="img"></p>
<p>我们可以看出XGBoost在GBDT的误差函数基础上加入了L1和L2正则项，其中Loss函数可以是平方损失或逻辑损失，T代表叶子节点数，w代表叶子节点的分数。加入正则项的好处是防止过拟合，这个好处是由两方面体现的：一是预剪枝，因为正则项中有限定叶子节点数；二是正则项里leaf scroe的L2模平方的系数，对leaf scroe做了平滑。</p>
<p>接下来我们对目标函数进行目标函数的求解：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13256158-5b026b3e5a14bbfb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600/format/webp" alt="img"></p>
<p>该目标函数表示：第i样本的第t次迭代误差函数，后面的推导基于上式。这种学习方式已经从函数空间转到了函数空间：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13256158-4f9360d4c7487c00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/897/format/webp" alt="img"></p>
<p>下面对目标函数进行泰勒公式二级展开、化简：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13256158-1d130dae024ce5bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/631/format/webp" alt="img"></p>
<p>如果确定了树的结构，为了使目标函数最小，可以令其导数为0，解得每个叶节点的最优预测分数为：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13256158-9ad3bd4950d2c998.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/620/format/webp" alt="img"></p>
<p>代入目标函数，解得最小损失为：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13256158-4898aee83683a851.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/616/format/webp" alt="img"></p>
<p><strong>3. 节点切分算法</strong></p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13256158-47e06206581bbde4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1103/format/webp" alt="img"></p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13256158-f353f1792dc86df4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1111/format/webp" alt="img"></p>
<p>注： 近似算法中使用到了分位数，关于分位数的选取，论文提出了一种算法Weighted Quantile Sketch 。XGBoost不是按照样本个数进行分位，而是以二阶导数为权重</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13256158-3db04745ddaac55c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/867/format/webp" alt="img"></p>
<p>Q: 为什么使用hi加权？</p>
<p>A: 比较直观的解释是因为目标函数可以化简为如下形式：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13256158-fea529fd271544f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/581/format/webp" alt="img"></p>
<ol>
<li>
<p>其他
在实际工作中，大多数输入是稀疏的。造成稀疏的原因有很多种，比如：缺失值、one-hot编码等。因此，论文提出为树中的节点设置一个默认方向来应对稀疏输入。论文实验表明稀疏感知算法 要比传统方法快50倍，算法如下：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13256158-e4134963fdc7e272.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/556/format/webp" alt="img"></p>
<p>下面通过例子具体说明：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/13256158-61bc7a4de78ccf01.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/419/format/webp" alt="img"></p>
</li>
</ol>
<p>注： 红色路径代表默认方向</p>
<blockquote>
<p><strong>Shrinkage</strong>: 可以理解为学习率，算法每次迭代后会乘这个系数；
<strong>列采样</strong>： 降低过拟合，论文实验表明列采样比行采样效果好。</p>
</blockquote>
<h3 id="2实现代码-10">2、实现代码</h3>
<p><strong>模型参数</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">params <span style="color:#f92672">=</span> {
    <span style="color:#e6db74">&#39;booster&#39;</span>:<span style="color:#e6db74">&#39;gbtree&#39;</span>,
    <span style="color:#e6db74">&#39;objective&#39;</span>:<span style="color:#e6db74">&#39;multi:softmax&#39;</span>,   <span style="color:#75715e"># 多分类问题</span>
    <span style="color:#e6db74">&#39;num_class&#39;</span>:<span style="color:#ae81ff">10</span>,  <span style="color:#75715e"># 类别数，与multi softmax并用</span>
    <span style="color:#e6db74">&#39;gamma&#39;</span>:<span style="color:#ae81ff">0.1</span>,    <span style="color:#75715e"># 用于控制是否后剪枝的参数，越大越保守，一般0.1 0.2的样子</span>
    <span style="color:#e6db74">&#39;max_depth&#39;</span>:<span style="color:#ae81ff">12</span>,  <span style="color:#75715e"># 构建树的深度，越大越容易过拟合</span>
    <span style="color:#e6db74">&#39;lambda&#39;</span>:<span style="color:#ae81ff">2</span>,  <span style="color:#75715e"># 控制模型复杂度的权重值的L2 正则化项参数，参数越大，模型越不容易过拟合</span>
    <span style="color:#e6db74">&#39;subsample&#39;</span>:<span style="color:#ae81ff">0.7</span>, <span style="color:#75715e"># 随机采样训练样本</span>
    <span style="color:#e6db74">&#39;colsample_bytree&#39;</span>:<span style="color:#ae81ff">3</span>,<span style="color:#75715e"># 这个参数默认为1，是每个叶子里面h的和至少是多少</span>
    <span style="color:#75715e"># 对于正负样本不均衡时的0-1分类而言，假设h在0.01附近，min_child_weight为1</span>
    <span style="color:#75715e">#意味着叶子节点中最少需要包含100个样本。这个参数非常影响结果，</span>
    <span style="color:#75715e"># 控制叶子节点中二阶导的和的最小值，该参数值越小，越容易过拟合</span>
    <span style="color:#e6db74">&#39;silent&#39;</span>:<span style="color:#ae81ff">0</span>,  <span style="color:#75715e"># 设置成1 则没有运行信息输入，最好是设置成0</span>
    <span style="color:#e6db74">&#39;eta&#39;</span>:<span style="color:#ae81ff">0.007</span>,  <span style="color:#75715e"># 如同学习率</span>
    <span style="color:#e6db74">&#39;seed&#39;</span>:<span style="color:#ae81ff">1000</span>,
    <span style="color:#e6db74">&#39;nthread&#39;</span>:<span style="color:#ae81ff">7</span>,  <span style="color:#75715e">#CPU线程数</span>
    <span style="color:#75715e">#&#39;eval_metric&#39;:&#39;auc&#39;</span>
}
</code></pre></div><p>模型参数分为三类：通用参数、Booster参数、目标函数参数。</p>
<p><strong>通用参数</strong>是模型的宏观参数，通常不用刻意去设置。</p>
<p>1、booster参数是迭代的模型，包括gbtree（基于树的模型）和gblinear（基于线性模型），默认选择是gbtree。</p>
<p>2、silent 参数决定是否输出信息，默认是0。</p>
<p>3、nthread 参数是多线程控制，默认为最大线程，就是使用CPU的全部核。</p>
<p>4、num_feature 参数是特征维数，不需要手动设置，模型会自动设置。</p>
<p><strong>Booster参数</strong>通常就是tree booster的参数，因为linear booster的表现通常不如tree booster，因此很少使用。</p>
<p>1、eta（默认0.3），模型在更新时计算新的权重，通过减少每一步的权重，使模型更加保守，来防止过拟合。</p>
<p>2、gama（默认0），给定了损失函数的最低值，大于该值时节点才会分裂，该值越大模型越保守。</p>
<p>3、max_depth（默认6）， 代表树的最大深度，该值越大模型对数据的拟合程度越高，适当控制最大深度可以防止模型过拟合，可以通过交叉验证cv函数来调参学习，通常取值范围在3-10之间。</p>
<p>4、min_child_weight（默认1），代表树模型的最小叶子节点样本的权重和，如果叶子节点的样本权重和小于该值，则拆分过程结束，该参数值较大时可以避免模型学习到局部的特殊样本防止模型过拟合，但该参数值过大会导致模型欠拟合，可以通过交叉验证cv函数来调参学习。</p>
<p>5、subsample（默认1），代表每棵树随机采样的比例，该参数值较小可以避免过拟合，但过小会导致模型欠拟合。</p>
<p>6、colsample_bytree（默认1），代表每棵树随机采样的列数占比，每一列是一个特征。</p>
<p>7、scale_pos_weight（默认0），在样本类别不平衡时，该参数值取大于0的值可以帮助模型更快收敛。</p>
<p>8、lambda（默认1），模型权重的L2正则化惩罚系数，平时很少使用，但可以用来降低过拟合。</p>
<p>9、alpha（默认0），模型权重的L1正则化惩罚系数，适用于数据维度很高时，算法速度更快。</p>
<p><strong>目标参数</strong>用来控制理想的优化目标和每一步输出结果的度量方法。</p>
<p>1、objective（默认reg：linear），代表学习任务需要最小化的损失函数，可选的目标函数有：</p>
<p>“reg:linear” ：线性回归。</p>
<p>“reg:logistic” ：逻辑回归。</p>
<p>“binary:logistic” ：二分类的逻辑回归问题，输出为概率。</p>
<p>“binary:logitraw” ：二分类的逻辑回归问题，输出的结果为wTx。</p>
<p>“count:poisson” ：计数问题的poisson回归，输出结果为poisson分布。</p>
<p>在poisson回归中，max_delta_step的缺省值为0.7。(used to safeguard     optimization)</p>
<p>“multi:softmax” ：让XGBoost采用softmax目标函数处理多分类问题，同时需要设置参数num_class（类别个数）</p>
<p>“multi:softprob” ：和softmax一样，但是输出的是ndata * nclass的向量，可以将该向量reshape成ndata行nclass列的矩阵。每行数据表示样本所属于每个类别的概率。</p>
<p>“rank:pairwise”：–set XGBoost to do ranking task by minimizing the pairwise loss</p>
<p>​    2、base_score（默认0.5），所有样本的初始预测值，一般不需要设置。</p>
<p>​    3、eval_metric（默认值取决于前面objective参数的取值），代表模型校验数据所需要的评价指标，不同的目标函数对应不同的默认评价指标（rmse for regression, and error for classification, mean average precision for ranking），用户也可以自己添加多种评价指标。 常用的评价指标： “rmse”: root mean square error均方误差</p>
<p>“mae”:mean absolute error 平均绝对误差</p>
<p>“logloss”: negative log-likelihood负对数似然</p>
<p>“error”: Binary classification error rate 二分类错误率，阈值为0.5</p>
<p>“merror”: Multiclass classification error rate多分类错误率</p>
<p>“mlogloss”: Multiclass logloss多分类对数损失</p>
<p>“auc”: Area under the curve for ranking evaluation ROC曲线下面积</p>
<p>4、seed（默认0），随机数的种子，设置该参数可以复现随机数据的结果，固定该值可以得到相同的随机划分。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> xgboost <span style="color:#f92672">as</span> xgb
<span style="color:#f92672">from</span> xgboost <span style="color:#f92672">import</span> plot_importance
<span style="color:#f92672">from</span> matplotlib <span style="color:#f92672">import</span> pyplot <span style="color:#66d9ef">as</span> plt
<span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
<span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> accuracy_score
<span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> warnings
<span style="color:#f92672">from</span> xgboost.sklearn <span style="color:#f92672">import</span> XGBClassifier
<span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> metrics

feature_file <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_excel(<span style="color:#e6db74">&#34;data.xlsx&#34;</span>)

x <span style="color:#f92672">=</span> []<span style="color:#75715e"># 特征数据</span>
y <span style="color:#f92672">=</span> []<span style="color:#75715e"># 标签</span>
<span style="color:#66d9ef">for</span> index <span style="color:#f92672">in</span> feature_file<span style="color:#f92672">.</span>index<span style="color:#f92672">.</span>values:
    <span style="color:#75715e"># print(&#39;index&#39;, index)</span>
    <span style="color:#75715e"># print(feature_file.ix[index].values) </span>
    x<span style="color:#f92672">.</span>append(feature_file<span style="color:#f92672">.</span>ix[index]<span style="color:#f92672">.</span>values[<span style="color:#ae81ff">1</span>: <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]) <span style="color:#75715e"># 每一行都是ID+特征+Label</span>
    y<span style="color:#f92672">.</span>append(feature_file<span style="color:#f92672">.</span>ix[index]<span style="color:#f92672">.</span>values[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#75715e">#</span>
x, y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(x), np<span style="color:#f92672">.</span>array(y)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;x,y shape&#39;</span>, np<span style="color:#f92672">.</span>array(x)<span style="color:#f92672">.</span>shape, np<span style="color:#f92672">.</span>array(y)<span style="color:#f92672">.</span>shape)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;样本数&#39;</span>, len(feature_file<span style="color:#f92672">.</span>index<span style="color:#f92672">.</span>values))
<span style="color:#75715e"># 分训练集和测试集</span>
X_train,X_test,y_train,y_test <span style="color:#f92672">=</span> train_test_split(x,y,test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>,random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">12343</span>)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;训练集和测试集 shape&#39;</span>, X_train<span style="color:#f92672">.</span>shape, y_train<span style="color:#f92672">.</span>shape, X_test<span style="color:#f92672">.</span>shape, y_test<span style="color:#f92672">.</span>shape)
<span style="color:#75715e"># 共有218个样本，每个样本106个特征和1个标签，训练集174个样本，验证集44个样本</span>

<span style="color:#75715e"># 性能评估以XGboost为例</span>
xgb <span style="color:#f92672">=</span> xgb<span style="color:#f92672">.</span>XGBClassifier()
<span style="color:#75715e"># 对训练集训练模型</span>
xgb<span style="color:#f92672">.</span>fit(X_train,y_train)
<span style="color:#75715e"># 对测试集进行预测</span>
y_pred <span style="color:#f92672">=</span> xgb<span style="color:#f92672">.</span>predict(X_test)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">模型的平均准确率（mean accuracy = (TP+TN)/(P+N) ）&#34;</span>)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">Xgboost：&#34;</span>,xgb<span style="color:#f92672">.</span>score(X_test,y_test))
<span style="color:#75715e"># print(&#39;(y_test,y_pred)&#39;, y_test,y_pred)    print(&#34;\n性能评价：&#34;)</span>
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">预测结果评价报表：</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, metrics<span style="color:#f92672">.</span>classification_report(y_test,y_pred))
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">混淆矩阵：</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, metrics<span style="color:#f92672">.</span>confusion_matrix(y_test,y_pred))
</code></pre></div><h3 id="3优缺点及适用场景-10">3、优缺点及适用场景</h3>
<p><strong>XGBoost与GBDT相比，其优势：</strong></p>
<ul>
<li>将树模型的复杂度加入到正则项中，来避免过拟合，因此泛化性能会优于GBDT。</li>
<li>损失函数用泰勒展开式展开，同时用到了一阶和二阶导数，可以加快优化速度。</li>
<li>GBDT只支持CART作为基学习器，XGBoost还支持线性分类器作为基学习器。</li>
<li>引进了特征子采样，像随机森林那样，既能避免过拟合，又能减少计算。</li>
<li>在寻找最优分割点时，考虑到传统的贪心算法效率较低，实现了一种近似贪心算法，用来加速和减少内存小号，除此之外，还考虑了稀疏数据集合缺失值的处理。</li>
<li>XGBoost支持并行处理。XGBoost的并行不是模型生成的并行，而是在特征上的并行，将特征排序后以block的形式存储在内存中，在后面迭代重复使用这个结构。这个block也使得并行化成为了可能，其次在节点分裂时，计算每个特征的增益，最终选择增益最大的那个特征去做分割，那么各个特征的增益计算就可以开多线程进行。</li>
</ul>
<p><strong>与lightGBM相比的不足点：</strong></p>
<ul>
<li>XGBoosting采用预排序，在迭代之前，对结点的特征做预排序，遍历选择最优分割点，数据量大时，贪心法耗时，LightGBM方法采用histogram算法，占用的内存低，数据分割的复杂度更低。</li>
<li>XGBoosting采用level-wise生成决策树，同时分裂同一层的叶子，从而进行多线程优化，不容易过拟合，但很多叶子节点的分裂增益较低，没必要进行跟进一步的分裂，这就带来了不必要的开销；LightGBM采用深度优化，leaf-wise生长策略，每次从当前叶子中选择增益最大的结点进行分裂，循环迭代，但会生长出更深的决策树，产生过拟合，因此引入了一个阈值进行限制，防止过拟合。</li>
</ul>
<h2 id="十四随机森林">十四、随机森林</h2>
<h3 id="1主要思想-11">1、主要思想</h3>
<p>随机森林（Random Forest，简称RF），通过集成学习的思想将多棵决策树集成的一种算法，它的基本单元是<a href="https://lz5z.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a>。从直观角度来解释，每棵决策树都是一个分类器（假设现在针对的是分类问题），那么对于一个输入样本，N棵树会有N个分类结果。而随机森林集成了所有的分类投票结果，将投票次数最多的类别指定为最终的输出。</p>
<p><img src="https://gitee.com/zerealsong/blogimage/raw/master/img/202202212100428.png" alt="image-20220221210003246"></p>
<p><strong>随机森林构建</strong></p>
<p><strong>随机采样</strong></p>
<p>首先是两个随机采样的过程，random forest 对输入的数据要进行行、列的采样。</p>
<p>对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为 N 个，那么采样的样本也为 N 个，这选择好了的 N 个样本用来训练一个决策树，作为决策树根节点处的样本，同时使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现 over-fitting。</p>
<p>对于列采样，从 M 个 feature 中，选择 m 个 (m &laquo; M)，即：当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这 M 个属性中选取出 m 个属性，满足条件 m &laquo; M。</p>
<p><strong>完全分裂</strong></p>
<p>对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。分裂的办法是：采用上面说的列采样的过程从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。</p>
<p>决策树形成过程中每个节点都要按完全分裂的方式来分裂，一直到不能够再分裂为止（如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。</p>
<p><strong>每棵树的按照如下规则生成：</strong></p>
<p>（1）如果训练集大小为N，对于每棵树而言，随机且<strong>有放回</strong>地从训练集中的抽取N个训练样本（就是bootstrap sample方法, 拔靴法采样）作为该树的训练集；从这里我们可以知道：每棵树的训练集都是不同的，而且里面包含重复的训练样本。</p>
<p>（2）如果存在M个特征，则在每个节点分裂的时候，从M中随机选择m个特征维度（m &laquo; M），使用这些m个特征维度中最佳特征(最大化信息增益)来分割节点。在森林生长期间，m的值保持不变。</p>
<p>一开始我们提到的随机森林中的“<strong>随机</strong>”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。</p>
<p><strong>随机森林分类效果（错误率）与两个因素有关：</strong></p>
<ul>
<li>森林中任意两棵树的相关性：相关性越大，错误率越大；（弱分类器应该good且<strong>different</strong>）</li>
<li>森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。（弱分类器应该<strong>good</strong>且different）</li>
</ul>
<p>减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m，这也是随机森林的一个重要参数。</p>
<p><strong>随机森林参数</strong></p>
<p>在scikit-learn中，RF的分类器是RandomForestClassifier，回归器是RandomForestRegressor。RF的参数也包括两部分，第一部分是Bagging框架的参数，第二部分是一棵CART决策树的参数。具体的参数参考随机森林分类器的函数原型：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sklearn<span style="color:#f92672">.</span>ensemble<span style="color:#f92672">.</span>RandomForestClassifier(
        n_estimators<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, criterion<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gini&#39;</span>,
        max_depth<span style="color:#f92672">=</span>None,min_samples_split<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, 
        min_samples_leaf<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, min_weight_fraction_leaf<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>,
        max_features<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;auto&#39;</span>, max_leaf_nodes<span style="color:#f92672">=</span>None,
        min_impurity_split<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-07</span>,bootstrap<span style="color:#f92672">=</span>True,
        oob_score<span style="color:#f92672">=</span>False, n_jobs<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, 
        random_state<span style="color:#f92672">=</span>None, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,
        warm_start<span style="color:#f92672">=</span>False, class_weight<span style="color:#f92672">=</span>None)
</code></pre></div><h3 id="2实现代码-11">2、实现代码</h3>
<h3 id="3优缺点及适用场景-11">3、优缺点及适用场景</h3>
<p><strong>优点</strong></p>
<ol>
<li>比较适合做多分类问题，训练和预测速度快，在数据集上表现良好；</li>
<li>对训练数据的容错能力强，是一种有效地估计缺失数据的一种方法，当数据集中有大比例的数据缺失时仍然可以保持精度不变和能够有效地处理大的数据集；</li>
<li>能够处理很高维度的数据，并且不用做特征选择，即：可以处理没有删减的成千上万的变量；</li>
<li>能够在分类的过程中可以生成一个泛化误差的内部无偏估计；</li>
<li>能够在训练过程中检测到特征之间的相互影响以及特征的重要性程度；</li>
<li>不会出现过度拟合；</li>
<li>实现简单并且容易实现并行化。</li>
</ol>
<h2 id="十五lightgbm算法">十五、LightGBM算法</h2>
<h3 id="1主要思想-12">1、主要思想</h3>
<h3 id="2实现代码-12">2、实现代码</h3>
<h3 id="3优缺点及适用场景-12">3、优缺点及适用场景</h3>
<h2 id="十六马尔可夫">十六、马尔可夫</h2>
<h3 id="1主要思想-13">1、主要思想</h3>
<h3 id="2实现代码-13">2、实现代码</h3>
<h3 id="3优缺点及适用场景-13">3、优缺点及适用场景</h3>
<h3 id="heading"></h3>

    </div>
    <div class="post-footer">
      
    </div>
    <div class="post-comment">
      
      


<span id="/post/ml%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5/" class="leancloud_visitors" data-flag-title="">
    <span class="post-meta-item-text">文章阅读量 </span>
    <span class="leancloud-visitors-count">1000000</span>
    <p></p>
  </span>
<div id="vcomments"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<script type="text/javascript">
  new Valine({
    el: '#vcomments' ,
    appId: 'cScQmclMsD4OOWCclCP1pNsz-gzGzoHsz',
    appKey: 'dtSaHLmdH3J4ICVVInYg9YFM',
    notify:  false ,
  verify:  false ,
  avatar:'mm',
    placeholder: '说点什么吧...',
    visitor:  true 
  });
</script>

    </div>
  </article>

    </main>
  </body>
</html>
